# DeepSearchAgents Configuration File

# Debug mode (enables verbose output)
debug: false

# Model configuration
models:
  orchestrator_id: "openai/o4-mini-high"  # Used for main LLM orchestration
  search_id: "openai/o4-mini-high"        # Used for search (only if different)
  reranker_type: "jina-reranker-m0"             # # or `jina-colbert-v2` , Default is `m0` M0 this Multilingual multimodal reranker model for ranking visual documents
# Agent common settings
agents:
  common:
    verbose_tool_callbacks: true        # If true, show full tool inputs/outputs
  
  # Settings specific to the ReAct agent
  react:
    max_steps: 25                       # Max number of reasoning steps
    enable_streaming: false             # Enable streaming output for final answer
    planning_interval: 7                # Interval for agent planning steps

  # Settings specific to the CodeAct agent
  codact:
    executor_type: "local"              # local or lambda (for PROD must be set to Secure code execution eg. secure Docker/E2B/AWS Lambda...)
    max_steps: 25                       # Max number of steps in execution
    verbosity_level: 1                  # 0=minimal, 1=normal, 2=verbose
    enable_streaming: false             # Enable streaming response (CLI priority)
    executor_kwargs: {}                 # Additional kwargs for executor
    additional_authorized_imports: []   # Additional Python modules to allow importing
    planning_interval: 4                # Interval for agent planning steps

# Service configuration
service:
  host: "0.0.0.0"
  port: 8000
  version: "v0.2.4.dev"
  deepsearch_agent_mode: "codact"       # "react" or "codact" Default is "codact" because it's more powerful and faster

# Logging configuration
logging:
  litellm_level: "WARNING"        # Use WARNING level to reduce INFO logs
  filter_repeated_logs: true      # Enable repeated log filtering
  filter_cost_calculator: true    # Filter cost calculation related logs
  filter_token_counter: true      # Filter token count related logs
  format: "minimal"               # Use simplified format

# --- LiteLLM specific settings (optional) ---
# litellm:
# base_url_var: litellm proxy server gateway "YOUR_LITELLM_BASE_UR" # e.g., "https://your-litellm-llms-api-gateway-url.com" 

# LiteLLM proxy server gateway LLM configs model_list
# In the config.yaml the model_name parameter is the user-facing name to use for your deployment.
# In the config below:
# model_name: the name to pass TO litellm from the external client
# litellm_params.model: the model string passed to the litellm.completion() function
#  Tested models:
model_list:
    - model_name: claude-3-7-sonnet-20250219
      litellm_params:
          model: claude-3-7-sonnet-20250219
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
      model_info:          
          supports_reasoning: True
    - model_name: gemini-2.5-pro-exp-03-25
      litellm_params:
          model: google/gemini-2.5-pro-exp-03-25
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
      model_info:          
          supports_reasoning: True          
    - model_name: o3
      litellm_params:
          model: openrouter/openai/o3
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
      model_info:          
          supports_reasoning: True          
    - model_name: o3-mini
      litellm_params:
          model: openrouter/openai/o3
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY                 
      model_info:
          supports_reasoning: True
    - model_name: o4-mini-high
      litellm_params:
          model: openrouter/openai/o4-mini-high
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
      model_info:
          supports_reasoning: True                   
    - model_name: o4-mini
      litellm_params:
          model: openrouter/openai/o4-mini
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
      model_info:
          supports_reasoning: True
    - model_name: gpt-4.1-2025-04-14
      litellm_params:
          model: openai/gpt-4.1-2025-04-14
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
    - model_name: gpt-4.1
      litellm_params:
          model: openai/gpt-4.1
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
    - model_name: gpt-4.1-mini-2025-04-14
      litellm_params:
          model: openai/gpt-4.1-mini-2025-04-14
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
    - model_name: claude-3.7-sonnet:thinking
      litellm_params:
          model: openrouter/anthropic/claude-3.7-sonnet:thinking
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
      model_info:
          supports_reasoning: True          
    - model_name: gemini-2.5-pro-preview-03-25
      litellm_params:
          model: openrouter/google/gemini-2.5-pro-preview-03-25
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY
      model_info:
          supports_reasoning: True          
    - model_name: o3-mini-high
      litellm_params:
          model: openrouter/openai/o3-mini-high
          api_base: LITELLM_BASE_URL
          api_key: LITELLM_MASTER_KEY      model_info:
          supports_reasoning: True
