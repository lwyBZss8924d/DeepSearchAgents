# --- s.jina.ai API Examp Request & Response ---


**Example:**

**Examp Request:**

```bash
curl "https://s.jina.ai/?q=Jina+AI+embedding-v4+news+and+summary&gl=US&hl=en" \
  -H "Accept: application/json" \
  -H "Authorization: Bearer $JINA_API_KEY" \
  -H "X-Engine: browser" \
  -H "X-Retain-Images: none" \
  -H "X-Return-Format: markdown" \
  -H "X-Timeout: 1000"
```

**Examp Response:**

```
{
  "code": 200,
  "status": 20000,
  "data": [
    {
      "title": "jina-embeddings-v4 - Search Foundation Models",
      "url": "https://jina.ai/models/jina-embeddings-v4",
      "description": "Universal embedding model for multimodal and multilingual retrieval",
      "date": "Jun 24, 2025",
      "content": "jina-embeddings-v4 - Search Foundation Models\n\n===============\n\n[](https://jina.ai/)\n\n_search_[News](https://jina.ai/news)[Models](https://jina.ai/models)Products _arrow\\_drop\\_down_ Company _arrow\\_drop\\_down_\n\n_language_\n\n [_login_ Log in](https://jina.ai/api-dashboard?login=true)\n\n_search_\n\n_sort_ Date _arrow\\_drop\\_down_\n\n \n\njina-embeddings-v4\n\njina-reranker-m0\n\nReaderLM-v2\n\njina-clip-v2\n\njina-embeddings-v3\n\njina-colbert-v2\n\nreader-lm-0.5b\n\nreader-lm-1.5b\n\njina-reranker-v2-base-multilingual\n\njina-clip-v1\n\njina-reranker-v1-tiny-en\n\njina-reranker-v1-turbo-en\n\njina-reranker-v1-base-en\n\njina-colbert-v1-en\n\njina-embeddings-v2-base-es\n\njina-embeddings-v2-base-code\n\njina-embeddings-v2-base-de\n\njina-embeddings-v2-base-zh\n\njina-embeddings-v2-base-en\n\njina-embedding-b-en-v1\n\n_copyright_\n\njina-embeddings-v4\n==================\n\nUniversal embedding model for multimodal and multilingual retrieval\n\n[Release Post _arrow\\_forward_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval)\n\nLicense\n\n_copyright_\n\nCC-BY-NC-4.0\n\nRelease Date\n\n_calendar\\_month_\n\n2025-06-24\n\nInput\n\n_abc_\n\nText\n\n_image_\n\nImage\n\n_picture\\_as\\_pdf_\n\nPDF\n\n_arrow\\_forward_\n\nOutput\n\n_more\\_horiz_\n\nVector\n\n_apps_\n\nMulti-Vector\n\nModel Details\n\nParameters: 3.8B\n\nInput Token Length: 32K\n\nInput Image Size: 768√ó28√ó28\n\nOutput Dimension: 2048\n\nLanguage Support\n\nüåç Multilingual support\n\nRelated Models\n\n_link_\n\njina-embeddings-v3\n\n_link_\n\njina-clip-v2\n\nTags\n\nmultimodal-embedding\n\ndocument retrieval\n\nmultilingual\n\nmulti-vector\n\nlong-context\n\nproduction\n\nmatryoshka\n\nAvailable via\n\n[Jina API](https://jina.ai/?sui&model=jina-embeddings-v4)[Commercial License](https://jina.ai/api-dashboard/license-config?login=true)[AWS SageMaker](https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy)[Microsoft Azure](https://azuremarketplace.microsoft.com/en-US/marketplace/apps?page=1&search=jina)[Hugging Face](https://huggingface.co/jinaai/jina-embeddings-v4)\n\nI/O graph 1\n\nI/O graph 2\n\nI/O graph 3\n\nI/O graph 4\n\nChoose models to compare \n\njina-embeddings-v4\n\njina-embeddings-v3\n\njina-clip-v2\n\n_cancel_\n\n_arrow\\_drop\\_down_\n\n_compare\\_arrows_ Compare\n\n \n\nPublications (1)\n\n[arXiv June 24, 2025 jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)\n\n#### Overview\n\nJina Embeddings V4 is a 3.8 billion parameter multimodal embedding model that provides unified text and image representation capabilities. Built on the Qwen2.5-VL-3B-Instruct backbone, the model features an architecture that supports both single-vector and multi-vector embeddings in the late interaction style, addressing limitations found in traditional CLIP-style dual-encoder models. The model incorporates three specialized task-specific LoRA adapters (60M parameters each) that optimize performance across different retrieval scenarios including asymmetric query-document retrieval, semantic text similarity, and code search without modifying the frozen backbone weights. The model demonstrates strong performance in processing visually rich content such as tables, charts, diagrams, screenshots, and mixed-media formats through a unified processing pathway that reduces the modality gap present in conventional architectures. Supporting multilingual capabilities, the model can handle input texts up to 32,768 tokens with images resized to 20 megapixels, making it suitable for various document retrieval and cross-modal search applications across different languages and domains.\n\n#### Methods\n\nJina Embeddings V4 implements a unified multimodal language model architecture that differs from CLIP-style dual-encoder approaches. The model processes inputs through a shared pathway where images are first converted to token sequences via a vision encoder, then both text and image modalities are processed together by the language model decoder with contextual attention layers. This architecture supports two output modes to accommodate different use cases: single-vector embeddings that produce 2048-dimensional vectors truncatable down to 128 dimensions through Matryoshka Representation Learning, generated via mean pooling for efficient similarity search; and multi-vector embeddings that output 128 dimensions per token via projection layers for late interaction style retrieval. The model includes three task-specific LoRA adapters that provide specialized optimization: the retrieval adapter uses prefix-based asymmetric encoding with hard negatives training for query-document scenarios, the text-matching adapter employs CoSENT loss for semantic similarity tasks, and the code adapter focuses on natural language-to-code retrieval applications. Training occurs in two phases: initial pair training using contrastive InfoNCE loss with both text-text and text-image pairs from over 300 sources, followed by task-specific fine-tuning of the three LoRA adapters using triplet-based methods and specialized loss functions tailored to each domain's requirements.\n\n#### Performance\n\nJina Embeddings V4 achieves competitive performance across multiple benchmark categories. On visual document retrieval, it scores 72.19 average on the JinaVDR benchmark compared to 64.50 for ColPali-v1.2, and 84.11 average on ViDoRe compared to 83.90 for ColPali, with the multi-vector mode reaching 90.17 on ViDoRe. For cross-modal retrieval, the model scores 84.11 on CLIP Benchmark, compared to jina-clip-v2 (81.12) and nllb-clip-large-siglip (83.19). In text retrieval tasks, it achieves 55.97 on MTEB-en and 66.49 on MMTEB, with notable performance in long document processing at 67.11 on LongEmbed compared to 55.66 for its predecessor. The model demonstrates solid semantic text similarity performance with 85.89 on English STS tasks and 72.70 on multilingual STS benchmarks. Code retrieval capabilities reach 71.59 on CoIR benchmark, though specialized models like voyage-code-3 (77.33) achieve higher scores in this domain. The model shows improved cross-modal alignment with a score of 0.71 compared to 0.15 for OpenAI CLIP, addressing the modality gap issue in multimodal models. Multi-vector mode consistently outperforms single-vector mode on visually rich tasks, while single-vector mode provides efficient performance for standard retrieval scenarios.\n\n#### Best Practice\n\nTo effectively utilize Jina Embeddings V4, select the appropriate LoRA adapter based on your specific application requirements. Use the 'retrieval' adapter for asymmetric query-document retrieval scenarios where queries and documents have different structures, ensuring proper prefixes are applied to distinguish between query and passage content. The 'text-matching' adapter is suitable for semantic similarity tasks and symmetric retrieval where the goal is to find similar content rather than answers to queries, making it appropriate for document clustering, duplicate detection, and content recommendation systems. For programming-related applications, the 'code' adapter is optimized for natural language-to-code retrieval, code-to-code similarity search, and technical question answering scenarios. Choose output modes based on your performance and efficiency requirements: single-vector embeddings offer efficient similarity search and are suitable for storage-constrained environments, with truncatable dimensions allowing reduction from 2048 to 128-512 dimensions with acceptable quality trade-offs, while multi-vector embeddings provide higher precision for complex retrieval tasks, particularly when working with visually rich documents where late interaction scoring captures detailed relationships. The model's unified architecture allows processing of mixed text-image inputs without requiring separate encoders or OCR preprocessing for visual documents. The model's cross-modal alignment capabilities and multilingual support make it suitable for international applications. For production deployments, consider the 60M parameter overhead per LoRA adapter when planning memory requirements, noting that all three adapters can be maintained simultaneously with less than 2% additional memory footprint, enabling flexible task switching during inference.\n\nBlogs that mention this model\n\n[June 30, 2025 ‚Ä¢ 8 minutes read Quantization-Aware Training of jina-embeddings-v4 Quantization gives smaller embeddings. We show you fine-tuned quantization gives you even lossless embeddings.](https://jina.ai/news/quantization-aware-training-of-jina-embeddings-v4)[June 25, 2025 ‚Ä¢ 12 minutes read Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval)[March 07, 2025 ‚Ä¢ 14 minutes read Long-Context Embedding Models are Blind Beyond 4K Tokens We investigate embedding models on new \"needle-in-haystack\" tasks and find that beyond 4K tokens, they're just rolling dice - even with exact lexical matches or query expansion, they can't tell signal from noise in long context.](https://jina.ai/news/long-context-embedding-models-are-blind-beyond-4k-tokens)[January 22, 2025 ‚Ä¢ 10 minutes read What Should We Learn From ModernBERT? Bigger training data, efficient parameter sizing, and a deep-but-thin architecture, ModernBERT sets a direction for future BERT-like models.](https://jina.ai/news/what-should-we-learn-from-modernbert)\n\n1\n\nOffices\n\n_location\\_on_\n\nSunnyvale, CA\n\n710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, USA\n\n_location\\_on_\n\nBerlin, Germany (HQ)\n\nPrinzessinnenstra√üe 19-20, 10969 Berlin, Germany\n\n_location\\_on_\n\nBeijing, China\n\nLevel 5, Building 6, No.48 Haidian West St. Beijing, China\n\n_location\\_on_\n\nShenzhen, China\n\n402 Floor 4, Fu'an Technology Building, Shenzhen, China\n\nSearch Foundation\n\n[Reader](https://jina.ai/reader)[Embeddings](https://jina.ai/embeddings)[Reranker](https://jina.ai/reranker)[DeepSearch](https://jina.ai/deepsearch)[Classifier](https://jina.ai/classifier)[Segmenter](https://jina.ai/segmenter)[API Documentation](https://docs.jina.ai/)\n\nGet Jina API key\n\n[Rate Limit](https://jina.ai/contact-sales#rate-limit)[API Status](https://status.jina.ai/)\n\nCompany\n\n[About us](https://jina.ai/about-us)[Contact sales](https://jina.ai/contact-sales)[Newsroom](https://jina.ai/news)[Intern program](https://jina.ai/internship)[Join us _open\\_in\\_new_](https://app.dover.com/jobs/jinaai)[Download logo _open\\_in\\_new_](https://jina.ai/logo-Jina-1024.zip)\n\nTerms\n\n[Security](https://jina.ai/legal#security-as-company-value)[Terms & Conditions](https://jina.ai/legal/#terms-and-conditions)[Privacy](https://jina.ai/legal/#privacy-policy)[Manage Cookies](javascript:UC_UI.showSecondLayer();)[](https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva)\n\n[](https://x.com/jinaAI_)[](https://www.linkedin.com/company/jinaai/)[](https://github.com/jina-ai)[](https://huggingface.co/jinaai)[](https://discord.jina.ai/)[_email_](mailto:support@jina.ai)\n\n Jina AI ¬© 2020-2025.",
      "publishedTime": "Fri, 04 Jul 2025 08:19:15 GMT",
      "metadata": {
        "title": "jina-embeddings-v4 - Search Foundation Models",
        "description": "Universal embedding model for multimodal and multilingual retrieval",
        "og:type": "website",
        "og:url": "https://jina.ai/models/jina-embeddings-v4",
        "og:title": "jina-embeddings-v4 - Search Foundation Models",
        "og:description": "Universal embedding model for multimodal and multilingual retrieval",
        "og:image": "https://jina.ai/models-banner/jina-embeddings-v4.png",
        "twitter:site": "@JinaAI_",
        "twitter:creator": "@JinaAI_",
        "twitter:card": "summary_large_image",
        "twitter:url": "https://jina.ai/models/jina-embeddings-v4",
        "twitter:title": "jina-embeddings-v4 - Search Foundation Models",
        "twitter:description": "Universal embedding model for multimodal and multilingual retrieval",
        "twitter:image": "https://jina.ai/models-banner/jina-embeddings-v4.png",
        "format-detection": "telephone=no",
        "msapplication-tap-highlight": "no",
        "viewport": "user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width",
        "author": "Jina AI",
        "twitter:label1": "Created by",
        "twitter:data1": "Jina AI"
      },
      "external": {
        "icon": {
          "https://jina.ai/icons/favicon-128x128.png": {
            "type": "image/png",
            "sizes": "128x128"
          },
          "https://jina.ai/icons/favicon-96x96.png": {
            "type": "image/png",
            "sizes": "96x96"
          },
          "https://jina.ai/icons/favicon-32x32.png": {
            "type": "image/png",
            "sizes": "32x32"
          },
          "https://jina.ai/icons/favicon-16x16.png": {
            "type": "image/png",
            "sizes": "16x16"
          },
          "https://jina.ai/favicon.ico": {
            "type": "image/ico"
          }
        },
        "apple-touch-startup-image": {
          "https://jina.ai/icons/apple-launch-1284x2778.png": {
            "media": "(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1170x2532.png": {
            "media": "(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-828x1792.png": {
            "media": "(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1125x2436.png": {
            "media": "(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1242x2688.png": {
            "media": "(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-750x1334.png": {
            "media": "(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1242x2208.png": {
            "media": "(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1620x2160.png": {
            "media": "(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1536x2048.png": {
            "media": "(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1668x2224.png": {
            "media": "(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1668x2388.png": {
            "media": "(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-2048x2732.png": {
            "media": "(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)"
          }
        },
        "modulepreload": {
          "https://jina.ai/assets/i18n-Dclf6KX4.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/dynamic-import-helper-BheWnx7M.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/index-Cg0dwsIc.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/register-PjUQza5y.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QTooltip-DcW-JGea.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/position-engine-CJBGmbEO.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/copy-to-clipboard-C58qSYsr.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/MainLayout-jrRgh55W.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/use-dialog-plugin-component-llHznHfT.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QBadge-CtHAmTAj.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/UserAvatarComponent-Cw_a8SvV.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QItemLabel-94hRcmuR.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QChip-B0j4JtDd.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QBtnDropdown-9xDa1KvQ.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QMenu-DiCh6933.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QList-CKwY99Um.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QLinearProgress-Ltdz_u2L.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QLayout-DJu7SXu4.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QResizeObserver-DNuaGZ1h.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QScrollObserver-CLJ1yQQ6.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/TouchPan-Bk1lhG68.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/touch-BjYP5sR0.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QExpansionItem-DuBr-1Qn.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QSpinnerRings-0rGENIKC.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/blogs-BoL5prqi.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/ClosePopup-B0Jf2AHc.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/search-BlN7aDv7.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/VideoDialog-BEUxr2nw.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/useRoute-CU2IaybH.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/ModelsPage-CVMoHnue.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QSpinnerDots-Dxuzzmwv.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QBanner-DwarftYB.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QPage-D8ptDSve.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/useModels-BydqC_uw.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/date-B5TnbI6_.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/use-fullscreen-BZCjD5Lt.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/orderBy-B3XeSi2D.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/isSymbol-CPOlG43U.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/H2TitleBlock-BOrV6xhH.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/ResearchersComponent-D0mFJ7-5.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/NewsFilterComponent-DIBJlZLQ.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/NewsVerticalCard-D9VEoEeZ.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/SXTooltip-DuoGIE6P.js": {
            "as": "script",
            "crossorigin": ""
          }
        },
        "preload": {
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/WebSdk.lib.bb0442d7.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/GdprCmpController.3cf0d250.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/UcGdprCmpView.bc665a94.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/sections.8a9daccf.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/secondLayer.383e56e4.js": {
            "as": "script"
          }
        }
      },
      "usage": {
        "tokens": 2616
      }
    },
    {
      "title": "Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval",
      "url": "https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
      "description": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
      "content": "Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval\n\n===============\n\n[](https://jina.ai/)\n\n_search_[News](https://jina.ai/news)[Models](https://jina.ai/models)Products _arrow\\_drop\\_down_ Company _arrow\\_drop\\_down_\n\n_language_\n\n [_login_ Log in](https://jina.ai/api-dashboard?login=true)\n\n_star_\n\nFeatured\n\nPress release\n\nJune 25, 2025\n\nJina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval\n==============================================================================\n\nJina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.\n\nJina AI ‚Ä¢ 12 minutes read\n\n[jina-embeddings-v4 - Search Foundation Models Universal embedding model for multimodal and multilingual retrieval Search Foundation Models Jina AI](https://jina.ai/models/jina-embeddings-v4)[jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval. arXiv.org Michael G√ºnther](https://arxiv.org/abs/2506.18902)[jinaai/jina-embeddings-v4 ¬∑ Hugging Face We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.](https://huggingface.co/jinaai/jina-embeddings-v4)\nToday we're releasing [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4), our new 3.8 billion parameter universal embedding model for text and images. It includes a set of task-specific LoRA adapters that optimize performance for the most popular retrieval tasks, including query-document retrieval, semantic matching, and code search. [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) achieves state-of-the-art retrieval performance on multimodal and multilingual tasks across MTEB, MMTEB, CoIR, LongEmbed, STS, [Jina-VDR](https://github.com/jina-ai/jina-vdr), CLIP, and ViDoRe benchmarks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixture of them. The model supports both single-vector and multi-vector embeddings.\n\nPerformance of [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) across visual document retrieval and multimodal benchmarks. The boxplot distributions show mean scores and performance variability for embedding models across six benchmark categories: ViDoRe (vision document retrieval), Jina-VDR (comprehensive visual document retrieval), Wikimedia Commons Retrieval (multilingual document-description matching), GitHub README Retrieval (code documentation retrieval), Tweet Stock Retrieval (financial chart analysis), and CLIP Benchmark (general text-to-image retrieval). Jina-embeddings-v4 variants (highlighted in cyan) demonstrate state-of-the-art performance across visually rich document tasks, with the multi-vector version achieving the highest scores in specialized visual document benchmarks (90.2 on ViDoRe, 80.2 on Jina-VDR), while maintaining competitive performance on general multimodal retrieval tasks (84.1 on CLIP Benchmark). Models are ranked by mean performance within each benchmark category, with individual data points showing score distributions across multiple evaluation tasks.\n\n[jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) is our most ambitious embedding model yet. As an open-source model, [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) outperforms leading closed-source embedding models from major providers, delivering 12% better performance than OpenAI's `text-embedding-3-large` on multilingual retrieval (66.49 vs 59.27), 28% improvement on long document tasks (67.11 vs 52.42), 15% better than `voyage-3` on code retrieval (71.59 vs 67.23), and matching Google's `gemini-embedding-001` performance. This makes v4 the most capable open-source universal embedding model available today, offering researchers and developers enterprise-grade multimodal embedding capabilities with full transparency into training process, architectural decisions, and model weights through [our comprehensive technical report.](https://arxiv.org/abs/2506.18902)\n\nPerformance of [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) across five retrieval benchmarks. The chart shows boxplot distributions with mean scores for each model across Text Retrieval, Code Retrieval, Multilingual Retrieval, Long Context Retrieval, and Semantic Textual Similarity (STS) benchmarks. [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) (highlighted in cyan) demonstrates competitive or state-of-the-art performance across all evaluation categories, with particularly strong results in text retrieval and STS. Models are ranked by mean performance within each benchmark category, with individual data points showing score distributions across multiple evaluation tasks.\n\n[_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#new-architecture \"New Architecture\")New Architecture\n---------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nArchitecture of [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4). The model is built on the `Qwen2.5-VL-3B-Instruct` backbone (3.8B parameters). Text and image inputs are processed through a shared pathway: images are first converted to token sequences via a vision encoder, then both modalities are jointly processed by the language model decoder with contextual attention layers. Three task-specific LoRA adapters (60M parameters each) provide specialized optimization for retrieval, text-matching, and code tasks without modifying the frozen backbone weights. The architecture supports dual output modes: (1) single-vector embeddings (2048 dimensions, truncatable to 128) generated via mean pooling for efficient similarity search, and (2) multi-vector embeddings (128 dimensions per token) via projection layers for late interaction retrieval strategies.\n\nThe upgrade from [jina-embeddings-v3](https://jina.ai/?sui&model=jina-embeddings-v3) to [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) represents a paradigm shift from text-only to multimodal embeddings. While v3 focused on optimizing text embeddings with task-specific LoRA adapters, v4 addresses the growing requirement for embedding both textual and visual content in unified representations.\n\n| **Aspect** | [jina-embeddings-v3](https://jina.ai/?sui&model=jina-embeddings-v3) | [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) |\n| --- | --- | --- |\n| Backbone Model | jina-XLM-RoBERTa | Qwen2.5-VL-3B-Instruct |\n| Parameters (Base) | 559M | 3.8B |\n| Parameters (with adapters) | 572M | 3.8B + 60M per adapter |\n| Modalities | Text only | Text + Images (multimodal) |\n| Max Input Length | 8,192 tokens | 32,768 tokens |\n| Image Processing | None | Up to 20 megapixels, visually rich documents |\n| Multilingual Support | 89 languages | 29+ languages |\n| Vector Types | Single-vector only | Single-vector + Multi-vector (late interaction) |\n| Single-vector Dimensions | 1024 (MRL truncatable to 32) | 2048 (MRL truncatable to 128) |\n| Multi-vector Dimensions | Not available | 128 per token |\n| Task LoRA Specializations | ‚Ä¢ Asymmetric retrieval ‚Ä¢ Semantic similarity ‚Ä¢ Classification ‚Ä¢ Separation | ‚Ä¢ Asymmetric retrieval ‚Ä¢ Semantic similarity ‚Ä¢ Code retrieval |\n| Training Stages | 3-stage: Pre-training ‚Üí Embedding fine-tuning ‚Üí Adapter training | 2-stage: Joint pair training ‚Üí Task-specific adapter training |\n| Loss Functions | InfoNCE, CoSent, Extended triplet loss | Joint InfoNCE + KL divergence for single/multi-vector |\n| Positional Encoding | RoPE (rotary base frequency tuning) | M-RoPE (Multimodal Rotary Position Embedding) |\n| Cross-modal Processing | N/A | Unified encoder (reduced modality gap) |\n| MRL Support | Yes | Yes |\n| Attention Implementation | FlashAttention2 | FlashAttention2 |\n\n### [_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#backbone \"Backbone\")Backbone\n\nThe most significant architectural change in v4 is the backbone change from `XLM-RoBERTa` to `Qwen2.5-VL-3B-Instruct`. This decision was driven by v4's core objective to create a universal embedding model that enables \"true multimodal processing\" where images are converted to token sequences and processed alongside text, eliminating the [modality gap](https://jina.ai/news/the-what-and-why-of-text-image-modality-gap-in-clip-models) present in dual-encoder architectures.\n\nThe backbone selection aligns with several key design goals: Qwen2.5-VL's excellence in document understanding directly supports v4's strength in processing visually rich content like tables, charts, and screenshots. The dynamic resolution capabilities enable v4 to handle images resized to 20 megapixels as specified in the architecture. The advanced positional encoding provides the foundation that allows v4 to achieve superior cross-modal alignment with a 0.71 alignment score compared to 0.15 for OpenAI CLIP.\n\n### [_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#lora-adapters \"LoRA Adapters\")LoRA Adapters\n\nV4 streamlines from v3's five tasks to three focused tasks, reflecting lessons learned about effectiveness and user adoption:\n\n*   **Asymmetric retrieval** (consolidating v3's query/passage adapters)\n*   **Symmetric similarity** (v3's text-matching equivalent for STS tasks)\n*   **Code retrieval** (learned from v2-code, missing in v3)\n\nThis consolidation removes v3's classification and separation adapters, focusing v4 on the most impactful embedding use cases - retrieval and STS.\n\n### [_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#output-embeddings \"Output Embeddings\")Output Embeddings\n\nV4 introduces a dual-output system supporting both single-vector and multi-vector embeddings, whereas v3 only provided single-vector outputs. This addresses different retrieval scenarios:\n\n*   **Single-vector mode**: 2048-dimensional embeddings (truncatable to 128 via MRL) for efficient similarity search\n*   **Multi-vector mode**: 128 dimensions per token for [late-interaction retrieval](https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search)\n\nThis dual approach provides greater effectiveness with multi-vector representations, particularly in visually rich document retrieval, while maintaining efficiency for standard similarity tasks. The consistent 7-10% performance advantage of multi-vector over single-vector mode across visual tasks suggests that late interaction provides fundamentally better semantic matching for multimodal content.\n\n### [_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#parameter-size \"Parameter Size\")Parameter Size\n\nWhile v4 is 6.7 times larger than v3 (3.8B vs 570M parameters), the text-only performance improvements are actually modest, suggesting the parameter scaling was primarily driven by multimodal requirements rather than text enhancement. On core text benchmarks, v4 achieves 66.49 on MMTEB compared to v3's 58.58 (14% improvement) and 55.97 on MTEB-EN versus v3's 54.33 (3% improvement). For code retrieval, v4 scores 71.59 on CoIR compared to v3's 55.07 (30% improvement), while long document performance shows v4 at 67.11 versus v3's 55.66 on LongEmbed (21% improvement). The substantial scaling becomes justified when considering v4's multimodal capabilities: achieving 84.11 nDCG@5 on visual document retrieval (Jina-VDR) and 90.17 on ViDoRe benchmarks - capabilities entirely absent in v3. The parameter increase thus represents our investment in multimodal functionality while maintaining competitive text performance, with the unified architecture eliminating the need for separate text and vision models while achieving 0.71 cross-modal alignment compared to 0.15 for traditional dual-encoder approaches.\n\n[_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#getting-started \"Getting Started\")Getting Started\n------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nFor a quick vibe-check, try our text-to-image demo in the Search Foundation toolbox. We've prepared a collection of document images from our website, and you can also add your own image URLs. Simply type your query and press enter to see ranked results. You may retreat it either like OCR or content-based image retrieval - also feel free to try queries in non-English.\n\n0:00\n\n /0:22\n\n1√ó\n\nThe demo is available at: [https://jina.ai/api-dashboard/m0-image-rerank](https://jina.ai/api-dashboard/m0-image-rerank) Please note that using this demo will consume your primary API key's tokens. Also the demo might seem a bit slow since it needs to download all images on the server from those URLs, and no cache is implemented for images.\n\n### [_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#via-api \"Via API\")Via API\n\nThe code below shows how to use [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4). You can pass a text string, a base64-encoded image, or an image URL. New users can get a Jina API key with 10 million free tokens.\n\n```bash\ncurl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer JINA_API_KEY\" \\\n  -d @- <<EOFEOF\n  {\n    \"model\": \"jina-embeddings-v4\",\n    \"task\": \"text-matching\",\n    \"input\": [\n        {\n            \"text\": \"A beautiful sunset over the beach\"\n        },\n        {\n            \"text\": \"Un beau coucher de soleil sur la plage\"\n        },\n        {\n            \"text\": \"Êµ∑Êª©‰∏äÁæé‰∏ΩÁöÑÊó•ËêΩ\"\n        },\n        {\n            \"text\": \"ÊµúËæ∫„Å´Ê≤à„ÇÄÁæé„Åó„ÅÑÂ§ïÊó•\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/nQNGqL0/beach1.jpg\"\n        },\n        {\n            \"image\": \"https://i.ibb.co/r5w8hG8/beach2.jpg\"\n        },\n        {\n            \"image\": \"iVBORw0KGgoAAAANSUhEUgAAABwAAAA4CAIAAABhUg/jAAAAMklEQVR4nO3MQREAMAgAoLkoFreTiSzhy4MARGe9bX99lEqlUqlUKpVKpVKpVCqVHksHaBwCA2cPf0cAAAAASUVORK5CYII=\"\n        }\n    ]\n  }\nEOFEOF\n```\n\nCopy\n\nDue to limited GPU resources, our Embedding API currently supports documents up to 8K tokens in length, despite [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4)'s native capability of handling up to 32K tokens. For applications requiring longer contexts beyond 8K tokens (such as [Late Chunking](https://jina.ai/news/what-late-chunking-really-is-and-what-its-not-part-ii)), we recommend deploying our models through CSPs or self-hosting the model.\n\n### [_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#via-csp-marketplaces \"Via CSP Marketplaces\")Via CSP Marketplaces\n\n[jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) will be soon available directly on AWS, Azure and GCP at the prices listed there.\n\n[AWS Marketplace: Jina Embeddings v4](https://aws.amazon.com/marketplace/pp/prodview-woadrus5knmjk)[Microsoft Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.jina-embeddings-v4?tab=Overview)\n### [_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#via-huggingface \"Via HuggingFace\")Via HuggingFace\n\nFor research and experiment purpose, you can use the model locally from our Hugging Face page. We've prepared a Google Colab notebook that demonstrates how it works.\n\n[Google Colab](https://colab.research.google.com/drive/1fb8jGCDPf-MXUnyXt-DNoe8_hmBDpDrl#scrollTo=M54aS0TvApyi)\n[_tag_](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval#conclusion \"Conclusion\")Conclusion\n---------------------------------------------------------------------------------------------------------------------------------------------\n\n[jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) represents our most significant leap yet‚Äîa 3.8 billion parameter universal embedding model that processes text and images through a unified pathway, supporting both dense and late-interaction retrieval while outperforming proprietary models from Google, OpenAI and Voyage AI especially on visually rich document retrieval. But this capability didn't emerge in isolation; it's the culmination of four generations of solving fundamental limitations.\n\nWhen we started with `jina-embeddings-v1` back in early 2022, everyone assumed more data meant better performance. We proved the opposite‚Äîfiltering 1.5 billion pairs down to 385 million high-quality examples outperformed much larger datasets. The lesson: curation beats collection.\n\n[Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating textual inputs into numerical representations, capturing the semantics of the text. These models excel in applications like dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of high-quality pairwise and triplet datasets. It underlines the crucial role of data cleaning in dataset preparation, offers in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Text Embedding Benchmark (MTEB). Furthermore, to increase the model‚Äôs awareness of grammatical negation, we construct a novel training and evaluation dataset of negated and non-negated statements, which we make publicly available to the community. arXiv.org Michael G√ºnther](https://arxiv.org/abs/2307.11224)\nBut users kept hitting BERT's 512-token wall. Training on longer sequences seemed expensive, until `jina-embeddings-v2` revealed an elegant solution: train short, deploy long. ALiBi's linear attention biases let models trained on 512 tokens seamlessly handle 8,192 tokens at inference. We got more capability for less compute.\n\n[Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency. To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only achieves state-of-the-art performance on a range of embedding-related tasks in the MTEB benchmark but also matches the performance of OpenAI‚Äôs proprietary ada-002 model. Additionally, our experiments indicate that an extended context can enhance performance in tasks such as NarrativeQA. arXiv.org Michael G√ºnther](https://arxiv.org/abs/2310.19923)\n`jina-embeddings-v2`'s success exposed another constraint‚Äîdifferent tasks needed different optimizations. Rather than building separate models, [jina-embeddings-v3](https://jina.ai/?sui&model=jina-embeddings-v3) used tiny 60M LoRA adapters to customize a 570M base model for any task. One model became five specialized models.\n\n[jina-embeddings-v3: Multilingual Embeddings With Task LoRA We introduce jina-embeddings-v3, a novel text embedding model with 570 million parameters, achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA) adapters to generate high-quality embeddings for query-document retrieval, clustering, classification, and text matching. Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, while achieving superior performance compared to multilingual-e5-large-instruct across all multilingual tasks. With a default output dimension of 1024, users can flexibly reduce the embedding dimensions to as low as 32 without compromising performance, enabled by Matryoshka Representation Learning. arXiv.org Saba Sturua](https://arxiv.org/abs/2409.10173)\nEven with task specialization, we remained text-only while users needed visual understanding. The standard CLIP based models like [jina-clip-v1](https://jina.ai/?sui&model=jina-clip-v1) and [jina-clip-v2](https://jina.ai/?sui&model=jina-clip-v2) uses separate encoders, creating a \"modality gap\" where similar content in different formats ends up far apart. Like our recently released [jina-reranker-m0](https://jina.ai/?sui&model=jina-reranker-m0), [jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4) eliminated this entirely‚Äîone unified pathway processes everything, removing the gap rather than bridging it.\n\n[jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-based information retrieval, cross-modal semantic similarity, and programming code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single- modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval. arXiv.org Michael G√ºnther](https://arxiv.org/abs/2506.18902)\nBoth[jina-embeddings-v4](https://jina.ai/?sui&model=jina-embeddings-v4)and[jina-reranker-m0](https://jina.ai/?sui&model=jina-reranker-m0)share a fundamental shift: using LLMs as backbones instead of encoder-only models. This isn't coincidental‚Äîit reflects a deep advantage most miss: Encoder-only models create \"modality gaps\" where images cluster separately from text. The decoder-only models opens up possibilities that weren't achievable with encoder-only architectures, including true mixed-modality representation and explainability.\n\nOur key insight: embeddings and generation are both about understanding semantics. LLMs that excel at generation naturally excel at representation. We believe the future lies in unified architectures where embedding and reranking emerge from **the same search foundation model**‚Äîand that's exactly what Jina AI is building toward.\n\nCategories:\n\n_star_\n\nFeatured\n\nPress release\n\n[](https://news.ycombinator.com/submitlink?u=https%3A%2F%2Fjina.ai%2Fnews%2Fjina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval%2F)[](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fjina.ai%2Fnews%2Fjina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval%2F)[](https://twitter.com/intent/tweet?url=https%3A%2F%2Fjina.ai%2Fnews%2Fjina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval%2F)[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fjina.ai%2Fnews%2Fjina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval%2F)[](https://reddit.com/submit?url=https%3A%2F%2Fjina.ai%2Fnews%2Fjina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval%2F)[_rss\\_feed_](https://jina.ai/feed.rss)\n\n* * *\n\nRead more\n\n[April 08, 2025 ‚Ä¢ 21 minutes read jina-reranker-m0: Multilingual Multimodal Document Reranker](https://jina.ai/news/jina-reranker-m0-multilingual-multimodal-document-reranker)[January 15, 2025 ‚Ä¢ 17 minutes read ReaderLM v2: Frontier Small Language Model for HTML to Markdown and JSON](https://jina.ai/news/readerlm-v2-frontier-small-language-model-for-html-to-markdown-and-json)[December 16, 2024 ‚Ä¢ 2 minutes read Re¬∑Search: Order 2024 Yearbook of Search Foundation Advances](https://jina.ai/news/re-search-order-2024-yearbook-of-search-foundation-advances)\n\nOffices\n\n_location\\_on_\n\nSunnyvale, CA\n\n710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, USA\n\n_location\\_on_\n\nBerlin, Germany (HQ)\n\nPrinzessinnenstra√üe 19-20, 10969 Berlin, Germany\n\n_location\\_on_\n\nBeijing, China\n\nLevel 5, Building 6, No.48 Haidian West St. Beijing, China\n\n_location\\_on_\n\nShenzhen, China\n\n402 Floor 4, Fu'an Technology Building, Shenzhen, China\n\nSearch Foundation\n\n[Reader](https://jina.ai/reader)[Embeddings](https://jina.ai/embeddings)[Reranker](https://jina.ai/reranker)[DeepSearch](https://jina.ai/deepsearch)[Classifier](https://jina.ai/classifier)[Segmenter](https://jina.ai/segmenter)[API Documentation](https://docs.jina.ai/)\n\nGet Jina API key\n\n[Rate Limit](https://jina.ai/contact-sales#rate-limit)[API Status](https://status.jina.ai/)\n\nCompany\n\n[About us](https://jina.ai/about-us)[Contact sales](https://jina.ai/contact-sales)[Newsroom](https://jina.ai/news)[Intern program](https://jina.ai/internship)[Join us _open\\_in\\_new_](https://app.dover.com/jobs/jinaai)[Download logo _open\\_in\\_new_](https://jina.ai/logo-Jina-1024.zip)\n\nTerms\n\n[Security](https://jina.ai/legal#security-as-company-value)[Terms & Conditions](https://jina.ai/legal/#terms-and-conditions)[Privacy](https://jina.ai/legal/#privacy-policy)[Manage Cookies](javascript:UC_UI.showSecondLayer();)[](https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva)\n\n[](https://x.com/jinaAI_)[](https://www.linkedin.com/company/jinaai/)[](https://github.com/jina-ai)[](https://huggingface.co/jinaai)[](https://discord.jina.ai/)[_email_](mailto:support@jina.ai)\n\n Jina AI ¬© 2020-2025.",
      "publishedTime": "2025-06-25T06:48:16.000+02:00",
      "metadata": {
        "title": "Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval",
        "description": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
        "og:type": "website",
        "og:url": "https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
        "og:title": "Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval",
        "og:description": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
        "og:image": "https://jina.ai/blog-banner/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval.webp",
        "twitter:site": "@JinaAI_",
        "twitter:creator": "@JinaAI_",
        "twitter:card": "summary_large_image",
        "twitter:url": "https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval",
        "twitter:title": "Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval",
        "twitter:description": "Jina Embeddings v4 is a 3.8 billion parameter universal embedding model for multimodal and multilingual retrieval that supports both single-vector and multi-vector embedding outputs.",
        "twitter:image": "https://jina.ai/blog-banner/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval.webp",
        "format-detection": "telephone=no",
        "msapplication-tap-highlight": "no",
        "viewport": "user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width",
        "author": "Jina AI",
        "twitter:label1": "Written by",
        "twitter:data1": "Jina AI",
        "twitter:label2": "Reading time",
        "twitter:data2": "12 mins read",
        "article:published_time": "2025-06-25T06:48:16.000+02:00",
        "article:modified_time": "2025-07-03T18:51:34.000+02:00"
      },
      "external": {
        "icon": {
          "https://jina.ai/icons/favicon-128x128.png": {
            "type": "image/png",
            "sizes": "128x128"
          },
          "https://jina.ai/icons/favicon-96x96.png": {
            "type": "image/png",
            "sizes": "96x96"
          },
          "https://jina.ai/icons/favicon-32x32.png": {
            "type": "image/png",
            "sizes": "32x32"
          },
          "https://jina.ai/icons/favicon-16x16.png": {
            "type": "image/png",
            "sizes": "16x16"
          },
          "https://jina.ai/favicon.ico": {
            "type": "image/ico"
          }
        },
        "apple-touch-startup-image": {
          "https://jina.ai/icons/apple-launch-1284x2778.png": {
            "media": "(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1170x2532.png": {
            "media": "(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-828x1792.png": {
            "media": "(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1125x2436.png": {
            "media": "(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1242x2688.png": {
            "media": "(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-750x1334.png": {
            "media": "(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1242x2208.png": {
            "media": "(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1620x2160.png": {
            "media": "(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1536x2048.png": {
            "media": "(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1668x2224.png": {
            "media": "(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1668x2388.png": {
            "media": "(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-2048x2732.png": {
            "media": "(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)"
          }
        },
        "modulepreload": {
          "https://jina.ai/assets/i18n-Bdzv5wkq.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/dynamic-import-helper-BheWnx7M.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/index-Cg0dwsIc.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/register-BVZZyrKh.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QTooltip-Bhb3YjpT.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/position-engine-C6jJfQWu.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/copy-to-clipboard-B2NwLaUR.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/MainLayout-iZRq4mDN.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/use-dialog-plugin-component-B0XQMl3F.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QBadge-COmWcOel.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/UserAvatarComponent-BgLGBt03.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QItemLabel-BpiRbV4W.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QChip-akpAcJA2.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QBtnDropdown-DCzAb9Oh.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QMenu-Cm1rPQ14.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QList-B9xrfd3b.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QLinearProgress-YROGvQfG.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QLayout-BuBW0QaA.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QResizeObserver-CQjNIUfW.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QScrollObserver-DtKL7UgA.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/TouchPan-DLCYdDWn.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/touch-BjYP5sR0.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QExpansionItem-Bp3L_jaT.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QSpinnerRings-mv8kdTDH.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/blogs-dG2W3-fu.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/ClosePopup-BV3m_dwK.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/search-De4nRjK2.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/VideoDialog-CilCHCHb.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/useRoute-CUxTLjTu.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/NewsPage-BRssGlk8.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QPage-Bf01fk3Y.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/NewsBadge--otySK25.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/SXTooltip-C2xxFrs6.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/NewsVerticalCard-CpXcJtUU.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/useModels-CxoY87wV.js": {
            "as": "script",
            "crossorigin": ""
          }
        },
        "preload": {
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/WebSdk.lib.bb0442d7.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/GdprCmpController.3cf0d250.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/UcGdprCmpView.bc665a94.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/sections.8a9daccf.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/secondLayer.383e56e4.js": {
            "as": "script"
          }
        }
      },
      "usage": {
        "tokens": 6524
      }
    },
    {
      "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
      "url": "https://arxiv.org/pdf/2506.18902",
      "description": "",
      "date": "Jun 23, 2025",
      "content": "> arXiv:2506.18902v2 [cs.AI] 24 Jun 2025\n\n# jina-embeddings-v4 : Universal Embeddings for Multimodal Multilingual Retrieval \n\nMichael G√ºnther ‚àó, Saba Sturua ‚àó, Mohammad Kalim Akram ‚àó,Isabelle Mohr ‚àó, Andrei Ungureanu ‚àó, Bo Wang ‚àó, Sedigheh Eslami , Scott Martens ,\n\nMaximilian Werk , Nan Wang and Han Xiao \n\nJina AI GmbH, Prinzessinnenstra√üe 19, 10969, Berlin, Germany \n\nresearch@jina.ai \n\nAbstract \n\nWe introduce jina-embeddings-v4 , a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incor-porates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text sim-ilarity, and code search. Comprehensive evalu-ations demonstrate that jina-embeddings-v4 \n\nachieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval. \n\n1 Introduction \n\nWe present jina-embeddings-v4 , a multimodal embedding model capable of processing text and image data to produce semantic embedding vectors of varying lengths, optimized for a broad array of applications. It incorporates optimized LoRA adapters [Hu et al., 2022] for information retrieval and semantic text similarity. An adapter is also provided for programming language embeddings, technical question-answering, and natural language code retrieval. It also brings new functionality to processing visually rich images (also called visual documents), i.e., materials mixing texts and images, containing tables, charts, diagrams, and other kinds of common mixed media [Ding et al., 2024]. We have also developed Jina-VDR, a new multilingual, multi-domain benchmark suite for a broad range of visual retrieval tasks, to evaluate the capabilities of jina-embeddings-v4 .\n\n> *Equal contribution.\n\nWe discuss the challenges of developing a multimodal, multi-functional, state-of-the-art embedding model capable of handling texts in a variety of languages, including computer coding languages, images, and ‚Äúvisually rich‚Äù data. The resulting model, jina-embeddings-v4 , projects inputs from all modalities into a unified semantic space, minimizing or eliminating the ‚Äúmodality gap‚Äù that has troubled similar projects [Liang et al., 2022a]. In addition, we introduce Jina-VDR, an advanced benchmark for images like screenshots and scans of visually complex documents. The major contributions of this work are as follows: ‚Ä¢ We introduce a unified multi-task learning paradigm that jointly optimizes embedding models to represent texts and images as single-and multi-vector embeddings. ‚Ä¢ Building on work done for \n\njina-embeddings-v3 , we train LoRA extensions to enhance support for specific domains and task types, achieving results comparable to specialized models. ‚Ä¢ We have made particularly strong progress in handling visually rich images, especially for tasks outside of the existing ViDoRe bench-mark [Faysse et al., 2025], which is limited to question-answering. jina-embeddings-v4 \n\noutperforms other multimodal models by a sig-nificant margin on this type of material and sup-ports a much more diverse set of use scenarios. ‚Ä¢ We construct a multilingual, multi-domain benchmark for screenshot retrieval. In contrast to other retrieval benchmarks (i.e., [Faysse et al., 2025, Xiao et al., 2025]) that focus on question answering and OCR-related tasks, we expand the scope of visual document benchmarking to multilingual retrieval, more query types, and a much more diverse array of materials, like maps, diagrams, advertisements, and other mixed media. \n\n2 Background \n\nThe underlying principles behind embedding mod-els are indifferent to data modality. An embedding model transforms digitally encoded objects into vectors in a high-dimensional embedding space such that some of the semantic features of the objects, depending on the model‚Äôs training regimen, correspond to subspaces in that embedding space. Objects with more such features in common will have corresponding vectors that are closer to each other by some metric (typically cosine similarity) than objects with fewer common features. Individual models, however, only support the modalities for which they are designed and trained. Embedding models initially developed primarily to support natural language texts, like \n\njina-embeddings-v3 [Sturua et al., 2024], but there are many image embedding models, and more recently, audio and video models. The semantic embedding paradigm can also encompass models that support more than one modality, like bimodal image-text models, including OpenAI‚Äôs CLIP [Rad-ford et al., 2021] and subsequent developments including jina-clip [Koukounas et al., 2024]. The principal purpose of multimodal embedding models is to project objects from multiple modalities into the same semantic embedding space, so that, for example, a picture of a cat and a text discussing or describing a cat will correspond to relatively close embedding vectors. Embedding models can also specialize in specific types of input within a single modality. There are text embedding models designed for programming code [Liu et al., 2024], legal texts [Voyage AI, 2024], and other special domains. There is also recent work in specialized image embedding models designed to support ‚Äúvisually rich‚Äù data, such as screenshots, charts, and printed pages that combine text and imagery and have internal visual structure [Faysse et al., 2025, Ma et al., 2024]. There are other dimensions of embedding model specialization as well. Models can be optimized for specific tasks, such as information retrieval, clus-tering, and classification [Sturua et al., 2024]. They can also vary based on the nature of the embeddings they produce. Most are single-/dense vector models, generating one embedding vector for whatever input they are given. There are also multi-vector/late interaction models, such as ColBERT [Khattab and Zaharia, 2020] and ColPali [Faysse et al., 2025]. Late interaction is generally a more precise measure of semantic similarity for retrieval, but has significantly greater storage and computing costs. Instead of specializing, jina-embeddings-v4 \n\nbuilds on a single base model to provide competitive performance as a text, image, and cross-modal em-bedding model with strong performance handling visually rich documents. It supports both single-vector and multi-vector and is optimized to provide embeddings of varying lengths. Furthermore, the model includes LoRA extensions that optimize it for specific application classes: information retrieval, multimodal semantic similarity, and computer code retrieval. This single-model approach entails significant savings in practical use cases when compared to deploying multiple AI models for different tasks and modalities. \n\n3 Related Work \n\nTransformer-based neural network architectures that generate semantic embeddings are well-established [Reimers and Gurevych, 2019], and there is a sizable literature on training techniques for them. Multi-stage contrastive training [Wang et al., 2022], and techniques for supporting longer texts [G√ºnther et al., 2023] are particularly relevant to this work. Compact embedding vectors bring valuable performance benefits to AI applications, and this motivates work in Matryoshka Representational Learning (MRL) [Kusupati et al., 2022] as a way to train models for truncatable embedding vectors. Contrastive text-image training has led to ground-breaking results in zero-shot image classification and cross-modal retrieval in conjunction with dual encoder architectures like CLIP [Radford et al., 2021]. However, recent work shows better performance from vision-language models (VLMs) like Qwen2.5-VL-3B-Instruct [Bai et al., 2025]. Jiang et al. [2024] show that VLMs suffer less from a modality gap than dual encoder architectures. In contrast to [Zhang et al., 2024], \n\njina-embeddings-v4 is trained on multilin-gual data, supports single as well as multi-vector retrieval, and does not require task-specific instruc-tions. Other VLM models are trained exclusively on data for text-to-image [Faysse et al., 2025, Ma et al., 2024] or text-to-text retrieval [Jiang et al., 2024]. Similarity scoring in late interaction models does not use simple cosine similarity. [Khattab and Zaharia, 2020] Instead, similarity is calculated asymmetrically over two sequences of token embed-dings ‚Äî a query and a document ‚Äî by summing up the maximum cosine similarity values of each query token embedding to any of the token embeddings from the document. Thus, for query embedding q\n\nand document embedding p, their late interaction similarity score slate (q,p ) is determined by: \n\nslate (q,p ) = \n\n> n\n\nX\n\n> i=1\n\nmax \n\n> j‚àà{ 1,...,m }\n\nqi ¬∑pT \n\n> j\n\n(1) Faysse et al. [2025] train a late-interaction embed-ding model to search document screenshots using text queries, performing significantly better than traditional approaches involving OCR and CLIP-style models trained on image captions. To show this, they introduce the ViDoRe (Vision Document Retrieval) benchmark. However, this benchmark is limited to question-answering tasks in English and French involving only charts, tables, and pages from PDF documents. Xiao et al. [2025] extend this benchmark to create MIEB (Massive Image Em-bedding Benchmark) by adding semantic textual similarity (STS) tasks for visually rich documents like screenshots. \n\n4 Model Architecture \n\nThe architecture of jina-embeddings-v4 ,schematized in Figure 1, employs a uni-fied multimodal language model built on the \n\nQwen2.5-VL-3B-Instruct 1 backbone [Bai et al., 2025]. Text and image inputs are processed through a shared pathway: Images are first converted to token sequences via a vision encoder, then both modalities are jointly processed by the language model decoder with contextual attention layers. This unified design eliminates the modality gap present in dual-encoder architectures while maintaining competitive performance across text, image, and cross-modal tasks. As shown in Figure 1, this architecture supports dual output modes, as outlined in Section 4.2. Fur-thermore, three task-specific LoRA adapters, each with 60M parameters, provide specialized task op-timization without modifying the frozen backbone weights. These are described in Section 4.3.            \n\n> 1https://huggingface .co/Qwen/Qwen2 .5-VL-3B-Instruct Model Parameters 3.8 billion ( 3.8√ó10 9) plus 60M per LoRA\n> Text Input Size Up to 32,768 tokens\n> Image Input All images resized to 20 megapixels\n> Single-vector Embedding Size 2048 dimensions, truncatable down to 128\n> Multi-vector Embedding Size 128 dimensions per token\n> Table 1: Basic specifications of jina-embeddings-v4\n\nThe core specifications of jina-embeddings-v4 \n\nare summarized in Table 1. \n\n4.1 True Multimodal Processing \n\nThe Qwen2.5-VL-3B-Instruct paradigm differs from CLIP-style dual-encoder models in offering a single processing path that‚Äôs truly multimodal. For text input, jina-embeddings-v4 and \n\nQwen2.5-VL-3B-Instruct initially behave like other transformer-based embedding models: The text is tokenized, each token is replaced with a vector representation from a lookup table, and then these vectors are stacked and presented to a large language model (LLM). In CLIP-style models, images are processed by a separate embedding model, typically atransformer-based model that divides them into patches and then processes them much like a text model. The text model and image model are aligned during training to produce similar embeddings for similar semantic content in the different media. The Qwen2.5-VL-3B-Instruct paradigm used in jina-embeddings-v4 also includes a discrete image model but uses it in a different way. It pro-duces a multi-vector result, comparable to late in-teraction models, and then passes this output to the LLM, as if it were a sequence of vectorized text to-kens. The image embedding model acts as a prepro-cessor for the LLM, converting the image into what amounts to a sequence of vectorized ‚Äúimage tokens.‚Äù This approach, which is the core of \n\njina-embeddings-v4 , beyond having perfor-mance advantages, makes it possible to pass a text prompt into the LLM along with an image. The VLM is truly multimodal, since it is one model supporting multiple data types in a single input field. \n\n4.2 Dual Mode Output \n\nIn contrast to Qwen2.5-VL-3B-Instruct and other embedding models in general, users can choose be-tween two output options: Traditional single (dense) vision encoder \n\nqwen2.5 LM decoder        \n\n> task= 'retrieval' doc= image OR text vector_type= 'multi_vector' input\n> output\n> Lora set base model\n> [retrieval] / [text-\n> matching] / [code search]\n> token embeddings mean pooling projector\n> single-vector\n> 128 to 2048-dim\n> multi-vector\n> N x 128-dim\n\nFigure 1: Architecture of jina-embeddings-v4 . The model employs a unified LM built on the Qwen2.5-VL-3B-Instruct backbone (3.8B parameters). Text and image inputs are processed through a shared pathway: images are first converted to token sequences via a vision encoder, then both modalities are jointly processed by the language model decoder with contextual attention layers. Three task-specific LoRA adapters (60M parameters each) provide specialized optimization for retrieval, text-matching, and code search tasks without modifying the frozen backbone weights. The architecture supports dual output modes: (1) single-vector embeddings (2048 dimensions, truncatable to 128) generated via mean pooling for efficient similarity search, and (2) multi-vector embeddings (128 dimensions per token) via projection layers for the late interaction style retrieval. \n\nvector embeddings and ColBERT-style multi-vector embeddings for late interaction strategies. Single-vector embeddings are 2048 dimensions, but can be truncated to as little as 128 with minimal loss of precision. jina-embeddings-v4 has been trained with Matryoshka Representation Learn-ing [Kusupati et al., 2022], so that the scalar values of single-vector embeddings are roughly ordered by semantic significance. Eliminating the least significant dimensions reduces precision very little. Multi-vector embeddings are the unpooled result of processing tokens through a transformer model. They correspond to tokens as the model analyses them, given their context. The length of the output vector is proportionate to the number of input tokens (including ‚Äúimage tokens‚Äù), with each token corresponding to a 128-dimensional output vector. This output is directly comparable to the unpooled embeddings produced by ColBERT [Khattab and Zaharia, 2020] and ColPali [Faysse et al., 2025] and is intended for use in late interaction comparison strategies. For single-vector embeddings, mean pooling is applied to the final layer of the base model to produce the output. The model incorporates an additional layer to project the output of the base model into multi-vector outputs. \n\n4.3 Task Specialization with LoRA \n\nFollowing the methods used for \n\njina-embeddings-v3 [Sturua et al., 2024], we have implemented three task-specific LoRA adapters for different information retrieval use cases: ‚Ä¢ Asymmetric Query-Document Retrieval ‚Ä¢ Semantic Similarity and Symmetric Retrieval ‚Ä¢ Code (i.e., computer programming language) Retrieval Asymmetric retrieval means encoding queries and documents differently in order to improve retrieval for queries that are not structured like documents, i.e., short queries, questions, etc. This is in contrast to symmetric retrieval, which assumes a symmetry between query and document, and is used to find comparable content. Each LoRA adapter set has only 60M parameters, so maintaining all three adds less than 2% to the memory footprint of jina-embeddings-v4 . Users can select among them at inference time, and all three support image and text encoding. See Section 7 for performance information about these adapters. \n\n5 Training Method \n\nBefore training, model weights are initialized to match Qwen/Qwen2.5-VL-3B-Instruct . The multi-vector projection layer and LoRA adapters are randomly initialized. The weights of the backbone model are not modified during the training process. The LoRA adapters modify the effect of the backbone model layers and the projection layer. Only the adapters are trained. Training proceeds in two phases: 1. A single LoRA adapter is trained using contrasting text pairs and text-image pairs. We use the contrastive InfoNCE [van den Oord et al., 2018] loss function to co-train for both single-vector and multi-vector similarity, as detailed in the section below. No task-specific training is performed at this stage. 2. The resulting LoRA adapter is duplicated to create the three task-specific adapters, which are then trained individually with task-specific text triplets and text-image triplets. In both phases of training, we apply Ma-tryoshka loss [Kusupati et al., 2022] to the base loss so that single-vector embeddings from \n\njina-embeddings-v4 are truncatable. \n\n5.1 Pair Training \n\nInitially, training is performed with a contrastive objective. Pairs of inputs are classed as related or un-related, and the model learns to embed related items closely together and unrelated items further apart. In each training step, we sample two different batches of training data: ‚Ä¢ A batch Btext of text pairs. ‚Ä¢ A batch Bmulti of multimodal pairs containing a text and a related image. We generate normalized single-vector and multi-vector embeddings for all texts and images in the se-lected pairs. We then construct a matrix of similarity values Sdense (B) by calculating the cosine similarity of all combinations of single-vector embeddings qi\n\nand pj in B. We construct an analogous matrix Slate \n\nfor each B for the multi-vector embeddings using a slightly modified version of Equation (1) to cal-culate their similarity. Our choice of loss function requires a normalized score, so we divide the late in-teraction score by the number of tokens in the query: \n\ns‚Ä≤\n\n> late\n\n(qi,p j ) = slate (qi,p j )\n\nt (2) where t is the number of tokens in qi and qi,p j ‚àà B \n\nThis modification is only for training. For re-trieval applications, normalization is not necessary since the query is invariant. Then, we apply the contrastive InfoNCE loss function LNCE [van den Oord et al., 2018] on each of the four resulting matrices of similarity scores \n\nsi,j ‚àà S\n\nsoftmax( S,œÑ,i,j ) := ln esi,j /œÑ nP\n\n> k=0\n\nesi,k /œÑ \n\n(3) \n\nLNCE (S(B),œÑ ) := ‚àí\n\n> n\n\nX\n\n> i,j =0\n\nsoftmax( S(B),œÑ,i,i )\n\n(4) where œÑ is the temperature parameter, n is the batch size, which increases the weight of small dif-ferences in similarity scores in calculating the loss. Following Hinton et al. [2015], we compensate for differences in error distributions between the single-vector and multi-vector by adding the weighted Kullback‚ÄìLeibler divergence (DKL )of the two sets of softmax-normalized similarity scores. This enables us to train for the single-vector and multi-vector outputs simultaneously, even though the multi-vector/late interaction scores have much less error. \n\nLD(B,œÑ ) := DKL (S‚Ä≤\n\n> dense\n\n(B)‚à•S‚Ä≤\n\n> late\n\n(B)) \n\nwhere S‚Ä≤ \n\n> i,j\n\n= softmax( S,œÑ,i,j ) (5) The resulting joint loss function, which we use in training, is defined as: \n\nLjoint (Btxt ,Bmulti ,œÑ ) := \n\nw1LNCE (Sdense (Btxt ),œÑ )+w2LNCE (Slate (Btxt ),œÑ )+ w3LD(Btxt )+w4LNCE (Sdense (Bmulti ),œÑ )+w5LNCE (Slate (Bmulti ),œÑ )+ w6LD(Bmulti )\n\n(6) Task Name Description         \n\n> retrieval Asymmetric embedding of queries and documents for retrieval\n> text-matching Semantic text similarity and sym-metric retrieval\n> code Retrieving code snippets\n\nTable 2: Supported tasks of jina-embeddings-v4 , each corresponding to a LoRA adapter and trained indepen-dently \n\nThe weights w1, ... , w 6 and temperature œÑ are training hyperparameters. \n\n5.1.1 Pair Training Data \n\nThe training data consists of text-text and text-image pairs from more than 300 sources. Text-text pairs are selected and filtered as described in Sturua et al. [2024]. Text-image pairs have been curated from a variety of sources following a more eclectic strategy than previous work on training text-image embedding models. In contrast to relying on image-caption pairs or pairs of queries and images derived from documents, we have also created images from other document types. Our training data includes website screenshots, rendered Markdown files, charts, tables, and other kinds of materials \"found in the wild.\" The queries consist primarily of questions, keywords and key phrases, long descriptions, and statements of fact. \n\n5.2 Task-Specific Training \n\nWe instantiate three copies of the pair-trained LoRA adapter and give each specific training for its intended task. Training data and loss functions differ for the three tasks. \n\n5.2.1 Asymmetric Retrieval Adapter \n\nAsymmetric retrieval assigns substantially and qualitatively different embeddings to documents and queries, even if they happen to have the very same text. Having distinct encoding mechanisms for the two often significantly benefits embeddings-based retrieval performance. Sturua et al. [2024] shows that this can be achieved either by training two separate adapters or by employing two distinct prefixes as proposed in Wang et al. [2022], so that embedding models can readily distinguish them when they generate embeddings. We have used the prefix method for \n\njina-embeddings-v4 . Previous work shows little benefit from combining both methods. Our training data contains hard negatives, i.e., triplets of a query, a document that matches the query, and a document that is closely semantically related but not a correct match [Wang et al., 2022, Li et al., 2023, G√ºnther et al., 2023]. For every pair \n\n(qi,p i) ‚àà B in a batch, pi is intended to be a good match for qi, and we presume that for all (qj ,p j ) ‚àà B \n\nwhere jÃ∏ = i, pj is a bad match for qi.We incorporate those additional negatives into the training process via an extended version of the LNCE loss described in G√ºnther et al. [2023], denoted as LNCE+ , in our joint loss function Ljoint :\n\nLNCE+ (S(B),œÑ ) := \n\nX\n\n> r‚ààB\n\n\"\n\n‚àíln es(q,p )/œÑ kP\n\n> i=1\n\nh\n\nes(q,p i)/œÑ + mP\n\n> j=1\n\nes(q,n j,i )/œÑ \n\ni#\n\n(7) with r = ( q,p,n 1,...,n m), where (q,p ) is a pair in batch B and n1,...,n m and the other p ‚àà B .Our dataset of text hard negatives is similar to the data used to train jina-embeddings-v3 [Sturua et al., 2024]. We rely on existing datasets to create multimodal hard negatives for training, including Wiki-SS [Ma et al., 2024] and VDR multilingual 2, but also mined hard negatives from curated multimodal datasets. \n\n5.2.2 Text Matching Adapter \n\nSymmetric semantic similarity tasks require different training from asymmetric retrieval. We find that training data with ground truth similarity values works best for this kind of task. As discussed in Sturua et al. [2024], we use the CoSENT 3 loss function Lco from Li and Li [2024]: \n\nLco (S(B),œÑ ) := ln \n\nh\n\n1+ X\n\n> (q1,p 1),\n> (q2,p 2)\n> ‚ààS(B)\n\nes(q2,p 2) ‚àíes(q1,p 1)\n\nœÑ\n\ni\n\n(8) where Œ∂(q, p ) is the ground truth semantic similarity of q with p, Œ∂(q1,p 1) > Œ∂ (q2,p 2), and œÑ\n\nis the temperature parameter. \n\n> 2https://huggingface .co/datasets/llamaindex/ vdr-multilingual-train\n> 3https://github .com/bojone/CoSENT\n\nThe loss function operates on two pairs of text values, (q1, p 1) and (q2, p 2), with known ground truth similarity. To train the model with this objective, we use data from semantic textual similarity (STS) training datasets such as STS12 [Agirre et al., 2012] and SICK [Marelli et al., 2014]. The amount of data in this format is limited, so we enhance our ground truth training data with pairs that do not have known similarity scores. For these pairs, we proceed the same way as we did for pair training in Section 5.1 and use the standard InfoNCE loss from Equation (4) . The joint loss function is calculated as in Equation (7) except the CoSENT loss is used where pairs with known ground truth values exist. \n\n5.2.3 Code Adapter \n\nCode embeddings in jina-embeddings-v4 are designed for natural language-to-code retrieval, code-to-code similarity search, and technical question answering. Code is a very specialized kind of text and requires distinct data sources. Because code embeddings do not involve image processing, the vision portion of jina-embeddings-v4 is not affected by training the code retrieval LoRA adapter. The backbone LLM Qwen2.5-VL-3B-Instruct \n\nwas pre-trained on data including the StackEx-changeQA 4 and the CodeSearchNet [Husain et al., 2020] datasets, giving it some capacity to support code embeddings before further adaptation. Our LoRA training used the same triplet-based method described in Section 5.2.1. Training triplets are derived from a variety of sources, including CodeSearchNet, CodeFeedback [Zheng et al., 2024], APPS [Hendrycks et al., 2021], and the CornStack dataset [Suresh et al., 2025]. We maintained a consistent training configura-tion by using the same input prefix tokens (e.g., query, passage) and temperature hyperparameter (set to 0.02 ) during the triplet-based training. \n\n6 Jina-VDR: Visually Rich Document Retrieval Benchmark \n\nTo evaluate the performance of \n\njina-embeddings-v4 across a broad range of visually rich document retrieval tasks, we have produced a new benchmark collection and released \n\n> 4https://github .com/laituan245/ StackExchangeQA\n\nit to the public. 5\n\nThis new collection tests an embedding model‚Äôs ability to integrate textual and visual understanding of documents that consist of in the form of rendered images of visual elements like charts, tables, and running text. It extends the ViDoRe benchmark [Faysse et al., 2025] by adding a diverse collection of datasets spanning a broad range of domains (e.g. legal texts, historic documents, marketing materials), covering a variety of material types (e.g. charts, tables, manuals, printed text, maps) and query types (e.g. questions, facts, descriptions), as well as multiple languages. The benchmark suite encompasses ViDoRe and adds 30 additional tests. These tests include re-purposed existing datasets, new manually-annotated datasets, and generated synthetic data For a comprehensive overview of the individual benchmarks, see Appendix A.1. \n\n6.1 Re-purposed Datasets \n\nWe have adapted a number of existing VQA and OCR datasets, modifying and restructuring them into appropriate query-document pairs. For example, for DonutVQA 6,TableVQA [Tom Agonnoude, 2024], MP-MQA [Zhang et al., 2023], CharXiv [Wang et al., 2024], and PlotQA [Methani et al., 2020], we used structured templates and generative language mod-els to formulate text queries to match their contents. JDocQAJP 7 and HungarianDocQA 8 already con-tain documents and queries in forms that require minimal processing to adapt as benchmarks. We also created datasets from available data that extend beyond conventional question formats. The OurWorldInData and WikimediaMaps datasets use encyclopedia article fragments and image descriptions as queries to match with charts and maps. The GitHubREADMERetrieval dataset contains rendered Markdown pages drawn from GitHub README files, paired with generated natural language descriptions in 17 languages. The WikimediaCommonsDocuments benchmark pairs multilingual document pages with paragraph-level    \n\n> 5Benchmark available at https://huggingface .co/ collections/jinaai/jinavdr-visual-document-retrieval-684831c022c53b21c313b449 .\n> 6https://huggingface .co/datasets/warshakhan/ donut_vqa_ISynHMP\n> 7https://huggingface .co/datasets/jlli/JDocQA-nonbinary\n> 8https://huggingface .co/datasets/jlli/ HungarianDocQA-OCR\n\nreferences extracted from Wikipedia. \n\n6.2 Manually Annotated Datasets \n\nWe have curated a number of human-annotated resources to better reflect real-world use cases. These include academic slides from Stanford lec-tures [Mitchener, 2021], educational figures in the TQA dataset [Kembhavi et al., 2017], and marketing and institutional documents such as the Jina AI 2024 Yearbook [Jina AI, 2024], Japanese Ramen [Niigata-shi Kank ¬Øo Kokusaik ¬Øory ¬Øubu Kank ¬Øo Suishinka, 2024], and the Shanghai Master Plan [Shanghai Municipal People‚Äôs Government Urban Planning and Land Resource Administration Bureau, 2018]. Documents in these datasets were paired with care-fully written human queries without template-based phrasing, capturing genuine information-seeking intent. Some of these datasets target specific languages and regions to provide broader coverage. We also incorporated pre-existing human-annotated datasets like ChartQA 9 and its Arabic counterpart, ArabicChartQA [Ghaboura et al., 2024], which focus on charts and infographics. \n\n6.3 Synthetic Data Generation \n\nWe have been attentive, in constructing Jina-VDR, to the lack of diversity that often plagues informa-tion retrieval benchmarks. We cannot commission human-annotated datasets for everything and have had recourse to generative AI to fill in the gaps. We obtained a number of datasets from primarily European sources containing scans of historical, legal, and journalistic documents in German, French, Spanish, Italian, and Dutch. We used Qwen2 to generate queries for these documents. We handled the HindiGovernmentVQA and RussianBeverages datasets in the same way, adding not only often underrepresented languages, but also public service documents and commercial catalogs to this benchmark set. TweetStockRetrieval 10 is a collection of chart-based financial data, which we have paired with mul-tilingual template-based generated queries. AirBn-BRetrieval 11 is a collection of rendered tables that we have paired with queries in 10 languages gener-ated from a template. \n\n> 9https://huggingface .co/datasets/ HuggingFaceM4/ChartQA\n> 10 https://www .kaggle .com/datasets/ thedevastator/tweet-sentiment-s-impact-on-stock-returns\n> 11 https://www .kaggle .com/datasets/dgomonov/new-york-city-airbnb-open-data\n\nIn several cases, such as TableVQA [Delestre, 2024], we introduced bilingual examples (e.g., French/English) to better assess cross-lingual retrieval performance, with questions and answers synthesized using advanced multilingual LLMs such as Gemini 1.5 Pro and Claude 3.5 Sonnet. \n\n6.4 Jina-VDR in a Nutshell \n\nJina-VDR extends the ViDoRe benchmark with: ‚Ä¢ 30 new tasks, using both real-world and synthetic data ‚Ä¢ All datasets adapted for retrieval and designed to be compatible with ViDoRe ‚Ä¢ LLM-based filtering to ensure all queries are relevant and reflective of real-world querying ‚Ä¢ Non-question queries, such as GitHub descrip-tions matched to rendered markdown images, and map images from Wikimedia Commons with accompanying textual descriptions ‚Ä¢ Multilingual coverage, with some datasets spanning up to 20 languages \n\n7 Evaluation \n\nWe have evaluated jina-embeddings-v4 on a diverse set of benchmarks to reflect its multiple functions. Table 3 provides an overview of benchmark averages for jina-embeddings-v4 \n\nand other embedding models. \n\n7.1 Multilingual Text Retrieval \n\nMTEB and MMTEB [Enevoldsen et al., 2025] are the most widely used text retrieval benchmarks. For most tasks, we have used the asymmetric retrieval adapter, but for some symmetric retrieval tasks like ArguAna 12 , we have used the text matching adapter instead. For evaluation, we prepend the query with the prefix ‚ÄúGiven a claim, find documents that refute the claim‚Äù to reflect the task‚Äôs focus on retrieving passages that contradict, rather than support, the input claim, similar to Wang et al. [2023]. The results are tabulated in Appendix A.4. For the MTEB benchmarks, which are all in English, see Table A11, and for the multilingual MMTEB, see Table A12. The performance of this new model is generally better than our previous model jina-embeddings-v3 and broadly comparable with the state-of-the-art. \n\n> 12 https://huggingface .co/datasets/mteb/arguana\n\nTable 3: Average Retrieval Scores of Embedding Models on Various Benchmarks.                                                                                       \n\n> Model J-VDR ViDoRe CLIPB MMTEB MTEB-en COIR LEMB STS-m STS-en\n> jina-embeddings-v4 (dense) 72.19 84.11 84.11 66.49 55.97 71.59 67.11 72.70 85.89 jina-embeddings-v4 (late) 79.29 90.17 text-embedding-3-large ‚Äì‚Äì‚Äì59.27 57.98 62.36 52.42 70.17 81.44 bge-m3 ‚Äì‚Äì‚Äì55.36 58.73 multilingual-e5-large-instruct ‚Äì‚Äì‚Äì57.12 53.47 41.76 jina-embeddings-v3 47.06 26.02 ‚Äì58.58 54.33 55.07 55.66 75.77 85.82 voyage-3 ‚Äì‚Äì‚Äì66.13 53.46 67.23 74.06 68.33 78.59 gemini-embedding-001 ‚Äì‚Äì‚Äì67.71 64.35 73.11 78.35 85.29 jina-embedings-v2-code ‚Äì‚Äì‚Äì52.24 voyage-code ‚Äì‚Äì‚Äì77.33 nllb-clip-large-siglip 83.19 jina-clip-v2 39.84 53.61 81.12 colpali-v1.2 (late) 64.50 83.90 dse-qwen2-2b-mrl-v1 (dense) 65.96 85.80 voyage-multimodal-v3 (dense) 84.24\n> Task Acronyms: J-VDR = Jina VDR, VidoRE = ViDoRe, CLIPB = CLIP Benchmark, MMTEB = MTEB(Multilingual, v2) Retrieval Tasks, MTEB-EN = MTEB(eng, v2) Retrieval Tasks, COIR = CoIR Code Retrieval, LEMB = LongEmbed, STS-m = MTEB(Multilingual, v2) Semantic Textual Similarity Tasks, STS-en = MTEB(eng, v2) Semantic Textual Similarity Tasks\n> Average Calculation: For J-VDR and ViDoRE, we calculate the average for the multilingual tasks first and consider this as a single score before calculating the average across all tasks. Scores are nDCG@5 for J-VDR, ViDoRe, and CLIPB, and nDCG@10 for MMTEB, MTEB-en, COIR, and LEMB, and Spearman coefficient for STS-m and STS-en.\n> Evaluation of Text Retrieval Models on J-VDR: For evaluating text retrieval models on J-VDR, we used EasyOCR ( https: //github .com/JaidedAI/EasyOCR ) and the provided extracted texts from the original ViDoRe datasets.\n\nWe have also evaluated the performance of our model on retrieval tasks that involve long text documents using the LongEmbed benchmark [Zhu et al., 2024]. The results are tabulated in Table A13 of Appendix A.4. Long document performance for jina-embeddings-v4 significantly outpaces competing models except the voyage-3 series and improves dramatically on jina-embeddings-v3 ‚Äôs performance. \n\n7.2 Textual Semantic Similarity \n\nWe evaluated jina-embeddings-v4 with text-based semantic similarity (STS) benchmarks. The results for MTEB STS and MMTEB STS benchmarks are tabulated in Tables A14 and A15 of Appendix A.4 respectively. Our results are competitive with the state-of-the-art and are best-in-class for English similarity tasks. \n\n7.3 Multimodal Retrieval \n\nTo evaluate the model‚Äôs performance on typical text-to-image search tasks, we used the common English and non-English tasks of the CLIP Bench-mark 13 . The results are tabulated in Tables A8 to A10 of Appendix A.3. jina-embeddings-v4 \n\n> 13 https://github .com/LAION-AI/CLIP_benchmark\n\nhas a higher average score than jina-clip-v2 \n\nand nllb-siglip-large , but the latter performs somewhat higher on the Crossmodal3600 bench-mark [Thapliyal et al., 2022] (see Table A10) because it includes content from low-resource lan-guages not supported in jina-embeddings-v4 ‚Äôs \n\nQwen2.5-VL-3B-Instruct backbone. We further tested jina-embeddings-v4 on the ViDoRe and Jina-VDR benchmarks, to evaluate its performance on visually rich documents. The results are compiled in Appendix A.2. ViDoRe scores are tabulated in Table A3. Table A2 provides an overview of jina-embeddings-v4 compared to other models, with Tables A4 to A7 providing details results for some individual Jina-VDR benchmarks. This suggests that other models are primarily trained to perform well on document retrieval tasks that are similar to the ViDoRe tasks but underperform on other tasks, e.g., that do not involve queries that resemble questions. \n\njina-embeddings-v4 excels at this benchmark, providing the current state-of-the-art, in both single-and multi-vector mode. Multi-vector/late interac-tion matching is generally recognized as more pre-cise than single-vector matching in other applica-tions, and this remains true for Jina-VDR. \n\n7.4 Code Retrieval \n\nTo assess performance on code retrieval, we evaluate the model on the MTEB-CoIR bench-mark [Li et al., 2024], which consists of 10 tasks spanning text-to-code, code-to-text, code-to-code, and hybrid code retrieval types. The results are reported in Table A16 of Appendix A.4. \n\njina-embeddings-v4 is competitive with the state-of-the-art in general-purpose embedding models, but the specialized voyage-code model has somewhat better benchmark performance. \n\n8 Analysis of the Embedding Space \n\nThe large difference in architecture between \n\njina-embeddings-v4 and CLIP-style models like OpenAI CLIP [Radford et al., 2021] and \n\njina-clip-v2 implies a large difference in the structure of the embedding spaces those models generate. We look here at a few of these issues. \n\n8.1 Modality Gap \n\nPrevious work has shed light on the so-called modality gap in multimodal models trained with contrastive learning [Liang et al., 2022b, Schrodi et al., 2024, Eslami and de Melo, 2025]. Good semantic matches across modalities tend to lie considerably further apart in the embedding space than comparable or even worse matches of the same modality, i.e., texts in CLIP-style models are more similar to semantically unrelated texts than to semantically similar images. We can see the modality gap directly by exam-ining the distribution of pairwise cosine similarities of matching image-text pairs versus matching text-text pairs. In Figure 2, we see the distribution of similarity values for the two pair types in OpenAI CLIP, jina-clip-v2 , and jina-embeddings-v4 .The gap is dramatically reduced with \n\njina-embeddings-v4 because of its cross-model encoder, illustrated in Figure 1. Eslami and de Melo [2025] has shown that sharing an encoder between modalities introduces an inductive bias to-wards using a shared region of the embedding space, while Liang et al. [2022b] shows the opposite is true for CLIP-style architectures with separate encoders. \n\n8.2 Cross-Modal Alignment \n\nEslami and de Melo [2025] has defined the cross-modal alignment score of a multimodal embed-ding model as the average of cosine similarities of 0.2 0.0 0.2 0.4 0.6 0.8 1.0              \n\n> 0\n> 2\n> 4\n> 6\n> 8\n> 10\n> Density\n> Image-Text Pairs Text-Text Pairs 0.2 0.0 0.2 0.4 0.6 0.8 1.0\n> 0\n> 2\n> 4\n> 6\n> 8\n> 10\n> Density 0.2 0.0 0.2 0.4 0.6 0.8 1.0\n> Cosine Similarity\n> 0\n> 1\n> 2\n> 3\n> 4\n> 5\n> Density\n\nFigure 2: Distribution of the cosine similarities of the paired image-text embeddings versus paired text-text embeddings from the Flickr8K 14 dataset. Top :OpenAI CLIP, Middle : jina-clip-v2 , Bottom :\n\njina-embeddings-v4 \n\nmatching pairs of image and text embeddings. Ta-ble 4 calculates this score for jina-embeddings-v4 \n\nand OpenAI CLIP with data sampled from the Flickr30K 15 , MSCOCO [Lin et al., 2014], and CIFAR-100 16 datasets. These results confirm that jina-embeddings-v4 \n\ngenerates a far better aligned cross-modal embed-ding space than CLIP-style models. It is worth noting that jina-embeddings-v4 \n\nshows much poorer alignment for CIFAR-100 data than MSCOCO and Flickr30K. This is because             \n\n> 15 https://www .kaggle .com/datasets/adityajn105/ flickr30k\n> 16 https://www .kaggle .com/datasets/fedesoriano/ cifar100 Model Flickr30K MSCOCO CIFAR-100\n> OpenAI-CLIP 0.15 0.14 0.2\n> jina-clip-v2 0.38 0.37 0.32\n> jina-embeddings-v4 0.71 0.72 0.56\n\nTable 4: Comparison of cross-modal alignment scores on 1K of random samples from each dataset. \n\nCIFAR-100 is a classification dataset and its labels are far less informative than the more descriptive texts in MSCOCO and Flickr30K. \n\n8.3 Cone Effect \n\nLiang et al. [2022b] demonstrate that multimodal models trained with contrastive loss suffer from an inductive bias known as the cone effect. Each modality tends to cluster together in randomized embedding spaces before training, and contrastive loss tends to make the cross-modal matching pairs form a kind of high-dimensional cone, linking one part of the embedding space to another rather than distributing embeddings evenly. The impact of the cone effect can be seen in Figure 3. The difference in cosine similarity between correct and incorrect text-image matches is quite small for OpenAI CLIP (top), signifi-cantly greater in jina-clip-v2 (middle), but \n\njina-embeddings-v4 (bottom) shows a much greater spread of cosine similarity ranges with very distinctly separate peaks for positive and negative pairs. This shows that jina-embeddings-v4 uses much more of the embedding space and image and text embeddings have a much greater overlap in distribution. \n\n9 Conclusion \n\nWe present jina-embeddings-v4 , a state-of-the-art multimodal and multilingual embedding model designed for a wide range of tasks, including semantic text retrieval, text-to-image retrieval, text-to-visually-rich document retrieval, and code search. The model achieves strong performance us-ing single-vector representations and demonstrates even greater effectiveness with multi-vector repre-sentations, particularly in visually rich document retrieval. jina-embeddings-v4 aligns representa-tions across modalities into a single, shared seman-tic space, sharply reducing structural gaps between modalities compared to CLIP-style dual-tower mod-els, enabling more effective cross-modal retrieval. In future work, we plan to further enhance this model‚Äôs multilingual capabilities and explore 0.2 0.0 0.2 0.4 0.6 0.8 1.0              \n\n> 0\n> 2\n> 4\n> 6\n> 8\n> 10\n> Density\n> Positive Samples Negative Samples 0.2 0.0 0.2 0.4 0.6 0.8 1.0\n> 0\n> 2\n> 4\n> 6\n> 8\n> 10\n> Density 0.2 0.0 0.2 0.4 0.6 0.8 1.0\n> Cosine Similarity\n> 0\n> 1\n> 2\n> 3\n> 4\n> 5\n> Density\n\nFigure 3: Distribution of the cosine similarities of positive (correct matches) versus negative (incorrect matches) image-text samples. (top) OpenAI CLIP, (mid-dle) jina-clip-v2 , (bottom) jina-embeddings-v4 .\n\ntechniques to create smaller, more efficient variants. References \n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large lan-guage models. In The Tenth International Conference on Learning Representations, ICLR , 2022. Yihao Ding, Soyeon Caren Han, Jean Lee, and Eduard Hovy. Deep learning based visually rich document content understanding: A survey. arXiv preprint arXiv:2408.01287 , 2024. Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Ser-ena Yeung, and James Y Zou. Mind the Gap: Under-standing the Modality Gap in Multi-modal Contrastive Representation Learning. Advances in Neural Infor-mation Processing Systems , 35:17612‚Äì17625, 2022a. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, C√©line Hudelot, and Pierre Colombo. ColPali: Efficient Document Retrieval with Vision Language Models. In The Thirteenth International Conference on Learning Representations , 2025. Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stir-ling, Xin Zhang, M√°rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. MIEB: Massive Image Embedding Benchmark. arXiv preprint arXiv:2504.10471 , 2025. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael G√ºnther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, et al. jina-embeddings-v3: Multilingual Embeddings With Task LoRA. arXiv preprint arXiv:2409.10173 , 2024. Alec Radford, Jong Wook Kim, et al. Learning Transferable Visual Models from Natural Language Supervision. In International Conference on Machine Learning , pages 8748‚Äì8763, 2021. Andreas Koukounas, Georgios Mastrapas, Michael G√ºn-ther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Mart√≠nez, Saahil Ognawala, et al. Jina CLIP: Your CLIP Model Is Also Your Text Retriever. arXiv preprint arXiv:2405.20204 , 2024. Ye Liu, Rui Meng, Shafiq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. CodeX-Embed: A Generalist Embedding Model Family for Multilingual and Multi-task Code Retrieval. arXiv preprint arXiv:2411.12644 , 2024. Voyage AI. Domain-Specific Embeddings and Retrieval: Legal Edition (voyage-law-2), 2024. https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying Multimodal Retrieval via Document Screenshot Embedding. In \n\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pages 6492‚Äì6505, 2024. Omar Khattab and Matei Zaharia. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pages 39‚Äì48, 2020. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sen-tence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3982‚Äì3992, 2019. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint arXiv:2212.03533 , 2022. Michael G√ºnther, Jackmin Ong, Isabelle Mohr, Alaed-dine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al. Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. arXiv preprint arXiv:2310.19923 , 2023. Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka Represen-tation Learning. Advances in Neural Information Processing Systems , 35:30233‚Äì30249, 2022. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923 , 2025. Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. E5-V: Universal Embeddings with Multimodal Large Language Models. arXiv preprint arXiv:2407.12580 , 2024. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv preprint arXiv:2412.16855 , 2024. A√§ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Predictive Coding. CoRR , abs/1807.03748, 2018. URL \n\nhttp://arxiv .org/abs/1807 .03748 .Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:1503.02531 , 2015. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards General Text Embeddings with Multi-stage Contrastive Learning. arXiv preprint arXiv:2308.03281 , 2023. Xianming Li and Jing Li. Aoe: Angle-optimized embeddings for semantic textual similarity. In \n\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1825‚Äì1839, 2024. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: A pilot on semantic textual similarity. In SEM 2012: The First Joint Conference on Lexical and Computational Semantics‚ÄìVolume 1: Proceedings of the main confer-ence and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012) , pages 385‚Äì393, 2012. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zampar-elli. A SICK cure for the evaluation of compositional distributional semantic models. In Nicoletta Cal-zolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, \n\nProceedings of the Ninth International Conference on Language Resources and Evaluation (LREC‚Äô14) ,pages 216‚Äì223, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL http://www .lrec-conf .org/proceedings/ lrec2014/pdf/363_Paper .pdf .Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. arXiv preprint arXiv:1909.09436 , 2020. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024 , pages 12834‚Äì12859. Association for Computational Linguistics, August 2024. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks , 2021. Tarun Suresh, Revanth Gangi Reddy, Yifei Xu, Zach Nussbaum, Andriy Mulyar, Brandon Duderstadt, and Heng Ji. Cornstack: High-quality contrastive data for better code retrieval and reranking. arXiv preprint arXiv:2412.01007 , 2025. Cyrile Delestre Tom Agonnoude, 2024. URL https:// huggingface .co/datasets/cmarkea/table-vqa .Liang Zhang, Anwen Hu, Jing Zhang, Shuo Hu, and Qin Jin. Mpmqa: Multimodal question answering on product manuals. 2023. URL \n\nhttps://arxiv .org/abs/2304 .09660 .Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521 , 2024. Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In The IEEE Winter Conference on Applications of Computer Vision (WACV) , March 2020. Kris James Mitchener. National banking statis-tics, 1867‚Äì1896. https://purl .stanford .edu/ mv327tb8364 , 2021. Dataset published via Stanford Digital Repository. Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Ha-jishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal ma-chine comprehension. pages 5376‚Äì5384, 07 2017. doi:10.1109/CVPR.2017.571. Jina AI. Re¬∑Search: Order 2024 Yearbook of Search Foundation Advances . Jina AI, Sunnyvale, CA & Berlin, Germany, December 16 2024. Hardcover with spot UV coating; includes complimentary digital copy. Available at: https://jina .ai/news/re-search-order-2024-yearbook-of-search-foundation-advances/ .Niigata-shi Kank ¬Øo Kokusaik ¬Øory ¬Øubu Kank ¬Øo Su-ishinka. Niigata City Ramen Guidebook . City of Niigata, Niigata, Japan, July 2024. URL \n\nhttps://www .city .niigata .lg .jp/kanko/kanko/ oshirase/ramen .files/guidebook .pdf . PDF, approximately 28 MB. Shanghai Municipal People‚Äôs Government Urban Planning and Land Resource Administration Bureau. \n\nShanghai Master Plan 2017‚Äì2035: Striving for the Excellent Global City . Shanghai Municipal People‚Äôs Government, Shanghai, China, January 2018. URL \n\nhttps://www .shanghai .gov .cn/newshanghai/ xxgkfj/2035004 .pdf . Public Reading edition; government-issued planning document. Sara Ghaboura, Ahmed Heakl, Omkar Thawakar, Ali Alharthi, Ines Riahi, Abduljalil Saif, Jorma Laaksonen, Fahad S. Khan, Salman Khan, and Rao M. Anwer. Camel-bench: A compre-hensive arabic lmm benchmark. 2024. URL \n\nhttps://arxiv .org/abs/2410 .18976 .Cyrile Delestre, 2024. URL https:// huggingface .co/datasets/cmarkea/aftdb .Kenneth Enevoldsen, Isaac Chung, et al. MMTEB: Mas-sive Multilingual Text Embedding Benchmark. 2025. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368 , 2023. Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. LongEmbed: Extending Embedding Models for Long Context Retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing ,pages 802‚Äì816, 2024. Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: A massively multilingual multimodal evaluation dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 715‚Äì729, 2022. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. Coir: A comprehensive benchmark for code information retrieval models. \n\nCoRR , 2024. Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Ser-ena Yeung, and James Y Zou. Mind the gap: Under-standing the modality gap in multi-modal contrastive representation learning. Advances in Neural Infor-mation Processing Systems , 35:17612‚Äì17625, 2022b. Simon Schrodi, David T Hoffmann, Max Argus, Volker Fischer, and Thomas Brox. Two effects, one trigger: on the modality gap, object bias, and information im-balance in contrastive vision-language representation learning. arXiv preprint arXiv:2404.07983 , 2024. Sedigheh Eslami and Gerard de Melo. Mitigate the gap: Improving cross-modal alignment in clip. In The Thirteenth International Conference on Learning Representations , 2025. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Com-puter Vision ‚Äì ECCV 2014 , pages 740‚Äì755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1. A Appendix \n\nA.1 Datasets in the Jina-VDR Benchmark \n\nTable A1: Overview of the Dataset Collection \n\nDataset Name Domain Document Format Query Format Number of Queries / Documents Languages \n\njinaai/airbnb-synthetic-retrieval‚Ä† Housing Tables Instruction 4953 / 10000 ar, de, en, es, fr, hi, hu, ja ru, zh jinaai/arabic_chartqa_ar Mixed Charts Question 745 / 745 ar jinaai/arabic_infographicsvqa_ar Mixed Illustrations Question 120 / 40 ar jinaai/automobile_catalogue_jp Marketing Catalog Question 45 / 15 ja jinaai/arxivqa Science Mixed Question 30 / 499 en jinaai/beverages_catalogue_ru Marketing Digital Docs Question 100 / 34 ru jinaai/ChartQA Mixed Charts Question 7996 / 1000 en jinaai/CharXiv-en Science Charts Question 999 / 1000 en jinaai/docvqa Mixed Scans Question 39 / 499 en jinaai/donut_vqa Medical Scans / Handwriting Question 704 / 800 en jinaai/docqa_artificial_intelligence Software / IT Digital Docs Question 70 / 962 en jinaai/docqa_energy Energy Digital Docs Question 69 / 972 en jinaai/docqa_gov_report Government Digital Docs Question 77 / 970 en jinaai/docqa_healthcare_industry Medial Digital Docs Question 90 / 963 en jinaai/europeana-de-news Historic Scans / News Articles Question 379 / 137 de jinaai/europeana-es-news Historic Scans / News Articles Question 474 / 179 es jinaai/europeana-fr-news Historic Scans / News Articles Question 237 / 145 fr jinaai/europeana-it-scans Historic Scans Question 618 / 265 it jinaai/europeana-nl-legal Legal Scans Question 199 / 300 nl jinaai/github-readme-retrieval-multilingual‚Ä† Software / IT Markdown Docs Description 16755 / 4398 ar, bn, de, en, es, fr, hi, id, it, ja, ko, nl pt, ru, th, vi, zh jinaai/hindi-gov-vqa Governmental Digital Docs Question 454 / 340 hi jinaai/hungarian_doc_qa_hu Mixed Digital Docs Question 54 / 54 hu jinaai/infovqa Mixed Illustrations Question 363 / 500 en jinaai/jdocqa News Digital Docs Question 744 / 758 ja jinaai/jina_2024_yearly_book Software / IT Digital Docs Question 75 / 33 en jinaai/medical-prescriptions Medical Digital Docs Question 100 / 100 en jinaai/mpmqa-small Manuals Digital Docs Question 155 / 782 en jinaai/MMTab Mixed Tables Fact 987 / 906 en jinaai/openai-news Software / IT Digital Docs Question 31 / 30 en jinaai/owid_charts_en Mixed Charts Question 132 / 972 en jinaai/plotqa Mixed Charts Question 610 / 986 en jinaai/ramen_benchmark_jp Marketing Catalog Question 29 / 10 ja jinaai/shanghai_master_plan Governmental Digital Docs Question / Key Phrase 57 / 23 zh, en jinaai/wikimedia-commons-documents-ml‚Ä† Mixed Mixed Description 14061 / 14661 ar, bn, de, en, es, fr, hi, hu, id, it, ja, ko, my, nl, pt, ru, th, ur, vi, zh jinaai/shiftproject Environmental Documents Digital Docs Question 89 / 998 fr jinaai/stanford_slide Education Slides Question 14 / 1000 en jinaai/student-enrollment Demographics Charts Question 1000 / 489 en jinaai/tabfquad Mixed Tables Question 126 / 70 fr, en jinaai/table-vqa Science Tables Question 992 / 1000 en jinaai/tatqa Finance Digital Docs Question 121 / 176 en jinaai/tqa Education Illustrations Question 981 / 394 en \n\nContinued on next page Table A1 ‚Äì continued from previous page \n\nDataset Name Domain Document Format Query Format Number of Queries / Documents Languages \n\njinaai/tweet-stock-synthetic-retrieval‚Ä† Finance Charts Question 6278 / 10000 ar, de, en, es, hi, hu, ja, ru, zh jinaai/wikimedia-commons-maps Mixed Maps Description 443 / 455 en ‚Ä†For multilingual datasets, the total number of queries and documents is the sum across all language-specific splits. \n\nA.2 JinaVDR (Visual Document Retrieval) Benchmark Results \n\nTable A2: Overview of JinaVDR Results for Various Models \n\nTask bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 45.48 48.14 40.21 65.51 48.62 73.86 80.16 medical-prescriptions 38.18 37.25 15.66 83.91 38.22 81.17 97.95 stanford_slide 81.78 95.28 91.48 100 100 100 100 donut_vqa 19.39 2.60 1.63 32.53 25.31 78.48 73.55 table-vqa 35.64 34.24 21.06 54.66 57.39 58.90 60.91 ChartQA 26.01 29.46 36.45 50.29 55.01 66.45 67.40 tqa 50.04 24.21 27.45 68.27 65.29 65.79 68.28 openai-news 76.63 87.30 70.05 94.82 92.56 93.75 97.62 europeana-de-news 11.26 12.05 11.19 34.64 44.75 49.05 65.65 europeana-es-news 51.99 44.03 13.14 44.74 60.58 60.10 80.58 europeana-it-scans 39.11 38.69 16.23 54.32 53.92 57.88 73.14 europeana-nl-legal 34.97 29.07 9.79 30.89 29.50 37.14 54.15 hindi-gov-vqa 1.83 7.52 5.02 13.04 9.80 15.40 21.94 automobile_catalogue_jp 20.92 49.53 32.54 39.69 65.62 77.10 81.32 beverages_catalogue_ru 11.05 14.11 39.45 60.36 81.07 85.31 86.65 ramen_benchmark_jp 28.03 62.19 39.79 47.46 51.75 89.50 94.65 jdocqa_jp_ocr 1.64 7.79 19.91 39.45 66.73 75.57 82.34 hungarian_doc_qa 32.43 55.42 47.91 73.08 53.80 71.19 75.20 arabic_chartqa_ar 6.27 5.91 4.12 12.03 35.37 44.73 49.79 arabic_infographicsvqa_ar 13.26 13.49 49.98 51.24 71.93 84.86 94.07 owid_charts_en 65.50 61.23 57.61 83.53 83.48 87.40 89.68 arxivqa 56.67 54.27 84.64 95.44 93.33 95.44 95.44 docvqa 76.37 43.87 45.23 88.84 86.28 84.05 92.98 shiftproject 59.43 68.43 32.10 76.56 78.76 81.65 90.72 docqa_artificial_intelligence 90.78 82.93 64.81 95.19 97.52 96.43 98.04 docqa_energy 87.52 78.68 65.39 94.95 90.08 89.19 96.57 docqa_gov_report 86.29 82.03 69.18 94.75 94.50 91.84 95.97 docqa_healthcare_industry 84.83 88.92 67.77 95.85 95.24 94.62 97.51 tabfquad 42.62 78.03 46.94 87.62 91.80 95.57 94.79 mpmqa_small 85.54 66.85 59.53 88.95 81.99 80.96 91.80 jina_2024_yearly_book 87.67 85.98 76.63 93.05 92.90 94.78 97.68 wikimedia-commons-maps 5.37 5.12 20.43 30.53 33.14 39.55 54.15 plotqa 61.13 51.39 23.98 73.46 75.98 77.97 78.77 MMTab 74.82 74.18 44.63 85.80 85.90 86.16 90.00 CharXiv-en 46.85 41.58 56.34 83.96 84.10 82.80 87.78 student-enrollment 1.05 1.19 0.72 3.92 4.08 7.88 12.02 tatqa 75.21 70.38 44.91 85.29 80.68 79.46 92.93 shanghai_master_plan 12.69 93.32 75.28 90.50 93.21 94.04 97.18 europeana-fr-news 24.55 23.78 16.21 30.99 37.49 36.10 50.44 infovqa 68.64 75.39 63.03 90.21 92.41 92.11 96.49 \n\nModels: bm25+OCR: BM25 with EasyOCR, jev3+OCR: jina-embeddings-v3 with EasyOCR, j-clip-v2: jina-clip-v2 , colpali-v1.2: ColPALI-v1.2, dse-qwen2- 2b-mrl-v1: DSE-QWen2-2b-MRL-V1, je4-single: jina-embeddings-v4 single-vector, jev4-multi: jina-embeddings-v4 multi-vector Table A3: Retrieval performance on ViDoRe (nDCG@10%). \n\nModel Avg AQA DVQA InfoVQA Shift AI Energy Gov Health TabFQ TQA \n\nOCR + jina-embeddings-v3 26.02 26.31 12.62 32.79 14.18 22.84 27.47 31.16 45.78 44.54 2.53 jina-clip-v2 53.61 68.33 27.62 60.6 34.12 66.55 64.69 67.47 68.38 46.89 31.43 voyage-multimodal-3 84.20 84.90 55.60 85.40 78.70 94.50 89.50 96.00 95.10 92.80 69.90 colpali-v1.2 83.90 78.00 57.20 82.80 79.10 98.10 95.20 94.80 96.70 89.70 68.10 dse-qwen2-2b-mrl-v1 85.80 85.60 57.10 88.10 82.00 97.50 92.90 96.00 96.40 93.10 69.40 OCR + bm25 65.50 31.60 36.80 62.90 64.30 92.80 85.90 83.90 87.20 46.50 62.70 siglip-so400m-patch14-384 51.40 43.20 30.30 64.10 18.70 62.50 65.70 66.10 79.10 58.10 26.20 jina-embeddings-v4 (dense) 84.11 83.57 50.54 87.85 84.07 97.16 91.66 91.48 94.92 94.48 65.35 jina-embeddings-v4 (late) 90.17 88.95 59.98 93.57 92.35 99.26 96.76 96.95 98.39 95.13 80.34 \n\nTasks: Avg: Mean nDCG@10% over all tasks, AQA: ArxivQA, Shift: Shift Project, DVQA: DocVQA, InfoVQA: InfographicVQA, AI: Artificial Intelligence, Gov: Government Reports, Health: Healthcare Industry, TabFQ: TabFQuad, TQA: TAT-DQA \n\nTable A4: Wikimedia Commons Retreival Benchmark Results \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 21.95 37.34 48.32 46.82 58.43 65.79 74.50 Arabic (ar) 19.60 38.06 45.29 41.96 62.92 72.07 81.19 Bengali (bn) 22.93 44.44 48.48 38.56 52.39 67.31 76.60 German (de) 12.74 39.45 52.72 51.91 63.00 70.32 80.69 English (en) 36.44 45.21 56.51 68.76 70.31 73.91 81.72 Spanish (es) 12.75 46.02 54.70 59.57 66.31 71.80 80.33 French (fr) 15.59 35.83 35.69 44.42 40.84 53.67 59.28 Hindi (hi) 16.53 36.69 47.47 38.13 50.56 63.45 68.78 Hungarian (hu) 25.32 33.86 44.20 26.22 52.60 65.36 76.10 Indonesian (id) 28.78 39.35 50.77 56.41 61.59 66.34 73.59 Italian (it) 19.58 37.93 49.61 49.08 60.12 64.18 73.31 Japanese (jp) 21.32 30.42 43.75 53.50 63.61 66.72 76.63 Korean (ko) 34.86 35.16 47.12 54.77 68.47 71.83 81.53 Burmese (my) 22.84 29.57 54.25 16.31 36.97 46.91 50.23 Dutch (nl) 14.89 39.76 50.23 56.87 64.67 68.97 78.42 Portuguese (pt) 23.27 45.57 53.97 58.17 67.26 69.23 78.43 Russian (ru) 16.82 38.92 49.34 46.77 64.26 69.56 80.45 Thai (th) 30.00 29.79 45.76 47.90 55.78 63.36 70.03 Urdu (ur) 13.64 32.78 36.30 15.68 38.49 48.71 61.91 Vietnamese (vi) 32.40 39.87 54.06 56.82 64.33 72.93 80.18 Chinese (zh) 18.76 28.18 46.24 54.62 64.14 69.11 80.55 \n\nTable A5: GitHub Readme Retrieval Benchmark Results \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 50.11 65.17 38.93 76.83 72.26 85.52 85.69 Arabic (ar) 27.49 27.99 31.07 58.53 55.63 75.04 75.71 Bengali (bn) 1.29 28.29 27.06 53.72 47.55 65.60 66.03 German (de) 60.11 84.50 43.35 87.16 80.76 91.10 91.39 English (en) 87.40 91.60 48.58 93.91 90.75 96.78 97.35 Spanish (es) 78.57 83.30 43.19 86.84 78.61 89.46 89.99 French (fr) 77.55 83.66 42.12 85.42 79.20 90.29 90.31 Hindi (hi) 2.72 48.19 28.53 57.93 46.62 68.98 70.88 Indonesian (id) 78.05 82.41 38.59 83.06 74.39 88.31 88.69 Italian (it) 78.83 86.53 44.33 86.94 80.93 91.74 91.36 \n\nContinued on next page Table A5 ‚Äì continued from previous page \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nJapanese (jp) 14.46 63.12 41.99 75.99 75.31 89.60 90.91 Korean (ko) 40.01 35.32 38.32 69.45 68.77 86.92 86.85 Dutch (nl) 76.52 86.33 43.17 88.01 82.97 92.83 91.39 Portuguese (pt) 80.33 84.54 43.86 86.83 80.22 91.58 91.50 Russian (ru) 39.78 51.07 36.80 80.67 79.08 89.55 88.63 Thai (th) 1.47 36.75 37.80 68.68 65.51 77.78 76.46 Vietnamese (vi) 66.70 79.65 37.08 75.00 68.15 86.93 87.13 Chinese (zh) 40.52 54.57 35.94 67.98 73.90 81.43 82.08 \n\nTable A6: Tweet Stock Retrieval Benchmark Results \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 22.29 42.69 55.36 78.32 62.81 64.05 85.46 Arabic (ar) 0.38 1.67 49.26 77.02 52.51 52.86 78.11 German (de) 48.27 66.85 52.29 75.70 57.50 64.22 85.97 English (en) 51.38 63.61 48.28 78.70 63.31 63.87 85.17 Spanish (es) 54.28 63.80 53.16 77.81 62.38 63.61 84.24 French (fr) 51.69 64.53 54.92 75.95 62.48 62.20 85.35 Hindi (hi) 0.08 0.08 89.06 96.51 96.62 87.14 96.86 Hungarian (hu) 15.55 61.71 52.05 73.50 58.36 63.75 85.70 Japanese (jp) 0.40 47.64 54.76 70.38 58.03 61.25 85.61 Russian (ru) 0.47 2.83 47.09 78.10 57.51 63.18 82.95 Chinese (zh) 0.45 54.16 52.78 79.51 59.07 58.44 84.66 \n\nTable A7: AirBnB Retrieval Benchmark Results \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 7.20 1.12 2.12 16.45 11.06 8.13 37.42 Arabic (ar) 1.10 0.39 0.47 2.59 3.26 2.28 6.63 German (de) 4.03 0.71 5.85 30.58 14.46 9.32 42.26 English (en) 48.39 1.72 4.73 47.51 13.07 13.06 63.48 Spanish (es) 6.25 0.18 1.93 24.15 8.46 9.12 40 French (fr) 3.86 1.92 2.03 18.02 12.21 8.55 29.69 Hindi (hi) 0.16 0.78 0.84 1.81 4.51 3.96 17.56 Hungarian (hu) 5.58 0.63 3.10 23.09 11.74 6.87 27.21 Japanese (jp) 0.36 1.70 0.46 4.12 14.71 7.65 46.21 Russian (ru) 1.67 1.39 0.79 11.18 13.98 8.43 40.43 Chinese (zh) 0.58 1.77 1.04 1.49 14.17 12.09 60.70 A.3 CLIP \n\nTable A8: Cross-modal (Text-to-image) retrieval performance (Recall@5%) on the CLIP benchmark. \n\nModel Avg flickr30k mscoco_captions crossmodal3600 xtd10 \n\nnllb-clip-large-siglip 83.19 92.24 70.84 82.07 87.60 jina-clip-v2 81.12 89.84 68.35 81.43 84.87 jina-embeddings-v4 84.11 91.36 76.18 79.42 89.46 Avg : Mean Recall@5% over all 4 tasks. \n\nTable A9: Text-to-image retrieval performance (Recall@5%) on xtd10 for all supported languages. \n\nLanguage jina-embeddings-v4 jina-clip-v2 nllb-clip-large-siglip average 89.46 84.87 87.60 de 92.10 85.70 88.30 en 93.10 89.40 89.40 es 91.50 85.90 88.20 fr 91.30 85.10 87.70 it 92.20 85.80 89.30 ko 86.30 82.10 85.20 pl 89.10 86.50 89.40 \n\nru 91.50 81.10 83.40 tr 84.70 83.70 88.30 \n\nzh 82.80 83.40 86.80 \n\nTable A10: Text-to-image retrieval performance (Recall@5%) on crossmodal3600 for all supported languages. \n\nLanguage jina-embeddings-v4 jina-clip-v2 nllb-clip-large-siglip average 79.42 81.43 82.07 \n\nar 75.75 73.56 78.92 \n\nbn 57.97 63.78 75.19 \n\nda 80.47 85.39 87.14 \n\nde 91.75 91.25 89.56 el 66.50 75.03 77.83 \n\nen 76.47 75.83 73.11 es 83.64 83.64 82.64 fi 66.67 82.83 86.42 \n\nfr 88.69 88.78 87.86 hi 47.81 55.25 60.31 \n\nid 87.41 84.22 86.31 it 87.97 88.33 85.94 ja 91.22 87.03 86.06 ko 82.19 78.81 78.75 nl 81.00 82.56 81.69 no 71.94 81.08 82.69 \n\npl 80.86 84.00 82.72 pt 81.42 82.42 82.69 \n\nro 84.33 89.36 90.03 \n\nru 90.28 88.97 86.44 sv 72.58 78.06 79.33 \n\nth 83.36 81.61 81.14 tr 73.08 81.31 83.47 \n\nuk 86.28 88.56 85.44 vi 88.81 86.64 85.56 zh 86.67 78.97 76.56 A.4 MTEB and MMTEB \n\nTable A11: Evaluation Results for Various Models on MTEB Retrieval Tasks (nDCG@10%) \n\nModel Arg CQG CQU CFHN FEV FiQA HPQA SCI TREC TOU AVG \n\nmultilingual-e5-large 54.36 58.70 39.89 26.00 83.79 43.82 70.55 17.45 71.15 49.59 51.53 e5-mistral-7b-instruct 61.65 63.52 46.75 28.50 86.99 56.81 73.21 16.32 87.03 55.44 57.62 text-embedding-3-large 57.99 65.40 50.02 30.10 88.53 55.00 71.66 23.07 79.56 58.42 57.98 gemini-embedding-001 86.44 70.68 53.69 31.06 88.98 61.78 87.01 25.15 86.32 52.39 64.35 jina-embedding-l-en-v1 48.3 51.68 38.66 25.93 71.16 41.02 57.26 18.54 60.34 62.34 47.52 jina-embeddings-v2-base-en 44.18 56.52 38.66 23.77 73.41 41.58 63.24 19.86 65.91 63.35 49.05 jina-embeddings-v3‚Ä† 54.33 58.02 43.52 43.14 89.90 47.35 64.70 19.92 77.74 55.28 55.39 jina-embeddings-v4‚Ä† 67.07 57.59 42.95 34.57 87.16 46.51 69.01 21.47 80.36 52.41 55.91 ‚Ä†using the text-matching adapter \n\nTasks : Arg: ArguAna, CQG: CQADupstackGamingRetrieval, CQU: CQADupstackUnixRetrieval, CFHN: ClimateFEVERHardNegatives, FEV: FEVERHardNegatives, FiQA: FiQA2018, HPQA: HotpotQAHardNegatives, SCI: SCIDOCS, TREC: TRECCOVID, TOU: Touche2020Retrieval.v3 \n\nTable A12: Evaluation Results for Various Models on MMTEB Retrieval Tasks (nDCG@10%) \n\nModel Avg AI Arg Bel Cov Hag PK LB MIR ML SD SQA SO TC STC TR TW Wiki WG \n\njina-embeddings-v3 58.6 32.8 54.3 73.4 78.6 98.7 38.0 93.4 62.6 73.4 19.8 0.7 90.8 77.7 39.2 0.6 73.0 89.1 18.6 jina-embeddings-v4 66.5 50.2 67.1 74.3 80.2 98.8 69.8 94.8 61.2 74.9 21.5 30.2 91.9 80.4 59.5 1.3 84.4 88.5 67.3 bge-m3 55.4 29.0 54.0 78.2 77.5 98.8 59.0 90.3 69.6 74.8 16.3 7.5 80.6 54.9 21.9 1.0 37.8 89.9 41.7 Cohere-embed-mult.-v3 59.2 29.7 55.1 81.1 77.1 98.8 38.2 93.8 68.0 76.1 19.3 4.7 89.4 83.4 24.2 0.9 75.8 90.9 58.4 gemini-embedding-001 68.1 48.8 86.4 90.7 79.1 99.3 38.5 96.0 70.4 84.2 25.2 10.3 96.7 86.3 51.1 3.0 98.0 94.2 60.5 text-embedding-3-large 61.1 42.0 58.0 68.8 68.4 99.1 69.8 95.2 56.9 73.2 23.1 7.4 92.4 79.6 31.1 2.1 81.4 89.2 29.1 voyage-3 66.0 42.5 61.0 76.5 88.5 98.6 94.8 94.5 57.7 75.7 21.4 10.7 94.3 80.5 49.2 1.2 85.7 89.7 67.7 voyage-multilingual-2 ‚Äì 45.0 61.8 ‚Äì ‚Äì 98.9 97.0 95.9 ‚Äì ‚Äì 22.5 10.2 ‚Äì 80.1 ‚Äì 1.4 87.3 ‚Äì 39.1 \n\nTasks: Avg: Mean nDCG@10% for all tasks, AI: AILAStatutes, Arg: ArguAna, Bel: BelebeleRetrieval, Cov: CovidRetrieval, Hag: HagridRetrieval, PK: LEMBPasskeyRetrieval, LB: LegalBenchCorporateLobbying, MIR: MIRACLRetrievalHardNegatives, ML: MLQARetrieval, SD: SCIDOCS, SQA: SpartQA, SO: StackOverflowQA, TC: TREC-COVID, STC: StatcanDialogueDatasetRetrieval, TR: TempReasonL1, TW: TwitterHjerneRetrieval, Wiki: WikipediaRetrievalMultilingual, WG: WinoGrande \n\nTable A13: Retrieval performance on MTEB LongEmbed (nDCG@10%) \n\nModel Avg NaQA Needle Passkey QMSum SummScreen Wikim \n\njina-embeddings-v3 55.66 34.30 64.00 38.00 39.34 92.33 66.02 jina-embeddings-v4 67.11 57.52 51.75 65.50 46.49 96.30 85.08 voyage-multilingual-2 79.17 64.69 75.25 97.00 51.50 99.11 87.49 voyage-3 74.07 54.12 57.75 94.75 51.05 97.82 88.90 voyage-3-lite 71.41 51.67 54.00 84.75 53.01 96.71 88.34 bge-m3 58.73 45.76 40.25 59.00 35.54 94.09 77.73 text-embedding-3-large 52.42 44.09 29.25 69.75 32.49 84.80 54.16 Cohere-embed-english-v3 42.11 25.04 30.50 38.50 23.82 75.77 59.03 multilingual-e5-large-instruct 41.76 26.71 29.50 37.75 26.08 72.75 57.79 multilingual-e5-large 40.44 24.22 28.00 38.25 24.26 71.12 56.80 \n\nTasks: Avg: Mean nDCG@10% for all tasks, NaQA: LEMBNarrativeQARetrieval, Needle: LEMBNeedleRetrieval, Passkey: LEMBPasskeyRetrieval, QMSum: LEMBQMSumRetrieval, SummScreen: LEMBSummScreenFDRetrieval, Wikim: LEMBWikimQARetrieval Table A14: STS performance on MTEB v2 (Spearman correlation %). \n\nModel Avg BIO SICK-R STS12 STS13 STS14 STS15 STS17 STS22 STSB \n\njina-embeddings-v3 85.82 88.69 89.62 82.44 89.49 84.95 89.32 90.01 68.45 89.43 jina-embeddings-v4 85.89 89.21 89.23 83.50 88.61 84.77 89.69 88.71 70.71 88.58 BAAI/bge-m3 80.61 ‚Äì 79.72 78.73 79.60 79.00 87.81 87.13 67.99 84.87 Cohere-embed-English-3 82.40 83.50 81.27 74.37 85.20 80.98 89.23 90.34 68.18 88.55 Cohere-embed-multilingual-v3 83.05 85.01 82.18 77.62 85.16 80.02 88.92 90.09 69.63 88.79 gemini-embedding-001 85.29 88.97 82.75 81.55 89.89 85.41 90.44 91.61 67.97 89.08 multilingual-e5-large 81.39 84.57 80.23 80.02 81.55 77.72 89.31 88.12 63.66 87.29 text-embedding-3-large 81.44 84.68 79.00 72.84 86.10 81.15 88.49 90.22 66.89 83.56 voyage-3 78.59 87.92 79.63 69.52 80.56 73.33 80.39 86.81 69.60 79.53 voyage-large-2 82.63 89.13 79.78 72.94 83.11 77.21 85.30 88.77 ‚Äì 84.78 voyage-multilingual-v2 76.98 87.11 78.97 67.30 80.09 71.98 78.07 86.52 67.02 75.79 \n\nTasks: Avg: Mean Spearman Correlation % for all tasks, BIO: BIOSSES, STS22: STS22v2, STSB: STSBenchmark \n\nTable A15: STS performance on MMTEB v2 (Spearman correlation %). \n\nModel Avg Faro FinPara Indic JSICK SICK-R STS12 STS13 STS14 STS15 STS17 STS22 STSB STSES SemRel \n\njina-embeddings-v4 72.70 72.28 14.43 35.22 80.33 89.23 83.50 88.61 84.77 89.69 88.71 70.71 88.58 75.31 56.46 jina-embeddings-v3 75.77 80.82 22.38 54.66 78.16 89.62 82.44 89.49 84.94 89.31 85.94 71.14 89.44 77.87 64.58 bge-m3 72.99 77.80 30.43 52.13 79.21 79.72 78.73 79.60 79.00 87.81 79.65 70.03 84.87 77.50 65.38 Cohere-embed-mult.-v3 73.77 75.95 28.24 46.73 77.19 82.18 77.62 85.16 80.02 88.92 90.09 69.36 88.79 78.76 63.84 gemini-embedding-001 78.35 86.12 28.60 62.87 84.99 82.75 81.55 89.89 85.41 90.44 88.58 71.69 89.08 81.75 73.14 text-embedding-3-large 70.17 74.96 23.51 12.59 81.24 79.00 72.84 86.10 81.15 88.49 90.22 69.29 83.56 74.20 65.25 voyage-3 68.33 72.51 22.51 41.63 71.76 79.63 69.52 80.56 73.33 80.39 76.24 71.88 79.53 72.51 64.66 voyage-multilingual-2 68.02 74.42 27.07 35.03 75.94 78.97 67.30 80.09 71.98 78.07 77.06 69.03 75.79 76.69 64.88 \n\nTasks: Avg: Mean Spearman Correlation % for all tasks, Faro: FaroeseSTS, FinPara: FinParaSTS, Indic: IndicCrosslingualSTS, STS22: STS22v2, STSB: STSBenchmark, SemRel: SemRel24STS \n\nTable A16: Performance on MTEB Code Information Retrieval (MTEB-CoIR) (nDCG@10%). \n\nModel Avg AppsR CCSN CodeMT CodeST CodeSN CodeTO CodeTD CosQA StackO SynSQL \n\njina-embeddings-v2-code 52.24 16.37 83.97 44.40 68.66 59.62 75.68 27.25 41.92 89.26 46.99 jina-embeddings-v3 55.07 29.01 ‚Äì 59.67 78.14 53.18 77.37 30.91 35.34 90.79 41.27 jina-embeddings-v4 71.59 76.08 84.05 70.60 85.06 83.69 89.34 44.19 31.48 93.45 70.45 Cohere-embed-English-3 51.36 13.72 ‚Äì 47.02 74.82 52.81 65.28 31.38 30.65 89.35 57.20 Cohere-embed-mult.-v3 54.31 31.91 ‚Äì 42.91 74.19 57.57 70.25 30.14 32.58 89.42 59.79 gemini-embedding-001 73.11 93.75 81.06 56.28 85.33 84.69 89.53 31.47 50.24 96.71 69.96 text-embedding-3-large 62.36 28.37 ‚Äì 68.92 80.42 73.18 84.25 34.23 31.00 92.44 68.45 voyage-3 67.23 73.03 ‚Äì 66.69 83.02 77.87 89.92 33.92 28.70 94.34 57.56 voyage-code-3 77.33 93.62 89.35 93.58 90.67 90.09 94.96 38.57 34.45 97.17 62.87 \n\nTasks: Avg: Mean nDCG@10% for all tasks, AppsR: AppsRetrieval, COIR: COIRCodeSearchNetRetrieval, CodeMT: CodeFeedbackMT, CodeST: CodeFeedbackST, CodeSN: CodeSearchNetCCRetrieval, CodeTO: CodeTransOceanContest, CodeTD: CodeTransOceanDL, StackO: StackOverflowQA, SynSQL: SyntheticText2SQL",
      "publishedTime": "Wed, 25 Jun 2025 01:01:18 GMT",
      "metadata": {},
      "external": {},
      "usage": {
        "tokens": 23627
      }
    },
    {
      "title": "Jina AI : How a 2020 Startup Just Beat Google, OpenAI, and Voyage at Their Own Game",
      "url": "https://medium.com/@amirabdallahpfe/jina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363",
      "description": "Jina AI is a leading artificial intelligence company founded in 2020 with a focus on neural search and multimodal AI, aiming to build an open-source ecosystem for developers and businesses. Jina‚Äôs‚Ä¶",
      "date": "Jun 29, 2025",
      "content": "Jina AI : How a 2020 Startup Just Beat Google, OpenAI, and Voyage at Their Own Game | by Paprika | Jun, 2025 | Medium\n\n===============\n\n[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9fd548bbc363&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\nJina AI : How a 2020 Startup Just Beat Google, OpenAI, and Voyage at Their Own Game\n===================================================================================\n\n[](https://medium.com/@amirabdallahpfe?source=post_page---byline--9fd548bbc363---------------------------------------)\n\n[Paprika](https://medium.com/@amirabdallahpfe?source=post_page---byline--9fd548bbc363---------------------------------------)\n\nFollow\n\n6 min read\n\n¬∑\n\n5 days ago\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F9fd548bbc363&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&user=Paprika&userId=d7c8dcc69467&source=---header_actions--9fd548bbc363---------------------clap_footer------------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9fd548bbc363&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&source=---header_actions--9fd548bbc363---------------------bookmark_footer------------------)\n\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D9fd548bbc363&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&source=---header_actions--9fd548bbc363---------------------post_audio_button------------------)\n\nShare\n\nJina Embeddings V4: The Multimodal AI Model That‚Äôs Crushing Closed-Source Competition\n\nJina AI Mission\n===============\n\nJina AI is a leading artificial intelligence company founded in 2020 with a focus on neural search and multimodal AI, aiming to build an open-source ecosystem for developers and businesses. Jina‚Äôs core mission is to make information within various data formats easily searchable and scalable.\n\nWith their series of open-source models ranging from embedding models to reranking models, they aim to democratize cutting-edge AI research in the field of search and retrieval, which helps build a vibrant ecosystem.\n\nAnd with their newest state-of-the-art embedding model ‚Äújina-embeddings-v4,‚Äù they want to push the boundaries further of what‚Äôs possible in the open-source world and challenge the closed-source labs in their field.\n\nIntroduction\n============\n\nJina AI released their new embedding model titled ‚Äújina-embeddings-v4,‚Äù a 3.8 billion parameter universal embedding model for text and images, which unifies the processing of text and images in a single pipeline, making it a true multimodal embedding model.\n\nThey also include a set of task-specific LoRA adapters that optimize performance for the most popular retrieval tasks, including query-document retrieval, semantic matching, and code search, which opens up possibilities for many real-world use cases from semantic search in your favorite Pinterest articles to your Cursor coding agent\n\nBenchmarks\n==========\n\nThe power of jina-embeddings-v4 is not only about the multimodal nature, but also its SOTA results in many retrieval, multimodal, and multilingual tasks across MTEB, MMTEB, CoIR, LongEmbed, STS, CLIP and ViDoRe benchmarks, with a particular strength in processing visually rich content such as tables, charts, diagrams, and mixtures of them.(See Figure 1.1)\n\nClosing the gap with Close Source\n=================================\n\nJina-embeddings-v4 demonstrates how rapidly the open-source community is closing the gap with closed-source solutions ‚Äî not only in the LLM space, but also in the embedding domain. Jina AI‚Äôs v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, outperforming leading closed-source embedding models from major providers.\n\nThe model delivers 12% better performance than OpenAI‚Äôs text-embedding-3-large on multilingual retrieval (66.49 vs 59.27), a 28% improvement on long document tasks (67.11 vs 52.42), 15% better performance than Voyage-3 on code retrieval (71.59 vs 67.23), and matches Google‚Äôs Gemini-embedding-001 performance.\n\nAll of this makes v4 the most capable open-source universal embedding model available in the market today, offering researchers and developers leading multimodal embedding capabilities with full transparency into the training process and architectural decision.\n\nHow Jina-embeddings-v4 Works\n============================\n\nThe model is based on Qwen2.5-VL-3B-Instruct, which has 3.8 billion parameters. It can handle both text and images using the same processing pipeline. When you give it an image, it first converts the image into text-like tokens, then processes both text and image tokens together using the same language model.\n\nThe model has three specialized add-on modules (60 million parameters each) that help it perform better on different tasks: finding relevant documents, matching text, and understanding code. These add-ons don‚Äôt change the main model ‚Äî they just provide extra fine-tuning.\n\nThe model can output results in two ways:\n\n*   **Single embedding**: One vector with 2,048 numbers (can be shortened to 128) that represents the entire input ‚Äî good for quick similarity searches\n*   **Multiple embeddings**: Many smaller vectors (128 numbers each, one per word/token) ‚Äî useful for more detailed matching between specific parts of documents\n\nThink of it like having one smart system that can read both text and images, then give you either a single ‚Äúsummary score‚Äù or detailed ‚Äúword-by-word scores‚Äù depending on what you need.\n\nThe upgrade from V3 To V4\n=========================\n\nThe upgrade from jina-embeddings-v3 to jina-embeddings-v4 represents a paradigm shift from text-only to multimodal embeddings. While v3 focused on optimizing text embeddings with task-specific LoRA adapters, v4 addresses the growing requirement for embedding both textual and visual content in unified representations.\n\nThe backbone model\n==================\n\nThe fundamental transformation in version 4 centers on replacing the XLM-RoBERTa foundation with Qwen2.5-VL-3B-Instruct as the underlying neural architecture. This strategic shift supports v4‚Äôs primary mission of developing a comprehensive embedding system that achieves authentic multimodal integration by transforming visual content into sequential tokens that can be processed simultaneously with textual data, thereby bridging the compatibility divide that exists in traditional dual-pathway architectures.\n\nThis architectural selection fulfills multiple strategic requirements: Qwen2.5-VL‚Äôs superior performance in document comprehension directly enhances v4‚Äôs capability to interpret content-rich materials including data tables, graphical representations, and interface captures. The adaptive image scaling technology empowers v4 to process visual inputs expanded to 20-megapixel resolution as outlined in the system specifications. The sophisticated spatial encoding framework establishes the groundwork enabling v4 to demonstrate exceptional multimodal synchronization, achieving a 0.71 compatibility metric versus OpenAI CLIP‚Äôs 0.15 score.\n\nThe Trick: LoRA Adapter\n=======================\n\nV4 streamlines from v3‚Äôs five tasks to three focused tasks, reflecting lessons learned about effectiveness and adapting to user feedback:\n\n*   Asymmetric retrieval (consolidating v3‚Äôs query/passage adapters)\n*   Symmetric similarity (v3‚Äôs text-matching equivalent for STS tasks)\n*   Code retrieval (learned from v2-code, missing in v3)\n\nThis consolidation removes v3‚Äôs classification and separation adapters, focusing v4 on the most impactful embedding use cases ‚Äî retrieval and STS.\n\nOutput Embeddings\n=================\n\nV4 introduces a dual-output system supporting both single-vector and multi-vector embeddings, whereas v3 only provided single-vector outputs. This addresses different retrieval scenarios:\n\n*   **Single-vector mode**: 2048-dimensional embeddings (truncatable to 128 via MRL) for efficient similarity search\n*   **Multi-vector mode**: 128 dimensions per token for late-interaction retrieval\n\nThis dual approach provides greater effectiveness with multi-vector representations, particularly in visually rich document retrieval, while maintaining efficiency for standard similarity tasks. The consistent 7‚Äì10% performance advantage of multi-vector over single-vector mode across visual tasks suggests that late interaction provides fundamentally better semantic matching for multimodal content.\n\nVibe Check\n==========\n\nNow let‚Äôs move on to some hands-on experimentation. The JinaAI team provides a quick vibe-check through their Semantic Search Web Application. You can try it out by simply typing your query and pressing Enter to see the ranked results. Your query can be either text-based or image-based. Feel free to try queries in non-English languages as well.\n\nThe demo is available at: [https://jina.ai/api-dashboard/m0-image-rerank](https://jina.ai/api-dashboard/m0-image-rerank)\n\nYou can also use jina-embeddings-v4 via the API. Using this documentation, you can pass a text string, a base64-encoded image, or an image URL. New users can get a Jina API key with 10 million free tokens.\n\nConclusion\n==========\n\nIn this blog, we try to present jina-embeddings-v4, a state-of-the-art multimodal and multilingual embedding model designed for a wide range of tasks, including semantic text retrieval, text-to-image retrieval, text-to-visually-rich document retrieval, and code search, which opens new possibilities for many world-class apps, from RAG chatbots and AI agents to coding assistance.\n\nIndex\n=====\n\n**MTEB** ‚Äî Massive Text Embedding Benchmark: Tests how well models understand and match text meaning\n\n**MMTEB** ‚Äî Multimodal Text Embedding Benchmark: Tests models on both text and image understanding together\n\n**CoIR** ‚Äî Code Information Retrieval: Measures how well models find and match programming code\n\n**LongEmbed** ‚Äî Tests model performance on very long documents and texts\n\n**STS** ‚Äî Semantic Textual Similarity: Measures how well models identify when different texts mean the same thing\n\n**CLIP** ‚Äî Contrastive Language-Image Pre-training: Tests how well models connect images with their text descriptions\n\n**ViDoRe** ‚Äî Visual Document Retrieval: Tests model ability to find and understand documents with charts, tables, and images\n\n**LoRA Adapters** ‚Äî Low-Rank Adaptation: Small add-on modules that help the main model perform better on specific tasks\n\n**Multimodal** ‚Äî Ability to process both text and images in the same system\n\n**Embedding** ‚Äî Converting text or images into numbers that computers can compare and search through\n\n**XLM-RoBERTa** ‚Äî A previous text-only AI model used in version 3\n\n**Qwen2.5-VL** ‚Äî The newer AI model foundation used in version 4 that handles both text and images\n\n[AI](https://medium.com/tag/ai?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Llm](https://medium.com/tag/llm?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Generative Ai Tools](https://medium.com/tag/generative-ai-tools?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Data Science](https://medium.com/tag/data-science?source=post_page-----9fd548bbc363---------------------------------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F9fd548bbc363&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&user=Paprika&userId=d7c8dcc69467&source=---footer_actions--9fd548bbc363---------------------clap_footer------------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F9fd548bbc363&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&user=Paprika&userId=d7c8dcc69467&source=---footer_actions--9fd548bbc363---------------------clap_footer------------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9fd548bbc363&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&source=---footer_actions--9fd548bbc363---------------------bookmark_footer------------------)\n\n[](https://medium.com/@amirabdallahpfe?source=post_page---post_author_info--9fd548bbc363---------------------------------------)\n\n[](https://medium.com/@amirabdallahpfe?source=post_page---post_author_info--9fd548bbc363---------------------------------------)\n\nFollow\n\n[Written by Paprika ------------------](https://medium.com/@amirabdallahpfe?source=post_page---post_author_info--9fd548bbc363---------------------------------------)\n\n[1 follower](https://medium.com/@amirabdallahpfe/followers?source=post_page---post_author_info--9fd548bbc363---------------------------------------)\n\n¬∑[2 following](https://medium.com/@amirabdallahpfe/following?source=post_page---post_author_info--9fd548bbc363---------------------------------------)\n\nOn a mission to create the best tech content you'll ever need\n\nFollow\n\nNo responses yet\n----------------\n\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--9fd548bbc363---------------------------------------)\n\nWrite a response\n\n[What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fjina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363&source=---post_responses--9fd548bbc363---------------------respond_sidebar------------------)\n\nCancel\n\nRespond\n\nMore from Paprika\n-----------------\n\n[](https://medium.com/@amirabdallahpfe?source=post_page---author_recirc--9fd548bbc363----0---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[Paprika](https://medium.com/@amirabdallahpfe?source=post_page---author_recirc--9fd548bbc363----0---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[Context Engineering: The Real Skill Behind AI Success ----------------------------------------------------- ### The AI world is buzzing with a new term that‚Äôs changing how we think about building with language models. It started with a simple tweet‚Ä¶](https://medium.com/@amirabdallahpfe/context-engineering-the-real-skill-behind-ai-success-be145782dc15?source=post_page---author_recirc--9fd548bbc363----0---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n4d ago\n\n[](https://medium.com/@amirabdallahpfe/context-engineering-the-real-skill-behind-ai-success-be145782dc15?source=post_page---author_recirc--9fd548bbc363----0---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe145782dc15&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fcontext-engineering-the-real-skill-behind-ai-success-be145782dc15&source=---author_recirc--9fd548bbc363----0-----------------bookmark_preview----176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[](https://medium.com/@amirabdallahpfe?source=post_page---author_recirc--9fd548bbc363----1---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[Paprika](https://medium.com/@amirabdallahpfe?source=post_page---author_recirc--9fd548bbc363----1---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[Let‚Äôs Build a DeepSearch Agent: Using Agno, FireCrawl, Nebuis AI, DeepSeek V3, and Streamlit -------------------------------------------------------------------------------------------- ### Imagine you want to search for critical information for your next project. You open your PC, type the first search query, then open another‚Ä¶](https://medium.com/@amirabdallahpfe/lets-build-a-deepsearch-agent-using-agno-firecrawl-nebuis-ai-deepseek-v3-and-streamlit-20cf5231f511?source=post_page---author_recirc--9fd548bbc363----1---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n1d ago\n\n[](https://medium.com/@amirabdallahpfe/lets-build-a-deepsearch-agent-using-agno-firecrawl-nebuis-ai-deepseek-v3-and-streamlit-20cf5231f511?source=post_page---author_recirc--9fd548bbc363----1---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20cf5231f511&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Flets-build-a-deepsearch-agent-using-agno-firecrawl-nebuis-ai-deepseek-v3-and-streamlit-20cf5231f511&source=---author_recirc--9fd548bbc363----1-----------------bookmark_preview----176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[](https://medium.com/@amirabdallahpfe?source=post_page---author_recirc--9fd548bbc363----2---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[Paprika](https://medium.com/@amirabdallahpfe?source=post_page---author_recirc--9fd548bbc363----2---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[Databricks Certified Data Engineer Associate-Day-1 -------------------------------------------------- ### In the data-driven world, all data practitioners ‚Äî whether you‚Äôre a data engineer, data architect, data analyst, or even a leader like a‚Ä¶](https://medium.com/@amirabdallahpfe/databricks-certified-data-engineer-associate-day-1-a84186cf14eb?source=post_page---author_recirc--9fd548bbc363----2---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\nNov 12, 2024\n\n[6](https://medium.com/@amirabdallahpfe/databricks-certified-data-engineer-associate-day-1-a84186cf14eb?source=post_page---author_recirc--9fd548bbc363----2---------------------176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa84186cf14eb&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40amirabdallahpfe%2Fdatabricks-certified-data-engineer-associate-day-1-a84186cf14eb&source=---author_recirc--9fd548bbc363----2-----------------bookmark_preview----176795f9_362c_4812_802e_23d0ad2c467f--------------)\n\n[See all from Paprika](https://medium.com/@amirabdallahpfe?source=post_page---author_recirc--9fd548bbc363---------------------------------------)\n\nRecommended from Medium\n-----------------------\n\n[](https://medium.com/utopian?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nIn\n\n[Utopian](https://medium.com/utopian?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nby\n\n[Derick David](https://medium.com/@jeazous?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[CEOs Are Literally Begging to Hire Anyone With This One Skill ------------------------------------------------------------- ### This is the hottest AI job in 2025.](https://medium.com/utopian/ceos-are-literally-begging-to-hire-anyone-with-this-one-skill-ec016bc4d46d?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nJun 27\n\n[1.3K 70](https://medium.com/utopian/ceos-are-literally-begging-to-hire-anyone-with-this-one-skill-ec016bc4d46d?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fec016bc4d46d&operation=register&redirect=https%3A%2F%2Fmedium.com%2Futopian%2Fceos-are-literally-begging-to-hire-anyone-with-this-one-skill-ec016bc4d46d&source=---read_next_recirc--9fd548bbc363----0-----------------bookmark_preview----f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/data-science-in-your-pocket?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nIn\n\n[Data Science in Your Pocket](https://medium.com/data-science-in-your-pocket?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nby\n\n[Mehul Gupta](https://medium.com/@mehulgupta_7991?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[Context Engineering vs Prompt Engineering ----------------------------------------- ### How Context Engineering different from Prompt Engineering](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nJun 28\n\n[514 3](https://medium.com/data-science-in-your-pocket/context-engineering-vs-prompt-engineering-379e9622e19d?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F379e9622e19d&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fdata-science-in-your-pocket%2Fcontext-engineering-vs-prompt-engineering-379e9622e19d&source=---read_next_recirc--9fd548bbc363----1-----------------bookmark_preview----f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/gitconnected?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nIn\n\n[Level Up Coding](https://medium.com/gitconnected?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nby\n\n[Sam Enfield](https://medium.com/@dhruvam?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[Why OpenAI Suddenly Erased Jony Ive from their Website ------------------------------------------------------ ### A billion-dollar collaboration‚Ä¶ quietly wiped out overnight. Here‚Äôs what happened.](https://medium.com/gitconnected/why-openai-suddenly-erased-jony-ive-from-their-website-5d6f431e5297?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nJun 25\n\n[1.7K 54](https://medium.com/gitconnected/why-openai-suddenly-erased-jony-ive-from-their-website-5d6f431e5297?source=post_page---read_next_recirc--9fd548bbc363----0---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d6f431e5297&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-openai-suddenly-erased-jony-ive-from-their-website-5d6f431e5297&source=---read_next_recirc--9fd548bbc363----0-----------------bookmark_preview----f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/@tam.tamanna18?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[Tamanna](https://medium.com/@tam.tamanna18?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[Building Next-Gen AI Agents with Google ADK, MCP, RAG, and Ollama ----------------------------------------------------------------- ### Introduction](https://medium.com/@tam.tamanna18/building-next-gen-ai-agents-with-google-adk-mcp-rag-and-ollama-ca3c1e5002da?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nJun 27\n\n[716 3](https://medium.com/@tam.tamanna18/building-next-gen-ai-agents-with-google-adk-mcp-rag-and-ollama-ca3c1e5002da?source=post_page---read_next_recirc--9fd548bbc363----1---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca3c1e5002da&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tam.tamanna18%2Fbuilding-next-gen-ai-agents-with-google-adk-mcp-rag-and-ollama-ca3c1e5002da&source=---read_next_recirc--9fd548bbc363----1-----------------bookmark_preview----f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/javascript-in-plain-english?source=post_page---read_next_recirc--9fd548bbc363----2---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nIn\n\n[JavaScript in Plain English](https://medium.com/javascript-in-plain-english?source=post_page---read_next_recirc--9fd548bbc363----2---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nby\n\n[Hassan Trabelsi](https://medium.com/@GeekSociety?source=post_page---read_next_recirc--9fd548bbc363----2---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[I Tried Google‚Äôs New Gemini CLI. It‚Äôs the Most Powerful Open-Source Dev Tool ---------------------------------------------------------------------------- ### Google quietly released a local AI agent that builds apps, debugs code, parses your repo, and fetches real-time data, right inside your‚Ä¶](https://medium.com/javascript-in-plain-english/i-tried-googles-new-gemini-cli-it-s-the-most-powerful-open-source-dev-tool-e8c35ee338a6?source=post_page---read_next_recirc--9fd548bbc363----2---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nJun 27\n\n[1.3K 35](https://medium.com/javascript-in-plain-english/i-tried-googles-new-gemini-cli-it-s-the-most-powerful-open-source-dev-tool-e8c35ee338a6?source=post_page---read_next_recirc--9fd548bbc363----2---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8c35ee338a6&operation=register&redirect=https%3A%2F%2Fjavascript.plainenglish.io%2Fi-tried-googles-new-gemini-cli-it-s-the-most-powerful-open-source-dev-tool-e8c35ee338a6&source=---read_next_recirc--9fd548bbc363----2-----------------bookmark_preview----f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/ai-advances?source=post_page---read_next_recirc--9fd548bbc363----3---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nIn\n\n[AI Advances](https://medium.com/ai-advances?source=post_page---read_next_recirc--9fd548bbc363----3---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nby\n\n[Dr. Ashish Bamania](https://medium.com/@bamania-ashish?source=post_page---read_next_recirc--9fd548bbc363----3---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[LLMs Can Now Be Pre-Trained Using Pure Reinforcement Learning ------------------------------------------------------------- ### A deep dive into Reinforcement Pre-Training (RPT), a new technique introduced by Microsoft researchers to scalably pre-train LLMs using RL.](https://medium.com/ai-advances/llms-can-now-be-pre-trained-using-pure-reinforcement-learning-7676f579cbee?source=post_page---read_next_recirc--9fd548bbc363----3---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\nJun 26\n\n[796 6](https://medium.com/ai-advances/llms-can-now-be-pre-trained-using-pure-reinforcement-learning-7676f579cbee?source=post_page---read_next_recirc--9fd548bbc363----3---------------------f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7676f579cbee&operation=register&redirect=https%3A%2F%2Fai.gopubby.com%2Fllms-can-now-be-pre-trained-using-pure-reinforcement-learning-7676f579cbee&source=---read_next_recirc--9fd548bbc363----3-----------------bookmark_preview----f0a2aa23_ad04_40ab_9e97_996d313aec30--------------)\n\n[See more recommendations](https://medium.com/?source=post_page---read_next_recirc--9fd548bbc363---------------------------------------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----9fd548bbc363---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----9fd548bbc363---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9fd548bbc363---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----9fd548bbc363---------------------------------------)",
      "publishedTime": "2025-06-29T12:27:43.868Z",
      "metadata": {
        "viewport": "width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1",
        "theme-color": "#000000",
        "twitter:app:name:iphone": "Medium",
        "twitter:app:id:iphone": "828256236",
        "al:ios:app_name": "Medium",
        "al:ios:app_store_id": "828256236",
        "al:android:package": "com.medium.reader",
        "fb:app_id": "542599432471018",
        "og:site_name": "Medium",
        "apple-itunes-app": "app-id=828256236, app-argument=/@amirabdallahpfe/jina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363, affiliate-data=pt=698524&ct=smart_app_banner&mt=8",
        "og:type": "article",
        "article:published_time": "2025-06-29T12:27:43.868Z",
        "title": "Jina AI : How a 2020 Startup Just Beat Google, OpenAI, and Voyage at Their Own Game | by Paprika | Jun, 2025 | Medium",
        "og:title": "Jina AI : How a 2020 Startup Just Beat Google, OpenAI, and Voyage at Their Own Game",
        "al:android:url": "medium://p/9fd548bbc363",
        "al:ios:url": "medium://p/9fd548bbc363",
        "al:android:app_name": "Medium",
        "description": "Jina AI is a leading artificial intelligence company founded in 2020 with a focus on neural search and multimodal AI, aiming to build an open-source ecosystem for developers and businesses. Jina‚Äôs‚Ä¶",
        "og:description": "Jina Embeddings V4: The Multimodal AI Model That‚Äôs Crushing Closed-Source Competition",
        "og:url": "https://medium.com/@amirabdallahpfe/jina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363",
        "al:web:url": "https://medium.com/@amirabdallahpfe/jina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363",
        "og:image": "https://miro.medium.com/v2/resize:fit:1200/1*Xyh8del3bz66P1BGhTSs_g.png",
        "article:author": "https://medium.com/@amirabdallahpfe",
        "author": "Paprika",
        "robots": "index,noarchive,follow,max-image-preview:large",
        "referrer": "unsafe-url",
        "twitter:title": "Jina AI : How a 2020 Startup Just Beat Google, OpenAI, and Voyage at Their Own Game",
        "twitter:site": "@Medium",
        "twitter:app:url:iphone": "medium://p/9fd548bbc363",
        "twitter:description": "Jina Embeddings V4: The Multimodal AI Model That‚Äôs Crushing Closed-Source Competition",
        "twitter:image:src": "https://miro.medium.com/v2/resize:fit:1200/1*Xyh8del3bz66P1BGhTSs_g.png",
        "twitter:card": "summary_large_image",
        "twitter:label1": "Reading time",
        "twitter:data1": "6 min read"
      },
      "external": {
        "icon": {
          "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19": {
            "data-rh": "true"
          }
        },
        "search": {
          "https://medium.com/osd.xml": {
            "data-rh": "true",
            "type": "application/opensearchdescription+xml",
            "title": "Medium"
          }
        },
        "preconnect": {
          "https://glyph.medium.com/": {
            "data-rh": "true",
            "crossorigin": ""
          }
        },
        "manifest": {
          "https://medium.com/manifest.json": {
            "data-rh": "true"
          }
        },
        "preload": {
          "https://glyph.medium.com/css/unbound.css": {
            "data-rh": "true",
            "id": "glyph_preload_link",
            "as": "style",
            "type": "text/css"
          }
        },
        "author": {
          "https://medium.com/@amirabdallahpfe": {
            "data-rh": "true"
          }
        },
        "canonical": {
          "https://medium.com/@amirabdallahpfe/jina-ai-how-a-2020-startup-just-beat-google-openai-and-voyage-at-their-own-game-9fd548bbc363": {
            "data-rh": "true"
          }
        },
        "alternate": {
          "android-app://com.medium.reader/https/medium.com/p/9fd548bbc363": {
            "data-rh": "true"
          }
        },
        "apple-touch-icon": {
          "https://miro.medium.com/v2/resize:fill:152:152/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156": {
            "sizes": "152x152",
            "data-rh": "true"
          },
          "https://miro.medium.com/v2/resize:fill:120:120/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156": {
            "sizes": "120x120",
            "data-rh": "true"
          },
          "https://miro.medium.com/v2/resize:fill:76:76/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156": {
            "sizes": "76x76",
            "data-rh": "true"
          },
          "https://miro.medium.com/v2/resize:fill:60:60/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156": {
            "sizes": "60x60",
            "data-rh": "true"
          }
        },
        "mask-icon": {
          "https://miro.medium.com/v2/resize:fill:500:500/7*GAOKVe--MXbEJmV9230oOQ.png": {
            "color": "#171717",
            "data-rh": "true"
          }
        }
      },
      "usage": {
        "tokens": 8327
      }
    },
    {
      "title": "The AI Timeline on X: \"jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval\n\nAuthor's Explanation:\nhttps://t.co/doZKYD35I9\n\nOverview:\nJina-embeddings-v4 introduces a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel https://t.co/7Qj4SjtVsV\" / X",
      "url": "https://x.com/TheAITimeline/status/1939497033301139777",
      "description": "",
      "date": "Jun 30, 2025",
      "content": "The AI Timeline on X: \"jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval Author's Explanation: https://t.co/doZKYD35I9 Overview: Jina-embeddings-v4 introduces a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel https://t.co/7Qj4SjtVsV\" / X\n\n===============\n\nDon‚Äôt miss what‚Äôs happening\n\nPeople on X are the first to know.\n\n[Log in](https://x.com/login)\n\n[Sign up](https://x.com/i/flow/signup)\n\n[](https://x.com/)\n==================\n\nPost\n----\n\nSee new posts\n\nConversation\n============\n\n[](https://x.com/TheAITimeline)\n\n[The AI Timeline](https://x.com/TheAITimeline)\n\n[@TheAITimeline](https://x.com/TheAITimeline)\n\njina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval Author's Explanation: [https://x.com/JinaAI_/status/1937880654127226929‚Ä¶](https://x.com/JinaAI_/status/1937880654127226929) Overview: Jina-embeddings-v4 introduces a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture leveraging single-vector and multi-vector late interaction embeddings and task-specific LoRA adapters, demonstrating state-of-the-art performance in diverse retrieval scenarios, particularly excelling with visually rich content, and contributing the Jina-VDR benchmark. Paper: [https://arxiv.org/abs/2506.18902](https://t.co/7JNRp8a64I)\n\n[](https://x.com/TheAITimeline/status/1939497033301139777/photo/1)\n\nQuote\n\nJina AI\n\n@JinaAI_\n\n¬∑\n\nJun 25\n\nToday we're releasing jina-embeddings-v4, our new 3.8B universal embedding model for retrieving text, images, visual documents and code. V4 achieves state-of-the-art retrieval performance on multimodal and multilingual tasks across MTEB, MMTEB, CoIR, LongEmbed, STS, Jina-VDR,\n\n[Show more](https://x.com/JinaAI_/status/1937880654127226929)\n\nThe media could not be played.\n\nReload\n\n[1:31 AM ¬∑ Jun 30, 2025](https://x.com/TheAITimeline/status/1939497033301139777)\n\n¬∑\n\n898\n\nViews\n\n1\n\n1\n\n8\n\n2\n\nNew to X?\n---------\n\nSign up now to get your own personalized timeline!\n\nSign up with Apple\n\n[Create account](https://x.com/i/flow/signup)\n\nBy signing up, you agree to the [Terms of Service](https://x.com/tos) and [Privacy Policy](https://x.com/privacy), including [Cookie Use.](https://help.x.com/rules-and-policies/twitter-cookies)\n\nSomething went wrong. Try reloading.\n\nRetry\n\n[Terms of Service](https://x.com/tos)\n\n|\n\n[Privacy Policy](https://x.com/privacy)\n\n|\n\n[Cookie Policy](https://support.x.com/articles/20170514)\n\n|\n\n[Accessibility](https://help.x.com/resources/accessibility)\n\n|\n\n[Ads info](https://business.x.com/en/help/troubleshooting/how-twitter-ads-work.html?ref=web-twc-ao-gbl-adsinfo&utm_source=twc&utm_medium=web&utm_campaign=ao&utm_content=adsinfo)\n\n|\n\nMore\n\n¬© 2025 X Corp.",
      "publishedTime": "Sat, 05 Jul 2025 12:34:34 GMT",
      "metadata": {
        "viewport": "width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover",
        "fb:app_id": "2231777543",
        "og:site_name": "X (formerly Twitter)",
        "google-site-verification": "reUF-TgZq93ZGtzImw42sfYglI2hY0QiGRmfc4jeKbs",
        "facebook-domain-verification": "x6sdcc8b5ju3bh8nbm59eswogvg6t1",
        "mobile-web-app-capable": "yes",
        "apple-mobile-web-app-title": "Twitter",
        "apple-mobile-web-app-status-bar-style": "white",
        "twitter-site-verification": "aJKu0v9SEcP5I5piamHJrxqDp5iEEL1lUzQuEolHK1GegPN6jAp6n/ibjzEtmsbG",
        "theme-color": [
          "#FFFFFF",
          "#000000"
        ],
        "og:image": "https://abs.twimg.com/responsive-web/client-web/icon-ios.77d25eba.png",
        "apple-itunes-app": "app-id=333903271",
        "og:title": "The AI Timeline on X: \"jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval\n\nAuthor's Explanation:\nhttps://t.co/doZKYD35I9\n\nOverview:\nJina-embeddings-v4 introduces a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel https://t.co/7Qj4SjtVsV\" / X"
      },
      "external": {
        "preconnect": {
          "https://abs.twimg.com/": {},
          "https://api.twitter.com/": {},
          "https://api.x.com/": {},
          "https://pbs.twimg.com/": {},
          "https://t.co/": {},
          "https://video.twimg.com/": {}
        },
        "dns-prefetch": {
          "https://abs.twimg.com/": {},
          "https://api.twitter.com/": {},
          "https://api.x.com/": {},
          "https://pbs.twimg.com/": {},
          "https://t.co/": {},
          "https://video.twimg.com/": {}
        },
        "preload": {
          "https://abs.twimg.com/responsive-web/client-web/vendor-7940b00b.aafae34a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-adcb47af.21d62e8a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-38c57b44.7266e9ba.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-85aa29ea.b9ec208a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-cb2d071c.790f8aca.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-aaaf2b0c.5c59c02a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-49d0a293.b1a6b16a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-6b20cc7c.3951c3aa.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-dfe82965.8f17b49a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-e395fecc.40e2e8aa.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-63e37921.8411268a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-de539588.b967545a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-669c86db.f6db5f1a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-3dfac8a4.346d055a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-27545368.77b8a3ba.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-58c6fc15.0f415a1a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-57216f32.d6191e3a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-e5bca7e4.1458567a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-744bed60.79286f6a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-3e5eb623.067d49da.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-98a766dd.9802f00a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-2bf3abf4.f8393bda.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-ec4c1ee7.89c8b22a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-c22f700c.0254af5a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-bfc04956.5b6ec3fa.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-48a4958c.22189dca.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-eb8eaa08.b1c15f8a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-91c40cd8.e33ef86a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-c4d1d074.c9bdc52a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-ccf8c62e.f0cf07fa.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-821262ff.fc6cf03a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-49ceb22a.1a89ac1a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/i18n/en.5a97e14a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/main.7359a36a.js": {
            "nonce": "NWMzYjgzZWYtZTJlOS00NTczLWE5NDgtNmU4MWRiZjliMzdl",
            "as": "script",
            "crossorigin": "anonymous"
          }
        },
        "search": {
          "https://x.com/os-x.xml": {
            "type": "application/opensearchdescription+xml",
            "title": "X"
          },
          "https://x.com/os-grok.xml": {
            "type": "application/opensearchdescription+xml",
            "title": "Grok"
          }
        },
        "apple-touch-icon": {
          "https://abs.twimg.com/responsive-web/client-web/icon-ios.77d25eba.png": {
            "sizes": "192x192"
          }
        },
        "manifest": {
          "https://x.com/manifest.json": {
            "crossorigin": "use-credentials"
          }
        },
        "mask-icon": {
          "https://abs.twimg.com/responsive-web/client-web/icon-svg.ea5ff4aa.svg": {
            "sizes": "any",
            "color": "#1D9BF0"
          }
        },
        "icon": {
          "https://abs.twimg.com/favicons/twitter.3.ico": {}
        },
        "canonical": {
          "https://x.com/TheAITimeline/status/1939497033301139777": {
            "data-rh": "true"
          }
        },
        "alternate": {
          "https://publish.x.com/oembed?url=https://x.com/TheAITimeline/status/1939497033301139777": {
            "title": "The AI Timeline on X: \"jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval\n\nAuthor's Explanation:\nhttps://t.co/doZKYD35I9\n\nOverview:\nJina-embeddings-v4 introduces a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel https://t.co/7Qj4SjtVsV\"",
            "type": "application/json+oembed",
            "data-rh": "true"
          }
        }
      },
      "usage": {
        "tokens": 747
      }
    },
    {
      "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
      "url": "https://www.emergentmind.com/articles/2506.18902",
      "description": "Explore jina-embeddings-v4, a 3.8B parameter model unifying text and image representations for advanced multimodal retrieval tasks.",
      "date": "Jun 23, 2025",
      "content": "> arXiv:2506.18902v2 [cs.AI] 24 Jun 2025\n\n# jina-embeddings-v4 : Universal Embeddings for Multimodal Multilingual Retrieval \n\nMichael G√ºnther ‚àó, Saba Sturua ‚àó, Mohammad Kalim Akram ‚àó,Isabelle Mohr ‚àó, Andrei Ungureanu ‚àó, Bo Wang ‚àó, Sedigheh Eslami , Scott Martens ,\n\nMaximilian Werk , Nan Wang and Han Xiao \n\nJina AI GmbH, Prinzessinnenstra√üe 19, 10969, Berlin, Germany \n\nresearch@jina.ai \n\nAbstract \n\nWe introduce jina-embeddings-v4 , a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incor-porates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text sim-ilarity, and code search. Comprehensive evalu-ations demonstrate that jina-embeddings-v4 \n\nachieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval. \n\n1 Introduction \n\nWe present jina-embeddings-v4 , a multimodal embedding model capable of processing text and image data to produce semantic embedding vectors of varying lengths, optimized for a broad array of applications. It incorporates optimized LoRA adapters [Hu et al., 2022] for information retrieval and semantic text similarity. An adapter is also provided for programming language embeddings, technical question-answering, and natural language code retrieval. It also brings new functionality to processing visually rich images (also called visual documents), i.e., materials mixing texts and images, containing tables, charts, diagrams, and other kinds of common mixed media [Ding et al., 2024]. We have also developed Jina-VDR, a new multilingual, multi-domain benchmark suite for a broad range of visual retrieval tasks, to evaluate the capabilities of jina-embeddings-v4 .\n\n> *Equal contribution.\n\nWe discuss the challenges of developing a multimodal, multi-functional, state-of-the-art embedding model capable of handling texts in a variety of languages, including computer coding languages, images, and ‚Äúvisually rich‚Äù data. The resulting model, jina-embeddings-v4 , projects inputs from all modalities into a unified semantic space, minimizing or eliminating the ‚Äúmodality gap‚Äù that has troubled similar projects [Liang et al., 2022a]. In addition, we introduce Jina-VDR, an advanced benchmark for images like screenshots and scans of visually complex documents. The major contributions of this work are as follows: ‚Ä¢ We introduce a unified multi-task learning paradigm that jointly optimizes embedding models to represent texts and images as single-and multi-vector embeddings. ‚Ä¢ Building on work done for \n\njina-embeddings-v3 , we train LoRA extensions to enhance support for specific domains and task types, achieving results comparable to specialized models. ‚Ä¢ We have made particularly strong progress in handling visually rich images, especially for tasks outside of the existing ViDoRe bench-mark [Faysse et al., 2025], which is limited to question-answering. jina-embeddings-v4 \n\noutperforms other multimodal models by a sig-nificant margin on this type of material and sup-ports a much more diverse set of use scenarios. ‚Ä¢ We construct a multilingual, multi-domain benchmark for screenshot retrieval. In contrast to other retrieval benchmarks (i.e., [Faysse et al., 2025, Xiao et al., 2025]) that focus on question answering and OCR-related tasks, we expand the scope of visual document benchmarking to multilingual retrieval, more query types, and a much more diverse array of materials, like maps, diagrams, advertisements, and other mixed media. \n\n2 Background \n\nThe underlying principles behind embedding mod-els are indifferent to data modality. An embedding model transforms digitally encoded objects into vectors in a high-dimensional embedding space such that some of the semantic features of the objects, depending on the model‚Äôs training regimen, correspond to subspaces in that embedding space. Objects with more such features in common will have corresponding vectors that are closer to each other by some metric (typically cosine similarity) than objects with fewer common features. Individual models, however, only support the modalities for which they are designed and trained. Embedding models initially developed primarily to support natural language texts, like \n\njina-embeddings-v3 [Sturua et al., 2024], but there are many image embedding models, and more recently, audio and video models. The semantic embedding paradigm can also encompass models that support more than one modality, like bimodal image-text models, including OpenAI‚Äôs CLIP [Rad-ford et al., 2021] and subsequent developments including jina-clip [Koukounas et al., 2024]. The principal purpose of multimodal embedding models is to project objects from multiple modalities into the same semantic embedding space, so that, for example, a picture of a cat and a text discussing or describing a cat will correspond to relatively close embedding vectors. Embedding models can also specialize in specific types of input within a single modality. There are text embedding models designed for programming code [Liu et al., 2024], legal texts [Voyage AI, 2024], and other special domains. There is also recent work in specialized image embedding models designed to support ‚Äúvisually rich‚Äù data, such as screenshots, charts, and printed pages that combine text and imagery and have internal visual structure [Faysse et al., 2025, Ma et al., 2024]. There are other dimensions of embedding model specialization as well. Models can be optimized for specific tasks, such as information retrieval, clus-tering, and classification [Sturua et al., 2024]. They can also vary based on the nature of the embeddings they produce. Most are single-/dense vector models, generating one embedding vector for whatever input they are given. There are also multi-vector/late interaction models, such as ColBERT [Khattab and Zaharia, 2020] and ColPali [Faysse et al., 2025]. Late interaction is generally a more precise measure of semantic similarity for retrieval, but has significantly greater storage and computing costs. Instead of specializing, jina-embeddings-v4 \n\nbuilds on a single base model to provide competitive performance as a text, image, and cross-modal em-bedding model with strong performance handling visually rich documents. It supports both single-vector and multi-vector and is optimized to provide embeddings of varying lengths. Furthermore, the model includes LoRA extensions that optimize it for specific application classes: information retrieval, multimodal semantic similarity, and computer code retrieval. This single-model approach entails significant savings in practical use cases when compared to deploying multiple AI models for different tasks and modalities. \n\n3 Related Work \n\nTransformer-based neural network architectures that generate semantic embeddings are well-established [Reimers and Gurevych, 2019], and there is a sizable literature on training techniques for them. Multi-stage contrastive training [Wang et al., 2022], and techniques for supporting longer texts [G√ºnther et al., 2023] are particularly relevant to this work. Compact embedding vectors bring valuable performance benefits to AI applications, and this motivates work in Matryoshka Representational Learning (MRL) [Kusupati et al., 2022] as a way to train models for truncatable embedding vectors. Contrastive text-image training has led to ground-breaking results in zero-shot image classification and cross-modal retrieval in conjunction with dual encoder architectures like CLIP [Radford et al., 2021]. However, recent work shows better performance from vision-language models (VLMs) like Qwen2.5-VL-3B-Instruct [Bai et al., 2025]. Jiang et al. [2024] show that VLMs suffer less from a modality gap than dual encoder architectures. In contrast to [Zhang et al., 2024], \n\njina-embeddings-v4 is trained on multilin-gual data, supports single as well as multi-vector retrieval, and does not require task-specific instruc-tions. Other VLM models are trained exclusively on data for text-to-image [Faysse et al., 2025, Ma et al., 2024] or text-to-text retrieval [Jiang et al., 2024]. Similarity scoring in late interaction models does not use simple cosine similarity. [Khattab and Zaharia, 2020] Instead, similarity is calculated asymmetrically over two sequences of token embed-dings ‚Äî a query and a document ‚Äî by summing up the maximum cosine similarity values of each query token embedding to any of the token embeddings from the document. Thus, for query embedding q\n\nand document embedding p, their late interaction similarity score slate (q,p ) is determined by: \n\nslate (q,p ) = \n\n> n\n\nX\n\n> i=1\n\nmax \n\n> j‚àà{ 1,...,m }\n\nqi ¬∑pT \n\n> j\n\n(1) Faysse et al. [2025] train a late-interaction embed-ding model to search document screenshots using text queries, performing significantly better than traditional approaches involving OCR and CLIP-style models trained on image captions. To show this, they introduce the ViDoRe (Vision Document Retrieval) benchmark. However, this benchmark is limited to question-answering tasks in English and French involving only charts, tables, and pages from PDF documents. Xiao et al. [2025] extend this benchmark to create MIEB (Massive Image Em-bedding Benchmark) by adding semantic textual similarity (STS) tasks for visually rich documents like screenshots. \n\n4 Model Architecture \n\nThe architecture of jina-embeddings-v4 ,schematized in Figure 1, employs a uni-fied multimodal language model built on the \n\nQwen2.5-VL-3B-Instruct 1 backbone [Bai et al., 2025]. Text and image inputs are processed through a shared pathway: Images are first converted to token sequences via a vision encoder, then both modalities are jointly processed by the language model decoder with contextual attention layers. This unified design eliminates the modality gap present in dual-encoder architectures while maintaining competitive performance across text, image, and cross-modal tasks. As shown in Figure 1, this architecture supports dual output modes, as outlined in Section 4.2. Fur-thermore, three task-specific LoRA adapters, each with 60M parameters, provide specialized task op-timization without modifying the frozen backbone weights. These are described in Section 4.3.            \n\n> 1https://huggingface .co/Qwen/Qwen2 .5-VL-3B-Instruct Model Parameters 3.8 billion ( 3.8√ó10 9) plus 60M per LoRA\n> Text Input Size Up to 32,768 tokens\n> Image Input All images resized to 20 megapixels\n> Single-vector Embedding Size 2048 dimensions, truncatable down to 128\n> Multi-vector Embedding Size 128 dimensions per token\n> Table 1: Basic specifications of jina-embeddings-v4\n\nThe core specifications of jina-embeddings-v4 \n\nare summarized in Table 1. \n\n4.1 True Multimodal Processing \n\nThe Qwen2.5-VL-3B-Instruct paradigm differs from CLIP-style dual-encoder models in offering a single processing path that‚Äôs truly multimodal. For text input, jina-embeddings-v4 and \n\nQwen2.5-VL-3B-Instruct initially behave like other transformer-based embedding models: The text is tokenized, each token is replaced with a vector representation from a lookup table, and then these vectors are stacked and presented to a large language model (LLM). In CLIP-style models, images are processed by a separate embedding model, typically atransformer-based model that divides them into patches and then processes them much like a text model. The text model and image model are aligned during training to produce similar embeddings for similar semantic content in the different media. The Qwen2.5-VL-3B-Instruct paradigm used in jina-embeddings-v4 also includes a discrete image model but uses it in a different way. It pro-duces a multi-vector result, comparable to late in-teraction models, and then passes this output to the LLM, as if it were a sequence of vectorized text to-kens. The image embedding model acts as a prepro-cessor for the LLM, converting the image into what amounts to a sequence of vectorized ‚Äúimage tokens.‚Äù This approach, which is the core of \n\njina-embeddings-v4 , beyond having perfor-mance advantages, makes it possible to pass a text prompt into the LLM along with an image. The VLM is truly multimodal, since it is one model supporting multiple data types in a single input field. \n\n4.2 Dual Mode Output \n\nIn contrast to Qwen2.5-VL-3B-Instruct and other embedding models in general, users can choose be-tween two output options: Traditional single (dense) vision encoder \n\nqwen2.5 LM decoder        \n\n> task= 'retrieval' doc= image OR text vector_type= 'multi_vector' input\n> output\n> Lora set base model\n> [retrieval] / [text-\n> matching] / [code search]\n> token embeddings mean pooling projector\n> single-vector\n> 128 to 2048-dim\n> multi-vector\n> N x 128-dim\n\nFigure 1: Architecture of jina-embeddings-v4 . The model employs a unified LM built on the Qwen2.5-VL-3B-Instruct backbone (3.8B parameters). Text and image inputs are processed through a shared pathway: images are first converted to token sequences via a vision encoder, then both modalities are jointly processed by the language model decoder with contextual attention layers. Three task-specific LoRA adapters (60M parameters each) provide specialized optimization for retrieval, text-matching, and code search tasks without modifying the frozen backbone weights. The architecture supports dual output modes: (1) single-vector embeddings (2048 dimensions, truncatable to 128) generated via mean pooling for efficient similarity search, and (2) multi-vector embeddings (128 dimensions per token) via projection layers for the late interaction style retrieval. \n\nvector embeddings and ColBERT-style multi-vector embeddings for late interaction strategies. Single-vector embeddings are 2048 dimensions, but can be truncated to as little as 128 with minimal loss of precision. jina-embeddings-v4 has been trained with Matryoshka Representation Learn-ing [Kusupati et al., 2022], so that the scalar values of single-vector embeddings are roughly ordered by semantic significance. Eliminating the least significant dimensions reduces precision very little. Multi-vector embeddings are the unpooled result of processing tokens through a transformer model. They correspond to tokens as the model analyses them, given their context. The length of the output vector is proportionate to the number of input tokens (including ‚Äúimage tokens‚Äù), with each token corresponding to a 128-dimensional output vector. This output is directly comparable to the unpooled embeddings produced by ColBERT [Khattab and Zaharia, 2020] and ColPali [Faysse et al., 2025] and is intended for use in late interaction comparison strategies. For single-vector embeddings, mean pooling is applied to the final layer of the base model to produce the output. The model incorporates an additional layer to project the output of the base model into multi-vector outputs. \n\n4.3 Task Specialization with LoRA \n\nFollowing the methods used for \n\njina-embeddings-v3 [Sturua et al., 2024], we have implemented three task-specific LoRA adapters for different information retrieval use cases: ‚Ä¢ Asymmetric Query-Document Retrieval ‚Ä¢ Semantic Similarity and Symmetric Retrieval ‚Ä¢ Code (i.e., computer programming language) Retrieval Asymmetric retrieval means encoding queries and documents differently in order to improve retrieval for queries that are not structured like documents, i.e., short queries, questions, etc. This is in contrast to symmetric retrieval, which assumes a symmetry between query and document, and is used to find comparable content. Each LoRA adapter set has only 60M parameters, so maintaining all three adds less than 2% to the memory footprint of jina-embeddings-v4 . Users can select among them at inference time, and all three support image and text encoding. See Section 7 for performance information about these adapters. \n\n5 Training Method \n\nBefore training, model weights are initialized to match Qwen/Qwen2.5-VL-3B-Instruct . The multi-vector projection layer and LoRA adapters are randomly initialized. The weights of the backbone model are not modified during the training process. The LoRA adapters modify the effect of the backbone model layers and the projection layer. Only the adapters are trained. Training proceeds in two phases: 1. A single LoRA adapter is trained using contrasting text pairs and text-image pairs. We use the contrastive InfoNCE [van den Oord et al., 2018] loss function to co-train for both single-vector and multi-vector similarity, as detailed in the section below. No task-specific training is performed at this stage. 2. The resulting LoRA adapter is duplicated to create the three task-specific adapters, which are then trained individually with task-specific text triplets and text-image triplets. In both phases of training, we apply Ma-tryoshka loss [Kusupati et al., 2022] to the base loss so that single-vector embeddings from \n\njina-embeddings-v4 are truncatable. \n\n5.1 Pair Training \n\nInitially, training is performed with a contrastive objective. Pairs of inputs are classed as related or un-related, and the model learns to embed related items closely together and unrelated items further apart. In each training step, we sample two different batches of training data: ‚Ä¢ A batch Btext of text pairs. ‚Ä¢ A batch Bmulti of multimodal pairs containing a text and a related image. We generate normalized single-vector and multi-vector embeddings for all texts and images in the se-lected pairs. We then construct a matrix of similarity values Sdense (B) by calculating the cosine similarity of all combinations of single-vector embeddings qi\n\nand pj in B. We construct an analogous matrix Slate \n\nfor each B for the multi-vector embeddings using a slightly modified version of Equation (1) to cal-culate their similarity. Our choice of loss function requires a normalized score, so we divide the late in-teraction score by the number of tokens in the query: \n\ns‚Ä≤\n\n> late\n\n(qi,p j ) = slate (qi,p j )\n\nt (2) where t is the number of tokens in qi and qi,p j ‚àà B \n\nThis modification is only for training. For re-trieval applications, normalization is not necessary since the query is invariant. Then, we apply the contrastive InfoNCE loss function LNCE [van den Oord et al., 2018] on each of the four resulting matrices of similarity scores \n\nsi,j ‚àà S\n\nsoftmax( S,œÑ,i,j ) := ln esi,j /œÑ nP\n\n> k=0\n\nesi,k /œÑ \n\n(3) \n\nLNCE (S(B),œÑ ) := ‚àí\n\n> n\n\nX\n\n> i,j =0\n\nsoftmax( S(B),œÑ,i,i )\n\n(4) where œÑ is the temperature parameter, n is the batch size, which increases the weight of small dif-ferences in similarity scores in calculating the loss. Following Hinton et al. [2015], we compensate for differences in error distributions between the single-vector and multi-vector by adding the weighted Kullback‚ÄìLeibler divergence (DKL )of the two sets of softmax-normalized similarity scores. This enables us to train for the single-vector and multi-vector outputs simultaneously, even though the multi-vector/late interaction scores have much less error. \n\nLD(B,œÑ ) := DKL (S‚Ä≤\n\n> dense\n\n(B)‚à•S‚Ä≤\n\n> late\n\n(B)) \n\nwhere S‚Ä≤ \n\n> i,j\n\n= softmax( S,œÑ,i,j ) (5) The resulting joint loss function, which we use in training, is defined as: \n\nLjoint (Btxt ,Bmulti ,œÑ ) := \n\nw1LNCE (Sdense (Btxt ),œÑ )+w2LNCE (Slate (Btxt ),œÑ )+ w3LD(Btxt )+w4LNCE (Sdense (Bmulti ),œÑ )+w5LNCE (Slate (Bmulti ),œÑ )+ w6LD(Bmulti )\n\n(6) Task Name Description         \n\n> retrieval Asymmetric embedding of queries and documents for retrieval\n> text-matching Semantic text similarity and sym-metric retrieval\n> code Retrieving code snippets\n\nTable 2: Supported tasks of jina-embeddings-v4 , each corresponding to a LoRA adapter and trained indepen-dently \n\nThe weights w1, ... , w 6 and temperature œÑ are training hyperparameters. \n\n5.1.1 Pair Training Data \n\nThe training data consists of text-text and text-image pairs from more than 300 sources. Text-text pairs are selected and filtered as described in Sturua et al. [2024]. Text-image pairs have been curated from a variety of sources following a more eclectic strategy than previous work on training text-image embedding models. In contrast to relying on image-caption pairs or pairs of queries and images derived from documents, we have also created images from other document types. Our training data includes website screenshots, rendered Markdown files, charts, tables, and other kinds of materials \"found in the wild.\" The queries consist primarily of questions, keywords and key phrases, long descriptions, and statements of fact. \n\n5.2 Task-Specific Training \n\nWe instantiate three copies of the pair-trained LoRA adapter and give each specific training for its intended task. Training data and loss functions differ for the three tasks. \n\n5.2.1 Asymmetric Retrieval Adapter \n\nAsymmetric retrieval assigns substantially and qualitatively different embeddings to documents and queries, even if they happen to have the very same text. Having distinct encoding mechanisms for the two often significantly benefits embeddings-based retrieval performance. Sturua et al. [2024] shows that this can be achieved either by training two separate adapters or by employing two distinct prefixes as proposed in Wang et al. [2022], so that embedding models can readily distinguish them when they generate embeddings. We have used the prefix method for \n\njina-embeddings-v4 . Previous work shows little benefit from combining both methods. Our training data contains hard negatives, i.e., triplets of a query, a document that matches the query, and a document that is closely semantically related but not a correct match [Wang et al., 2022, Li et al., 2023, G√ºnther et al., 2023]. For every pair \n\n(qi,p i) ‚àà B in a batch, pi is intended to be a good match for qi, and we presume that for all (qj ,p j ) ‚àà B \n\nwhere jÃ∏ = i, pj is a bad match for qi.We incorporate those additional negatives into the training process via an extended version of the LNCE loss described in G√ºnther et al. [2023], denoted as LNCE+ , in our joint loss function Ljoint :\n\nLNCE+ (S(B),œÑ ) := \n\nX\n\n> r‚ààB\n\n\"\n\n‚àíln es(q,p )/œÑ kP\n\n> i=1\n\nh\n\nes(q,p i)/œÑ + mP\n\n> j=1\n\nes(q,n j,i )/œÑ \n\ni#\n\n(7) with r = ( q,p,n 1,...,n m), where (q,p ) is a pair in batch B and n1,...,n m and the other p ‚àà B .Our dataset of text hard negatives is similar to the data used to train jina-embeddings-v3 [Sturua et al., 2024]. We rely on existing datasets to create multimodal hard negatives for training, including Wiki-SS [Ma et al., 2024] and VDR multilingual 2, but also mined hard negatives from curated multimodal datasets. \n\n5.2.2 Text Matching Adapter \n\nSymmetric semantic similarity tasks require different training from asymmetric retrieval. We find that training data with ground truth similarity values works best for this kind of task. As discussed in Sturua et al. [2024], we use the CoSENT 3 loss function Lco from Li and Li [2024]: \n\nLco (S(B),œÑ ) := ln \n\nh\n\n1+ X\n\n> (q1,p 1),\n> (q2,p 2)\n> ‚ààS(B)\n\nes(q2,p 2) ‚àíes(q1,p 1)\n\nœÑ\n\ni\n\n(8) where Œ∂(q, p ) is the ground truth semantic similarity of q with p, Œ∂(q1,p 1) > Œ∂ (q2,p 2), and œÑ\n\nis the temperature parameter. \n\n> 2https://huggingface .co/datasets/llamaindex/ vdr-multilingual-train\n> 3https://github .com/bojone/CoSENT\n\nThe loss function operates on two pairs of text values, (q1, p 1) and (q2, p 2), with known ground truth similarity. To train the model with this objective, we use data from semantic textual similarity (STS) training datasets such as STS12 [Agirre et al., 2012] and SICK [Marelli et al., 2014]. The amount of data in this format is limited, so we enhance our ground truth training data with pairs that do not have known similarity scores. For these pairs, we proceed the same way as we did for pair training in Section 5.1 and use the standard InfoNCE loss from Equation (4) . The joint loss function is calculated as in Equation (7) except the CoSENT loss is used where pairs with known ground truth values exist. \n\n5.2.3 Code Adapter \n\nCode embeddings in jina-embeddings-v4 are designed for natural language-to-code retrieval, code-to-code similarity search, and technical question answering. Code is a very specialized kind of text and requires distinct data sources. Because code embeddings do not involve image processing, the vision portion of jina-embeddings-v4 is not affected by training the code retrieval LoRA adapter. The backbone LLM Qwen2.5-VL-3B-Instruct \n\nwas pre-trained on data including the StackEx-changeQA 4 and the CodeSearchNet [Husain et al., 2020] datasets, giving it some capacity to support code embeddings before further adaptation. Our LoRA training used the same triplet-based method described in Section 5.2.1. Training triplets are derived from a variety of sources, including CodeSearchNet, CodeFeedback [Zheng et al., 2024], APPS [Hendrycks et al., 2021], and the CornStack dataset [Suresh et al., 2025]. We maintained a consistent training configura-tion by using the same input prefix tokens (e.g., query, passage) and temperature hyperparameter (set to 0.02 ) during the triplet-based training. \n\n6 Jina-VDR: Visually Rich Document Retrieval Benchmark \n\nTo evaluate the performance of \n\njina-embeddings-v4 across a broad range of visually rich document retrieval tasks, we have produced a new benchmark collection and released \n\n> 4https://github .com/laituan245/ StackExchangeQA\n\nit to the public. 5\n\nThis new collection tests an embedding model‚Äôs ability to integrate textual and visual understanding of documents that consist of in the form of rendered images of visual elements like charts, tables, and running text. It extends the ViDoRe benchmark [Faysse et al., 2025] by adding a diverse collection of datasets spanning a broad range of domains (e.g. legal texts, historic documents, marketing materials), covering a variety of material types (e.g. charts, tables, manuals, printed text, maps) and query types (e.g. questions, facts, descriptions), as well as multiple languages. The benchmark suite encompasses ViDoRe and adds 30 additional tests. These tests include re-purposed existing datasets, new manually-annotated datasets, and generated synthetic data For a comprehensive overview of the individual benchmarks, see Appendix A.1. \n\n6.1 Re-purposed Datasets \n\nWe have adapted a number of existing VQA and OCR datasets, modifying and restructuring them into appropriate query-document pairs. For example, for DonutVQA 6,TableVQA [Tom Agonnoude, 2024], MP-MQA [Zhang et al., 2023], CharXiv [Wang et al., 2024], and PlotQA [Methani et al., 2020], we used structured templates and generative language mod-els to formulate text queries to match their contents. JDocQAJP 7 and HungarianDocQA 8 already con-tain documents and queries in forms that require minimal processing to adapt as benchmarks. We also created datasets from available data that extend beyond conventional question formats. The OurWorldInData and WikimediaMaps datasets use encyclopedia article fragments and image descriptions as queries to match with charts and maps. The GitHubREADMERetrieval dataset contains rendered Markdown pages drawn from GitHub README files, paired with generated natural language descriptions in 17 languages. The WikimediaCommonsDocuments benchmark pairs multilingual document pages with paragraph-level    \n\n> 5Benchmark available at https://huggingface .co/ collections/jinaai/jinavdr-visual-document-retrieval-684831c022c53b21c313b449 .\n> 6https://huggingface .co/datasets/warshakhan/ donut_vqa_ISynHMP\n> 7https://huggingface .co/datasets/jlli/JDocQA-nonbinary\n> 8https://huggingface .co/datasets/jlli/ HungarianDocQA-OCR\n\nreferences extracted from Wikipedia. \n\n6.2 Manually Annotated Datasets \n\nWe have curated a number of human-annotated resources to better reflect real-world use cases. These include academic slides from Stanford lec-tures [Mitchener, 2021], educational figures in the TQA dataset [Kembhavi et al., 2017], and marketing and institutional documents such as the Jina AI 2024 Yearbook [Jina AI, 2024], Japanese Ramen [Niigata-shi Kank ¬Øo Kokusaik ¬Øory ¬Øubu Kank ¬Øo Suishinka, 2024], and the Shanghai Master Plan [Shanghai Municipal People‚Äôs Government Urban Planning and Land Resource Administration Bureau, 2018]. Documents in these datasets were paired with care-fully written human queries without template-based phrasing, capturing genuine information-seeking intent. Some of these datasets target specific languages and regions to provide broader coverage. We also incorporated pre-existing human-annotated datasets like ChartQA 9 and its Arabic counterpart, ArabicChartQA [Ghaboura et al., 2024], which focus on charts and infographics. \n\n6.3 Synthetic Data Generation \n\nWe have been attentive, in constructing Jina-VDR, to the lack of diversity that often plagues informa-tion retrieval benchmarks. We cannot commission human-annotated datasets for everything and have had recourse to generative AI to fill in the gaps. We obtained a number of datasets from primarily European sources containing scans of historical, legal, and journalistic documents in German, French, Spanish, Italian, and Dutch. We used Qwen2 to generate queries for these documents. We handled the HindiGovernmentVQA and RussianBeverages datasets in the same way, adding not only often underrepresented languages, but also public service documents and commercial catalogs to this benchmark set. TweetStockRetrieval 10 is a collection of chart-based financial data, which we have paired with mul-tilingual template-based generated queries. AirBn-BRetrieval 11 is a collection of rendered tables that we have paired with queries in 10 languages gener-ated from a template. \n\n> 9https://huggingface .co/datasets/ HuggingFaceM4/ChartQA\n> 10 https://www .kaggle .com/datasets/ thedevastator/tweet-sentiment-s-impact-on-stock-returns\n> 11 https://www .kaggle .com/datasets/dgomonov/new-york-city-airbnb-open-data\n\nIn several cases, such as TableVQA [Delestre, 2024], we introduced bilingual examples (e.g., French/English) to better assess cross-lingual retrieval performance, with questions and answers synthesized using advanced multilingual LLMs such as Gemini 1.5 Pro and Claude 3.5 Sonnet. \n\n6.4 Jina-VDR in a Nutshell \n\nJina-VDR extends the ViDoRe benchmark with: ‚Ä¢ 30 new tasks, using both real-world and synthetic data ‚Ä¢ All datasets adapted for retrieval and designed to be compatible with ViDoRe ‚Ä¢ LLM-based filtering to ensure all queries are relevant and reflective of real-world querying ‚Ä¢ Non-question queries, such as GitHub descrip-tions matched to rendered markdown images, and map images from Wikimedia Commons with accompanying textual descriptions ‚Ä¢ Multilingual coverage, with some datasets spanning up to 20 languages \n\n7 Evaluation \n\nWe have evaluated jina-embeddings-v4 on a diverse set of benchmarks to reflect its multiple functions. Table 3 provides an overview of benchmark averages for jina-embeddings-v4 \n\nand other embedding models. \n\n7.1 Multilingual Text Retrieval \n\nMTEB and MMTEB [Enevoldsen et al., 2025] are the most widely used text retrieval benchmarks. For most tasks, we have used the asymmetric retrieval adapter, but for some symmetric retrieval tasks like ArguAna 12 , we have used the text matching adapter instead. For evaluation, we prepend the query with the prefix ‚ÄúGiven a claim, find documents that refute the claim‚Äù to reflect the task‚Äôs focus on retrieving passages that contradict, rather than support, the input claim, similar to Wang et al. [2023]. The results are tabulated in Appendix A.4. For the MTEB benchmarks, which are all in English, see Table A11, and for the multilingual MMTEB, see Table A12. The performance of this new model is generally better than our previous model jina-embeddings-v3 and broadly comparable with the state-of-the-art. \n\n> 12 https://huggingface .co/datasets/mteb/arguana\n\nTable 3: Average Retrieval Scores of Embedding Models on Various Benchmarks.                                                                                       \n\n> Model J-VDR ViDoRe CLIPB MMTEB MTEB-en COIR LEMB STS-m STS-en\n> jina-embeddings-v4 (dense) 72.19 84.11 84.11 66.49 55.97 71.59 67.11 72.70 85.89 jina-embeddings-v4 (late) 79.29 90.17 text-embedding-3-large ‚Äì‚Äì‚Äì59.27 57.98 62.36 52.42 70.17 81.44 bge-m3 ‚Äì‚Äì‚Äì55.36 58.73 multilingual-e5-large-instruct ‚Äì‚Äì‚Äì57.12 53.47 41.76 jina-embeddings-v3 47.06 26.02 ‚Äì58.58 54.33 55.07 55.66 75.77 85.82 voyage-3 ‚Äì‚Äì‚Äì66.13 53.46 67.23 74.06 68.33 78.59 gemini-embedding-001 ‚Äì‚Äì‚Äì67.71 64.35 73.11 78.35 85.29 jina-embedings-v2-code ‚Äì‚Äì‚Äì52.24 voyage-code ‚Äì‚Äì‚Äì77.33 nllb-clip-large-siglip 83.19 jina-clip-v2 39.84 53.61 81.12 colpali-v1.2 (late) 64.50 83.90 dse-qwen2-2b-mrl-v1 (dense) 65.96 85.80 voyage-multimodal-v3 (dense) 84.24\n> Task Acronyms: J-VDR = Jina VDR, VidoRE = ViDoRe, CLIPB = CLIP Benchmark, MMTEB = MTEB(Multilingual, v2) Retrieval Tasks, MTEB-EN = MTEB(eng, v2) Retrieval Tasks, COIR = CoIR Code Retrieval, LEMB = LongEmbed, STS-m = MTEB(Multilingual, v2) Semantic Textual Similarity Tasks, STS-en = MTEB(eng, v2) Semantic Textual Similarity Tasks\n> Average Calculation: For J-VDR and ViDoRE, we calculate the average for the multilingual tasks first and consider this as a single score before calculating the average across all tasks. Scores are nDCG@5 for J-VDR, ViDoRe, and CLIPB, and nDCG@10 for MMTEB, MTEB-en, COIR, and LEMB, and Spearman coefficient for STS-m and STS-en.\n> Evaluation of Text Retrieval Models on J-VDR: For evaluating text retrieval models on J-VDR, we used EasyOCR ( https: //github .com/JaidedAI/EasyOCR ) and the provided extracted texts from the original ViDoRe datasets.\n\nWe have also evaluated the performance of our model on retrieval tasks that involve long text documents using the LongEmbed benchmark [Zhu et al., 2024]. The results are tabulated in Table A13 of Appendix A.4. Long document performance for jina-embeddings-v4 significantly outpaces competing models except the voyage-3 series and improves dramatically on jina-embeddings-v3 ‚Äôs performance. \n\n7.2 Textual Semantic Similarity \n\nWe evaluated jina-embeddings-v4 with text-based semantic similarity (STS) benchmarks. The results for MTEB STS and MMTEB STS benchmarks are tabulated in Tables A14 and A15 of Appendix A.4 respectively. Our results are competitive with the state-of-the-art and are best-in-class for English similarity tasks. \n\n7.3 Multimodal Retrieval \n\nTo evaluate the model‚Äôs performance on typical text-to-image search tasks, we used the common English and non-English tasks of the CLIP Bench-mark 13 . The results are tabulated in Tables A8 to A10 of Appendix A.3. jina-embeddings-v4 \n\n> 13 https://github .com/LAION-AI/CLIP_benchmark\n\nhas a higher average score than jina-clip-v2 \n\nand nllb-siglip-large , but the latter performs somewhat higher on the Crossmodal3600 bench-mark [Thapliyal et al., 2022] (see Table A10) because it includes content from low-resource lan-guages not supported in jina-embeddings-v4 ‚Äôs \n\nQwen2.5-VL-3B-Instruct backbone. We further tested jina-embeddings-v4 on the ViDoRe and Jina-VDR benchmarks, to evaluate its performance on visually rich documents. The results are compiled in Appendix A.2. ViDoRe scores are tabulated in Table A3. Table A2 provides an overview of jina-embeddings-v4 compared to other models, with Tables A4 to A7 providing details results for some individual Jina-VDR benchmarks. This suggests that other models are primarily trained to perform well on document retrieval tasks that are similar to the ViDoRe tasks but underperform on other tasks, e.g., that do not involve queries that resemble questions. \n\njina-embeddings-v4 excels at this benchmark, providing the current state-of-the-art, in both single-and multi-vector mode. Multi-vector/late interac-tion matching is generally recognized as more pre-cise than single-vector matching in other applica-tions, and this remains true for Jina-VDR. \n\n7.4 Code Retrieval \n\nTo assess performance on code retrieval, we evaluate the model on the MTEB-CoIR bench-mark [Li et al., 2024], which consists of 10 tasks spanning text-to-code, code-to-text, code-to-code, and hybrid code retrieval types. The results are reported in Table A16 of Appendix A.4. \n\njina-embeddings-v4 is competitive with the state-of-the-art in general-purpose embedding models, but the specialized voyage-code model has somewhat better benchmark performance. \n\n8 Analysis of the Embedding Space \n\nThe large difference in architecture between \n\njina-embeddings-v4 and CLIP-style models like OpenAI CLIP [Radford et al., 2021] and \n\njina-clip-v2 implies a large difference in the structure of the embedding spaces those models generate. We look here at a few of these issues. \n\n8.1 Modality Gap \n\nPrevious work has shed light on the so-called modality gap in multimodal models trained with contrastive learning [Liang et al., 2022b, Schrodi et al., 2024, Eslami and de Melo, 2025]. Good semantic matches across modalities tend to lie considerably further apart in the embedding space than comparable or even worse matches of the same modality, i.e., texts in CLIP-style models are more similar to semantically unrelated texts than to semantically similar images. We can see the modality gap directly by exam-ining the distribution of pairwise cosine similarities of matching image-text pairs versus matching text-text pairs. In Figure 2, we see the distribution of similarity values for the two pair types in OpenAI CLIP, jina-clip-v2 , and jina-embeddings-v4 .The gap is dramatically reduced with \n\njina-embeddings-v4 because of its cross-model encoder, illustrated in Figure 1. Eslami and de Melo [2025] has shown that sharing an encoder between modalities introduces an inductive bias to-wards using a shared region of the embedding space, while Liang et al. [2022b] shows the opposite is true for CLIP-style architectures with separate encoders. \n\n8.2 Cross-Modal Alignment \n\nEslami and de Melo [2025] has defined the cross-modal alignment score of a multimodal embed-ding model as the average of cosine similarities of 0.2 0.0 0.2 0.4 0.6 0.8 1.0              \n\n> 0\n> 2\n> 4\n> 6\n> 8\n> 10\n> Density\n> Image-Text Pairs Text-Text Pairs 0.2 0.0 0.2 0.4 0.6 0.8 1.0\n> 0\n> 2\n> 4\n> 6\n> 8\n> 10\n> Density 0.2 0.0 0.2 0.4 0.6 0.8 1.0\n> Cosine Similarity\n> 0\n> 1\n> 2\n> 3\n> 4\n> 5\n> Density\n\nFigure 2: Distribution of the cosine similarities of the paired image-text embeddings versus paired text-text embeddings from the Flickr8K 14 dataset. Top :OpenAI CLIP, Middle : jina-clip-v2 , Bottom :\n\njina-embeddings-v4 \n\nmatching pairs of image and text embeddings. Ta-ble 4 calculates this score for jina-embeddings-v4 \n\nand OpenAI CLIP with data sampled from the Flickr30K 15 , MSCOCO [Lin et al., 2014], and CIFAR-100 16 datasets. These results confirm that jina-embeddings-v4 \n\ngenerates a far better aligned cross-modal embed-ding space than CLIP-style models. It is worth noting that jina-embeddings-v4 \n\nshows much poorer alignment for CIFAR-100 data than MSCOCO and Flickr30K. This is because             \n\n> 15 https://www .kaggle .com/datasets/adityajn105/ flickr30k\n> 16 https://www .kaggle .com/datasets/fedesoriano/ cifar100 Model Flickr30K MSCOCO CIFAR-100\n> OpenAI-CLIP 0.15 0.14 0.2\n> jina-clip-v2 0.38 0.37 0.32\n> jina-embeddings-v4 0.71 0.72 0.56\n\nTable 4: Comparison of cross-modal alignment scores on 1K of random samples from each dataset. \n\nCIFAR-100 is a classification dataset and its labels are far less informative than the more descriptive texts in MSCOCO and Flickr30K. \n\n8.3 Cone Effect \n\nLiang et al. [2022b] demonstrate that multimodal models trained with contrastive loss suffer from an inductive bias known as the cone effect. Each modality tends to cluster together in randomized embedding spaces before training, and contrastive loss tends to make the cross-modal matching pairs form a kind of high-dimensional cone, linking one part of the embedding space to another rather than distributing embeddings evenly. The impact of the cone effect can be seen in Figure 3. The difference in cosine similarity between correct and incorrect text-image matches is quite small for OpenAI CLIP (top), signifi-cantly greater in jina-clip-v2 (middle), but \n\njina-embeddings-v4 (bottom) shows a much greater spread of cosine similarity ranges with very distinctly separate peaks for positive and negative pairs. This shows that jina-embeddings-v4 uses much more of the embedding space and image and text embeddings have a much greater overlap in distribution. \n\n9 Conclusion \n\nWe present jina-embeddings-v4 , a state-of-the-art multimodal and multilingual embedding model designed for a wide range of tasks, including semantic text retrieval, text-to-image retrieval, text-to-visually-rich document retrieval, and code search. The model achieves strong performance us-ing single-vector representations and demonstrates even greater effectiveness with multi-vector repre-sentations, particularly in visually rich document retrieval. jina-embeddings-v4 aligns representa-tions across modalities into a single, shared seman-tic space, sharply reducing structural gaps between modalities compared to CLIP-style dual-tower mod-els, enabling more effective cross-modal retrieval. In future work, we plan to further enhance this model‚Äôs multilingual capabilities and explore 0.2 0.0 0.2 0.4 0.6 0.8 1.0              \n\n> 0\n> 2\n> 4\n> 6\n> 8\n> 10\n> Density\n> Positive Samples Negative Samples 0.2 0.0 0.2 0.4 0.6 0.8 1.0\n> 0\n> 2\n> 4\n> 6\n> 8\n> 10\n> Density 0.2 0.0 0.2 0.4 0.6 0.8 1.0\n> Cosine Similarity\n> 0\n> 1\n> 2\n> 3\n> 4\n> 5\n> Density\n\nFigure 3: Distribution of the cosine similarities of positive (correct matches) versus negative (incorrect matches) image-text samples. (top) OpenAI CLIP, (mid-dle) jina-clip-v2 , (bottom) jina-embeddings-v4 .\n\ntechniques to create smaller, more efficient variants. References \n\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large lan-guage models. In The Tenth International Conference on Learning Representations, ICLR , 2022. Yihao Ding, Soyeon Caren Han, Jean Lee, and Eduard Hovy. Deep learning based visually rich document content understanding: A survey. arXiv preprint arXiv:2408.01287 , 2024. Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Ser-ena Yeung, and James Y Zou. Mind the Gap: Under-standing the Modality Gap in Multi-modal Contrastive Representation Learning. Advances in Neural Infor-mation Processing Systems , 35:17612‚Äì17625, 2022a. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, C√©line Hudelot, and Pierre Colombo. ColPali: Efficient Document Retrieval with Vision Language Models. In The Thirteenth International Conference on Learning Representations , 2025. Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stir-ling, Xin Zhang, M√°rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. MIEB: Massive Image Embedding Benchmark. arXiv preprint arXiv:2504.10471 , 2025. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael G√ºnther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, et al. jina-embeddings-v3: Multilingual Embeddings With Task LoRA. arXiv preprint arXiv:2409.10173 , 2024. Alec Radford, Jong Wook Kim, et al. Learning Transferable Visual Models from Natural Language Supervision. In International Conference on Machine Learning , pages 8748‚Äì8763, 2021. Andreas Koukounas, Georgios Mastrapas, Michael G√ºn-ther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Mart√≠nez, Saahil Ognawala, et al. Jina CLIP: Your CLIP Model Is Also Your Text Retriever. arXiv preprint arXiv:2405.20204 , 2024. Ye Liu, Rui Meng, Shafiq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. CodeX-Embed: A Generalist Embedding Model Family for Multilingual and Multi-task Code Retrieval. arXiv preprint arXiv:2411.12644 , 2024. Voyage AI. Domain-Specific Embeddings and Retrieval: Legal Edition (voyage-law-2), 2024. https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/. Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying Multimodal Retrieval via Document Screenshot Embedding. In \n\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pages 6492‚Äì6505, 2024. Omar Khattab and Matei Zaharia. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval , pages 39‚Äì48, 2020. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sen-tence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3982‚Äì3992, 2019. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv preprint arXiv:2212.03533 , 2022. Michael G√ºnther, Jackmin Ong, Isabelle Mohr, Alaed-dine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al. Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. arXiv preprint arXiv:2310.19923 , 2023. Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka Represen-tation Learning. Advances in Neural Information Processing Systems , 35:30233‚Äì30249, 2022. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923 , 2025. Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. E5-V: Universal Embeddings with Multimodal Large Language Models. arXiv preprint arXiv:2407.12580 , 2024. Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv preprint arXiv:2412.16855 , 2024. A√§ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive Predictive Coding. CoRR , abs/1807.03748, 2018. URL \n\nhttp://arxiv .org/abs/1807 .03748 .Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:1503.02531 , 2015. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards General Text Embeddings with Multi-stage Contrastive Learning. arXiv preprint arXiv:2308.03281 , 2023. Xianming Li and Jing Li. Aoe: Angle-optimized embeddings for semantic textual similarity. In \n\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1825‚Äì1839, 2024. Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: A pilot on semantic textual similarity. In SEM 2012: The First Joint Conference on Lexical and Computational Semantics‚ÄìVolume 1: Proceedings of the main confer-ence and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012) , pages 385‚Äì393, 2012. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zampar-elli. A SICK cure for the evaluation of compositional distributional semantic models. In Nicoletta Cal-zolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, \n\nProceedings of the Ninth International Conference on Language Resources and Evaluation (LREC‚Äô14) ,pages 216‚Äì223, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL http://www .lrec-conf .org/proceedings/ lrec2014/pdf/363_Paper .pdf .Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. arXiv preprint arXiv:1909.09436 , 2020. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024 , pages 12834‚Äì12859. Association for Computational Linguistics, August 2024. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks , 2021. Tarun Suresh, Revanth Gangi Reddy, Yifei Xu, Zach Nussbaum, Andriy Mulyar, Brandon Duderstadt, and Heng Ji. Cornstack: High-quality contrastive data for better code retrieval and reranking. arXiv preprint arXiv:2412.01007 , 2025. Cyrile Delestre Tom Agonnoude, 2024. URL https:// huggingface .co/datasets/cmarkea/table-vqa .Liang Zhang, Anwen Hu, Jing Zhang, Shuo Hu, and Qin Jin. Mpmqa: Multimodal question answering on product manuals. 2023. URL \n\nhttps://arxiv .org/abs/2304 .09660 .Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521 , 2024. Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In The IEEE Winter Conference on Applications of Computer Vision (WACV) , March 2020. Kris James Mitchener. National banking statis-tics, 1867‚Äì1896. https://purl .stanford .edu/ mv327tb8364 , 2021. Dataset published via Stanford Digital Repository. Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Ha-jishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal ma-chine comprehension. pages 5376‚Äì5384, 07 2017. doi:10.1109/CVPR.2017.571. Jina AI. Re¬∑Search: Order 2024 Yearbook of Search Foundation Advances . Jina AI, Sunnyvale, CA & Berlin, Germany, December 16 2024. Hardcover with spot UV coating; includes complimentary digital copy. Available at: https://jina .ai/news/re-search-order-2024-yearbook-of-search-foundation-advances/ .Niigata-shi Kank ¬Øo Kokusaik ¬Øory ¬Øubu Kank ¬Øo Su-ishinka. Niigata City Ramen Guidebook . City of Niigata, Niigata, Japan, July 2024. URL \n\nhttps://www .city .niigata .lg .jp/kanko/kanko/ oshirase/ramen .files/guidebook .pdf . PDF, approximately 28 MB. Shanghai Municipal People‚Äôs Government Urban Planning and Land Resource Administration Bureau. \n\nShanghai Master Plan 2017‚Äì2035: Striving for the Excellent Global City . Shanghai Municipal People‚Äôs Government, Shanghai, China, January 2018. URL \n\nhttps://www .shanghai .gov .cn/newshanghai/ xxgkfj/2035004 .pdf . Public Reading edition; government-issued planning document. Sara Ghaboura, Ahmed Heakl, Omkar Thawakar, Ali Alharthi, Ines Riahi, Abduljalil Saif, Jorma Laaksonen, Fahad S. Khan, Salman Khan, and Rao M. Anwer. Camel-bench: A compre-hensive arabic lmm benchmark. 2024. URL \n\nhttps://arxiv .org/abs/2410 .18976 .Cyrile Delestre, 2024. URL https:// huggingface .co/datasets/cmarkea/aftdb .Kenneth Enevoldsen, Isaac Chung, et al. MMTEB: Mas-sive Multilingual Text Embedding Benchmark. 2025. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368 , 2023. Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. LongEmbed: Extending Embedding Models for Long Context Retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing ,pages 802‚Äì816, 2024. Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: A massively multilingual multimodal evaluation dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 715‚Äì729, 2022. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. Coir: A comprehensive benchmark for code information retrieval models. \n\nCoRR , 2024. Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Ser-ena Yeung, and James Y Zou. Mind the gap: Under-standing the modality gap in multi-modal contrastive representation learning. Advances in Neural Infor-mation Processing Systems , 35:17612‚Äì17625, 2022b. Simon Schrodi, David T Hoffmann, Max Argus, Volker Fischer, and Thomas Brox. Two effects, one trigger: on the modality gap, object bias, and information im-balance in contrastive vision-language representation learning. arXiv preprint arXiv:2404.07983 , 2024. Sedigheh Eslami and Gerard de Melo. Mitigate the gap: Improving cross-modal alignment in clip. In The Thirteenth International Conference on Learning Representations , 2025. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Com-puter Vision ‚Äì ECCV 2014 , pages 740‚Äì755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1. A Appendix \n\nA.1 Datasets in the Jina-VDR Benchmark \n\nTable A1: Overview of the Dataset Collection \n\nDataset Name Domain Document Format Query Format Number of Queries / Documents Languages \n\njinaai/airbnb-synthetic-retrieval‚Ä† Housing Tables Instruction 4953 / 10000 ar, de, en, es, fr, hi, hu, ja ru, zh jinaai/arabic_chartqa_ar Mixed Charts Question 745 / 745 ar jinaai/arabic_infographicsvqa_ar Mixed Illustrations Question 120 / 40 ar jinaai/automobile_catalogue_jp Marketing Catalog Question 45 / 15 ja jinaai/arxivqa Science Mixed Question 30 / 499 en jinaai/beverages_catalogue_ru Marketing Digital Docs Question 100 / 34 ru jinaai/ChartQA Mixed Charts Question 7996 / 1000 en jinaai/CharXiv-en Science Charts Question 999 / 1000 en jinaai/docvqa Mixed Scans Question 39 / 499 en jinaai/donut_vqa Medical Scans / Handwriting Question 704 / 800 en jinaai/docqa_artificial_intelligence Software / IT Digital Docs Question 70 / 962 en jinaai/docqa_energy Energy Digital Docs Question 69 / 972 en jinaai/docqa_gov_report Government Digital Docs Question 77 / 970 en jinaai/docqa_healthcare_industry Medial Digital Docs Question 90 / 963 en jinaai/europeana-de-news Historic Scans / News Articles Question 379 / 137 de jinaai/europeana-es-news Historic Scans / News Articles Question 474 / 179 es jinaai/europeana-fr-news Historic Scans / News Articles Question 237 / 145 fr jinaai/europeana-it-scans Historic Scans Question 618 / 265 it jinaai/europeana-nl-legal Legal Scans Question 199 / 300 nl jinaai/github-readme-retrieval-multilingual‚Ä† Software / IT Markdown Docs Description 16755 / 4398 ar, bn, de, en, es, fr, hi, id, it, ja, ko, nl pt, ru, th, vi, zh jinaai/hindi-gov-vqa Governmental Digital Docs Question 454 / 340 hi jinaai/hungarian_doc_qa_hu Mixed Digital Docs Question 54 / 54 hu jinaai/infovqa Mixed Illustrations Question 363 / 500 en jinaai/jdocqa News Digital Docs Question 744 / 758 ja jinaai/jina_2024_yearly_book Software / IT Digital Docs Question 75 / 33 en jinaai/medical-prescriptions Medical Digital Docs Question 100 / 100 en jinaai/mpmqa-small Manuals Digital Docs Question 155 / 782 en jinaai/MMTab Mixed Tables Fact 987 / 906 en jinaai/openai-news Software / IT Digital Docs Question 31 / 30 en jinaai/owid_charts_en Mixed Charts Question 132 / 972 en jinaai/plotqa Mixed Charts Question 610 / 986 en jinaai/ramen_benchmark_jp Marketing Catalog Question 29 / 10 ja jinaai/shanghai_master_plan Governmental Digital Docs Question / Key Phrase 57 / 23 zh, en jinaai/wikimedia-commons-documents-ml‚Ä† Mixed Mixed Description 14061 / 14661 ar, bn, de, en, es, fr, hi, hu, id, it, ja, ko, my, nl, pt, ru, th, ur, vi, zh jinaai/shiftproject Environmental Documents Digital Docs Question 89 / 998 fr jinaai/stanford_slide Education Slides Question 14 / 1000 en jinaai/student-enrollment Demographics Charts Question 1000 / 489 en jinaai/tabfquad Mixed Tables Question 126 / 70 fr, en jinaai/table-vqa Science Tables Question 992 / 1000 en jinaai/tatqa Finance Digital Docs Question 121 / 176 en jinaai/tqa Education Illustrations Question 981 / 394 en \n\nContinued on next page Table A1 ‚Äì continued from previous page \n\nDataset Name Domain Document Format Query Format Number of Queries / Documents Languages \n\njinaai/tweet-stock-synthetic-retrieval‚Ä† Finance Charts Question 6278 / 10000 ar, de, en, es, hi, hu, ja, ru, zh jinaai/wikimedia-commons-maps Mixed Maps Description 443 / 455 en ‚Ä†For multilingual datasets, the total number of queries and documents is the sum across all language-specific splits. \n\nA.2 JinaVDR (Visual Document Retrieval) Benchmark Results \n\nTable A2: Overview of JinaVDR Results for Various Models \n\nTask bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 45.48 48.14 40.21 65.51 48.62 73.86 80.16 medical-prescriptions 38.18 37.25 15.66 83.91 38.22 81.17 97.95 stanford_slide 81.78 95.28 91.48 100 100 100 100 donut_vqa 19.39 2.60 1.63 32.53 25.31 78.48 73.55 table-vqa 35.64 34.24 21.06 54.66 57.39 58.90 60.91 ChartQA 26.01 29.46 36.45 50.29 55.01 66.45 67.40 tqa 50.04 24.21 27.45 68.27 65.29 65.79 68.28 openai-news 76.63 87.30 70.05 94.82 92.56 93.75 97.62 europeana-de-news 11.26 12.05 11.19 34.64 44.75 49.05 65.65 europeana-es-news 51.99 44.03 13.14 44.74 60.58 60.10 80.58 europeana-it-scans 39.11 38.69 16.23 54.32 53.92 57.88 73.14 europeana-nl-legal 34.97 29.07 9.79 30.89 29.50 37.14 54.15 hindi-gov-vqa 1.83 7.52 5.02 13.04 9.80 15.40 21.94 automobile_catalogue_jp 20.92 49.53 32.54 39.69 65.62 77.10 81.32 beverages_catalogue_ru 11.05 14.11 39.45 60.36 81.07 85.31 86.65 ramen_benchmark_jp 28.03 62.19 39.79 47.46 51.75 89.50 94.65 jdocqa_jp_ocr 1.64 7.79 19.91 39.45 66.73 75.57 82.34 hungarian_doc_qa 32.43 55.42 47.91 73.08 53.80 71.19 75.20 arabic_chartqa_ar 6.27 5.91 4.12 12.03 35.37 44.73 49.79 arabic_infographicsvqa_ar 13.26 13.49 49.98 51.24 71.93 84.86 94.07 owid_charts_en 65.50 61.23 57.61 83.53 83.48 87.40 89.68 arxivqa 56.67 54.27 84.64 95.44 93.33 95.44 95.44 docvqa 76.37 43.87 45.23 88.84 86.28 84.05 92.98 shiftproject 59.43 68.43 32.10 76.56 78.76 81.65 90.72 docqa_artificial_intelligence 90.78 82.93 64.81 95.19 97.52 96.43 98.04 docqa_energy 87.52 78.68 65.39 94.95 90.08 89.19 96.57 docqa_gov_report 86.29 82.03 69.18 94.75 94.50 91.84 95.97 docqa_healthcare_industry 84.83 88.92 67.77 95.85 95.24 94.62 97.51 tabfquad 42.62 78.03 46.94 87.62 91.80 95.57 94.79 mpmqa_small 85.54 66.85 59.53 88.95 81.99 80.96 91.80 jina_2024_yearly_book 87.67 85.98 76.63 93.05 92.90 94.78 97.68 wikimedia-commons-maps 5.37 5.12 20.43 30.53 33.14 39.55 54.15 plotqa 61.13 51.39 23.98 73.46 75.98 77.97 78.77 MMTab 74.82 74.18 44.63 85.80 85.90 86.16 90.00 CharXiv-en 46.85 41.58 56.34 83.96 84.10 82.80 87.78 student-enrollment 1.05 1.19 0.72 3.92 4.08 7.88 12.02 tatqa 75.21 70.38 44.91 85.29 80.68 79.46 92.93 shanghai_master_plan 12.69 93.32 75.28 90.50 93.21 94.04 97.18 europeana-fr-news 24.55 23.78 16.21 30.99 37.49 36.10 50.44 infovqa 68.64 75.39 63.03 90.21 92.41 92.11 96.49 \n\nModels: bm25+OCR: BM25 with EasyOCR, jev3+OCR: jina-embeddings-v3 with EasyOCR, j-clip-v2: jina-clip-v2 , colpali-v1.2: ColPALI-v1.2, dse-qwen2- 2b-mrl-v1: DSE-QWen2-2b-MRL-V1, je4-single: jina-embeddings-v4 single-vector, jev4-multi: jina-embeddings-v4 multi-vector Table A3: Retrieval performance on ViDoRe (nDCG@10%). \n\nModel Avg AQA DVQA InfoVQA Shift AI Energy Gov Health TabFQ TQA \n\nOCR + jina-embeddings-v3 26.02 26.31 12.62 32.79 14.18 22.84 27.47 31.16 45.78 44.54 2.53 jina-clip-v2 53.61 68.33 27.62 60.6 34.12 66.55 64.69 67.47 68.38 46.89 31.43 voyage-multimodal-3 84.20 84.90 55.60 85.40 78.70 94.50 89.50 96.00 95.10 92.80 69.90 colpali-v1.2 83.90 78.00 57.20 82.80 79.10 98.10 95.20 94.80 96.70 89.70 68.10 dse-qwen2-2b-mrl-v1 85.80 85.60 57.10 88.10 82.00 97.50 92.90 96.00 96.40 93.10 69.40 OCR + bm25 65.50 31.60 36.80 62.90 64.30 92.80 85.90 83.90 87.20 46.50 62.70 siglip-so400m-patch14-384 51.40 43.20 30.30 64.10 18.70 62.50 65.70 66.10 79.10 58.10 26.20 jina-embeddings-v4 (dense) 84.11 83.57 50.54 87.85 84.07 97.16 91.66 91.48 94.92 94.48 65.35 jina-embeddings-v4 (late) 90.17 88.95 59.98 93.57 92.35 99.26 96.76 96.95 98.39 95.13 80.34 \n\nTasks: Avg: Mean nDCG@10% over all tasks, AQA: ArxivQA, Shift: Shift Project, DVQA: DocVQA, InfoVQA: InfographicVQA, AI: Artificial Intelligence, Gov: Government Reports, Health: Healthcare Industry, TabFQ: TabFQuad, TQA: TAT-DQA \n\nTable A4: Wikimedia Commons Retreival Benchmark Results \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 21.95 37.34 48.32 46.82 58.43 65.79 74.50 Arabic (ar) 19.60 38.06 45.29 41.96 62.92 72.07 81.19 Bengali (bn) 22.93 44.44 48.48 38.56 52.39 67.31 76.60 German (de) 12.74 39.45 52.72 51.91 63.00 70.32 80.69 English (en) 36.44 45.21 56.51 68.76 70.31 73.91 81.72 Spanish (es) 12.75 46.02 54.70 59.57 66.31 71.80 80.33 French (fr) 15.59 35.83 35.69 44.42 40.84 53.67 59.28 Hindi (hi) 16.53 36.69 47.47 38.13 50.56 63.45 68.78 Hungarian (hu) 25.32 33.86 44.20 26.22 52.60 65.36 76.10 Indonesian (id) 28.78 39.35 50.77 56.41 61.59 66.34 73.59 Italian (it) 19.58 37.93 49.61 49.08 60.12 64.18 73.31 Japanese (jp) 21.32 30.42 43.75 53.50 63.61 66.72 76.63 Korean (ko) 34.86 35.16 47.12 54.77 68.47 71.83 81.53 Burmese (my) 22.84 29.57 54.25 16.31 36.97 46.91 50.23 Dutch (nl) 14.89 39.76 50.23 56.87 64.67 68.97 78.42 Portuguese (pt) 23.27 45.57 53.97 58.17 67.26 69.23 78.43 Russian (ru) 16.82 38.92 49.34 46.77 64.26 69.56 80.45 Thai (th) 30.00 29.79 45.76 47.90 55.78 63.36 70.03 Urdu (ur) 13.64 32.78 36.30 15.68 38.49 48.71 61.91 Vietnamese (vi) 32.40 39.87 54.06 56.82 64.33 72.93 80.18 Chinese (zh) 18.76 28.18 46.24 54.62 64.14 69.11 80.55 \n\nTable A5: GitHub Readme Retrieval Benchmark Results \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 50.11 65.17 38.93 76.83 72.26 85.52 85.69 Arabic (ar) 27.49 27.99 31.07 58.53 55.63 75.04 75.71 Bengali (bn) 1.29 28.29 27.06 53.72 47.55 65.60 66.03 German (de) 60.11 84.50 43.35 87.16 80.76 91.10 91.39 English (en) 87.40 91.60 48.58 93.91 90.75 96.78 97.35 Spanish (es) 78.57 83.30 43.19 86.84 78.61 89.46 89.99 French (fr) 77.55 83.66 42.12 85.42 79.20 90.29 90.31 Hindi (hi) 2.72 48.19 28.53 57.93 46.62 68.98 70.88 Indonesian (id) 78.05 82.41 38.59 83.06 74.39 88.31 88.69 Italian (it) 78.83 86.53 44.33 86.94 80.93 91.74 91.36 \n\nContinued on next page Table A5 ‚Äì continued from previous page \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nJapanese (jp) 14.46 63.12 41.99 75.99 75.31 89.60 90.91 Korean (ko) 40.01 35.32 38.32 69.45 68.77 86.92 86.85 Dutch (nl) 76.52 86.33 43.17 88.01 82.97 92.83 91.39 Portuguese (pt) 80.33 84.54 43.86 86.83 80.22 91.58 91.50 Russian (ru) 39.78 51.07 36.80 80.67 79.08 89.55 88.63 Thai (th) 1.47 36.75 37.80 68.68 65.51 77.78 76.46 Vietnamese (vi) 66.70 79.65 37.08 75.00 68.15 86.93 87.13 Chinese (zh) 40.52 54.57 35.94 67.98 73.90 81.43 82.08 \n\nTable A6: Tweet Stock Retrieval Benchmark Results \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 22.29 42.69 55.36 78.32 62.81 64.05 85.46 Arabic (ar) 0.38 1.67 49.26 77.02 52.51 52.86 78.11 German (de) 48.27 66.85 52.29 75.70 57.50 64.22 85.97 English (en) 51.38 63.61 48.28 78.70 63.31 63.87 85.17 Spanish (es) 54.28 63.80 53.16 77.81 62.38 63.61 84.24 French (fr) 51.69 64.53 54.92 75.95 62.48 62.20 85.35 Hindi (hi) 0.08 0.08 89.06 96.51 96.62 87.14 96.86 Hungarian (hu) 15.55 61.71 52.05 73.50 58.36 63.75 85.70 Japanese (jp) 0.40 47.64 54.76 70.38 58.03 61.25 85.61 Russian (ru) 0.47 2.83 47.09 78.10 57.51 63.18 82.95 Chinese (zh) 0.45 54.16 52.78 79.51 59.07 58.44 84.66 \n\nTable A7: AirBnB Retrieval Benchmark Results \n\nLanguage bm25 + OCR jev3 + OCR j-clip-v2 colpali-v1.2 dse-qwen2-2b-mrl-v1 jev4-single jev4-multi \n\nAverage 7.20 1.12 2.12 16.45 11.06 8.13 37.42 Arabic (ar) 1.10 0.39 0.47 2.59 3.26 2.28 6.63 German (de) 4.03 0.71 5.85 30.58 14.46 9.32 42.26 English (en) 48.39 1.72 4.73 47.51 13.07 13.06 63.48 Spanish (es) 6.25 0.18 1.93 24.15 8.46 9.12 40 French (fr) 3.86 1.92 2.03 18.02 12.21 8.55 29.69 Hindi (hi) 0.16 0.78 0.84 1.81 4.51 3.96 17.56 Hungarian (hu) 5.58 0.63 3.10 23.09 11.74 6.87 27.21 Japanese (jp) 0.36 1.70 0.46 4.12 14.71 7.65 46.21 Russian (ru) 1.67 1.39 0.79 11.18 13.98 8.43 40.43 Chinese (zh) 0.58 1.77 1.04 1.49 14.17 12.09 60.70 A.3 CLIP \n\nTable A8: Cross-modal (Text-to-image) retrieval performance (Recall@5%) on the CLIP benchmark. \n\nModel Avg flickr30k mscoco_captions crossmodal3600 xtd10 \n\nnllb-clip-large-siglip 83.19 92.24 70.84 82.07 87.60 jina-clip-v2 81.12 89.84 68.35 81.43 84.87 jina-embeddings-v4 84.11 91.36 76.18 79.42 89.46 Avg : Mean Recall@5% over all 4 tasks. \n\nTable A9: Text-to-image retrieval performance (Recall@5%) on xtd10 for all supported languages. \n\nLanguage jina-embeddings-v4 jina-clip-v2 nllb-clip-large-siglip average 89.46 84.87 87.60 de 92.10 85.70 88.30 en 93.10 89.40 89.40 es 91.50 85.90 88.20 fr 91.30 85.10 87.70 it 92.20 85.80 89.30 ko 86.30 82.10 85.20 pl 89.10 86.50 89.40 \n\nru 91.50 81.10 83.40 tr 84.70 83.70 88.30 \n\nzh 82.80 83.40 86.80 \n\nTable A10: Text-to-image retrieval performance (Recall@5%) on crossmodal3600 for all supported languages. \n\nLanguage jina-embeddings-v4 jina-clip-v2 nllb-clip-large-siglip average 79.42 81.43 82.07 \n\nar 75.75 73.56 78.92 \n\nbn 57.97 63.78 75.19 \n\nda 80.47 85.39 87.14 \n\nde 91.75 91.25 89.56 el 66.50 75.03 77.83 \n\nen 76.47 75.83 73.11 es 83.64 83.64 82.64 fi 66.67 82.83 86.42 \n\nfr 88.69 88.78 87.86 hi 47.81 55.25 60.31 \n\nid 87.41 84.22 86.31 it 87.97 88.33 85.94 ja 91.22 87.03 86.06 ko 82.19 78.81 78.75 nl 81.00 82.56 81.69 no 71.94 81.08 82.69 \n\npl 80.86 84.00 82.72 pt 81.42 82.42 82.69 \n\nro 84.33 89.36 90.03 \n\nru 90.28 88.97 86.44 sv 72.58 78.06 79.33 \n\nth 83.36 81.61 81.14 tr 73.08 81.31 83.47 \n\nuk 86.28 88.56 85.44 vi 88.81 86.64 85.56 zh 86.67 78.97 76.56 A.4 MTEB and MMTEB \n\nTable A11: Evaluation Results for Various Models on MTEB Retrieval Tasks (nDCG@10%) \n\nModel Arg CQG CQU CFHN FEV FiQA HPQA SCI TREC TOU AVG \n\nmultilingual-e5-large 54.36 58.70 39.89 26.00 83.79 43.82 70.55 17.45 71.15 49.59 51.53 e5-mistral-7b-instruct 61.65 63.52 46.75 28.50 86.99 56.81 73.21 16.32 87.03 55.44 57.62 text-embedding-3-large 57.99 65.40 50.02 30.10 88.53 55.00 71.66 23.07 79.56 58.42 57.98 gemini-embedding-001 86.44 70.68 53.69 31.06 88.98 61.78 87.01 25.15 86.32 52.39 64.35 jina-embedding-l-en-v1 48.3 51.68 38.66 25.93 71.16 41.02 57.26 18.54 60.34 62.34 47.52 jina-embeddings-v2-base-en 44.18 56.52 38.66 23.77 73.41 41.58 63.24 19.86 65.91 63.35 49.05 jina-embeddings-v3‚Ä† 54.33 58.02 43.52 43.14 89.90 47.35 64.70 19.92 77.74 55.28 55.39 jina-embeddings-v4‚Ä† 67.07 57.59 42.95 34.57 87.16 46.51 69.01 21.47 80.36 52.41 55.91 ‚Ä†using the text-matching adapter \n\nTasks : Arg: ArguAna, CQG: CQADupstackGamingRetrieval, CQU: CQADupstackUnixRetrieval, CFHN: ClimateFEVERHardNegatives, FEV: FEVERHardNegatives, FiQA: FiQA2018, HPQA: HotpotQAHardNegatives, SCI: SCIDOCS, TREC: TRECCOVID, TOU: Touche2020Retrieval.v3 \n\nTable A12: Evaluation Results for Various Models on MMTEB Retrieval Tasks (nDCG@10%) \n\nModel Avg AI Arg Bel Cov Hag PK LB MIR ML SD SQA SO TC STC TR TW Wiki WG \n\njina-embeddings-v3 58.6 32.8 54.3 73.4 78.6 98.7 38.0 93.4 62.6 73.4 19.8 0.7 90.8 77.7 39.2 0.6 73.0 89.1 18.6 jina-embeddings-v4 66.5 50.2 67.1 74.3 80.2 98.8 69.8 94.8 61.2 74.9 21.5 30.2 91.9 80.4 59.5 1.3 84.4 88.5 67.3 bge-m3 55.4 29.0 54.0 78.2 77.5 98.8 59.0 90.3 69.6 74.8 16.3 7.5 80.6 54.9 21.9 1.0 37.8 89.9 41.7 Cohere-embed-mult.-v3 59.2 29.7 55.1 81.1 77.1 98.8 38.2 93.8 68.0 76.1 19.3 4.7 89.4 83.4 24.2 0.9 75.8 90.9 58.4 gemini-embedding-001 68.1 48.8 86.4 90.7 79.1 99.3 38.5 96.0 70.4 84.2 25.2 10.3 96.7 86.3 51.1 3.0 98.0 94.2 60.5 text-embedding-3-large 61.1 42.0 58.0 68.8 68.4 99.1 69.8 95.2 56.9 73.2 23.1 7.4 92.4 79.6 31.1 2.1 81.4 89.2 29.1 voyage-3 66.0 42.5 61.0 76.5 88.5 98.6 94.8 94.5 57.7 75.7 21.4 10.7 94.3 80.5 49.2 1.2 85.7 89.7 67.7 voyage-multilingual-2 ‚Äì 45.0 61.8 ‚Äì ‚Äì 98.9 97.0 95.9 ‚Äì ‚Äì 22.5 10.2 ‚Äì 80.1 ‚Äì 1.4 87.3 ‚Äì 39.1 \n\nTasks: Avg: Mean nDCG@10% for all tasks, AI: AILAStatutes, Arg: ArguAna, Bel: BelebeleRetrieval, Cov: CovidRetrieval, Hag: HagridRetrieval, PK: LEMBPasskeyRetrieval, LB: LegalBenchCorporateLobbying, MIR: MIRACLRetrievalHardNegatives, ML: MLQARetrieval, SD: SCIDOCS, SQA: SpartQA, SO: StackOverflowQA, TC: TREC-COVID, STC: StatcanDialogueDatasetRetrieval, TR: TempReasonL1, TW: TwitterHjerneRetrieval, Wiki: WikipediaRetrievalMultilingual, WG: WinoGrande \n\nTable A13: Retrieval performance on MTEB LongEmbed (nDCG@10%) \n\nModel Avg NaQA Needle Passkey QMSum SummScreen Wikim \n\njina-embeddings-v3 55.66 34.30 64.00 38.00 39.34 92.33 66.02 jina-embeddings-v4 67.11 57.52 51.75 65.50 46.49 96.30 85.08 voyage-multilingual-2 79.17 64.69 75.25 97.00 51.50 99.11 87.49 voyage-3 74.07 54.12 57.75 94.75 51.05 97.82 88.90 voyage-3-lite 71.41 51.67 54.00 84.75 53.01 96.71 88.34 bge-m3 58.73 45.76 40.25 59.00 35.54 94.09 77.73 text-embedding-3-large 52.42 44.09 29.25 69.75 32.49 84.80 54.16 Cohere-embed-english-v3 42.11 25.04 30.50 38.50 23.82 75.77 59.03 multilingual-e5-large-instruct 41.76 26.71 29.50 37.75 26.08 72.75 57.79 multilingual-e5-large 40.44 24.22 28.00 38.25 24.26 71.12 56.80 \n\nTasks: Avg: Mean nDCG@10% for all tasks, NaQA: LEMBNarrativeQARetrieval, Needle: LEMBNeedleRetrieval, Passkey: LEMBPasskeyRetrieval, QMSum: LEMBQMSumRetrieval, SummScreen: LEMBSummScreenFDRetrieval, Wikim: LEMBWikimQARetrieval Table A14: STS performance on MTEB v2 (Spearman correlation %). \n\nModel Avg BIO SICK-R STS12 STS13 STS14 STS15 STS17 STS22 STSB \n\njina-embeddings-v3 85.82 88.69 89.62 82.44 89.49 84.95 89.32 90.01 68.45 89.43 jina-embeddings-v4 85.89 89.21 89.23 83.50 88.61 84.77 89.69 88.71 70.71 88.58 BAAI/bge-m3 80.61 ‚Äì 79.72 78.73 79.60 79.00 87.81 87.13 67.99 84.87 Cohere-embed-English-3 82.40 83.50 81.27 74.37 85.20 80.98 89.23 90.34 68.18 88.55 Cohere-embed-multilingual-v3 83.05 85.01 82.18 77.62 85.16 80.02 88.92 90.09 69.63 88.79 gemini-embedding-001 85.29 88.97 82.75 81.55 89.89 85.41 90.44 91.61 67.97 89.08 multilingual-e5-large 81.39 84.57 80.23 80.02 81.55 77.72 89.31 88.12 63.66 87.29 text-embedding-3-large 81.44 84.68 79.00 72.84 86.10 81.15 88.49 90.22 66.89 83.56 voyage-3 78.59 87.92 79.63 69.52 80.56 73.33 80.39 86.81 69.60 79.53 voyage-large-2 82.63 89.13 79.78 72.94 83.11 77.21 85.30 88.77 ‚Äì 84.78 voyage-multilingual-v2 76.98 87.11 78.97 67.30 80.09 71.98 78.07 86.52 67.02 75.79 \n\nTasks: Avg: Mean Spearman Correlation % for all tasks, BIO: BIOSSES, STS22: STS22v2, STSB: STSBenchmark \n\nTable A15: STS performance on MMTEB v2 (Spearman correlation %). \n\nModel Avg Faro FinPara Indic JSICK SICK-R STS12 STS13 STS14 STS15 STS17 STS22 STSB STSES SemRel \n\njina-embeddings-v4 72.70 72.28 14.43 35.22 80.33 89.23 83.50 88.61 84.77 89.69 88.71 70.71 88.58 75.31 56.46 jina-embeddings-v3 75.77 80.82 22.38 54.66 78.16 89.62 82.44 89.49 84.94 89.31 85.94 71.14 89.44 77.87 64.58 bge-m3 72.99 77.80 30.43 52.13 79.21 79.72 78.73 79.60 79.00 87.81 79.65 70.03 84.87 77.50 65.38 Cohere-embed-mult.-v3 73.77 75.95 28.24 46.73 77.19 82.18 77.62 85.16 80.02 88.92 90.09 69.36 88.79 78.76 63.84 gemini-embedding-001 78.35 86.12 28.60 62.87 84.99 82.75 81.55 89.89 85.41 90.44 88.58 71.69 89.08 81.75 73.14 text-embedding-3-large 70.17 74.96 23.51 12.59 81.24 79.00 72.84 86.10 81.15 88.49 90.22 69.29 83.56 74.20 65.25 voyage-3 68.33 72.51 22.51 41.63 71.76 79.63 69.52 80.56 73.33 80.39 76.24 71.88 79.53 72.51 64.66 voyage-multilingual-2 68.02 74.42 27.07 35.03 75.94 78.97 67.30 80.09 71.98 78.07 77.06 69.03 75.79 76.69 64.88 \n\nTasks: Avg: Mean Spearman Correlation % for all tasks, Faro: FaroeseSTS, FinPara: FinParaSTS, Indic: IndicCrosslingualSTS, STS22: STS22v2, STSB: STSBenchmark, SemRel: SemRel24STS \n\nTable A16: Performance on MTEB Code Information Retrieval (MTEB-CoIR) (nDCG@10%). \n\nModel Avg AppsR CCSN CodeMT CodeST CodeSN CodeTO CodeTD CosQA StackO SynSQL \n\njina-embeddings-v2-code 52.24 16.37 83.97 44.40 68.66 59.62 75.68 27.25 41.92 89.26 46.99 jina-embeddings-v3 55.07 29.01 ‚Äì 59.67 78.14 53.18 77.37 30.91 35.34 90.79 41.27 jina-embeddings-v4 71.59 76.08 84.05 70.60 85.06 83.69 89.34 44.19 31.48 93.45 70.45 Cohere-embed-English-3 51.36 13.72 ‚Äì 47.02 74.82 52.81 65.28 31.38 30.65 89.35 57.20 Cohere-embed-mult.-v3 54.31 31.91 ‚Äì 42.91 74.19 57.57 70.25 30.14 32.58 89.42 59.79 gemini-embedding-001 73.11 93.75 81.06 56.28 85.33 84.69 89.53 31.47 50.24 96.71 69.96 text-embedding-3-large 62.36 28.37 ‚Äì 68.92 80.42 73.18 84.25 34.23 31.00 92.44 68.45 voyage-3 67.23 73.03 ‚Äì 66.69 83.02 77.87 89.92 33.92 28.70 94.34 57.56 voyage-code-3 77.33 93.62 89.35 93.58 90.67 90.09 94.96 38.57 34.45 97.17 62.87 \n\nTasks: Avg: Mean nDCG@10% for all tasks, AppsR: AppsRetrieval, COIR: COIRCodeSearchNetRetrieval, CodeMT: CodeFeedbackMT, CodeST: CodeFeedbackST, CodeSN: CodeSearchNetCCRetrieval, CodeTO: CodeTransOceanContest, CodeTD: CodeTransOceanDL, StackO: StackOverflowQA, SynSQL: SyntheticText2SQL",
      "metadata": {
        "og:title": "Jina-Embeddings-v4: Universal Retrieval",
        "twitter:title": "Jina-Embeddings-v4: Universal Retrieval",
        "description": "Explore jina-embeddings-v4, a 3.8B parameter model unifying text and image representations for advanced multimodal retrieval tasks.",
        "og:description": "Explore jina-embeddings-v4, a 3.8B parameter model unifying text and image representations for advanced multimodal retrieval tasks.",
        "twitter:description": "Explore jina-embeddings-v4, a 3.8B parameter model unifying text and image representations for advanced multimodal retrieval tasks.",
        "viewport": "width=device-width, initial-scale=1, maximum-scale=1",
        "og:url": "https://www.emergentmind.com/papers/2506.18902",
        "og:type": "website",
        "og:image": "https://www.emergentmind.com/assets/600px-de01e05f621530a0ba35a8d965fe1df76d665f468a4b87d87cbd56b6fa43f6c2.png",
        "twitter:image": "https://www.emergentmind.com/assets/630px-1cccb16c85be4d313d7eb23413713404cfb13ba85a026ab5a579417980d157e4.png",
        "twitter:card": "summary_large_image",
        "twitter:site": "@emergentmind",
        "csrf-param": "authenticity_token",
        "csrf-token": "K_HeI9yvOz-W__-mWnIsXuoTxghFC8bQ7MWMo7pHdVMPD0ox7LkQKMIR5Dp7BtjbqwKZBNXSBjHYnx8acWzKTQ"
      },
      "external": {
        "canonical": {
          "https://www.emergentmind.com/papers/2506.18902": {}
        },
        "icon": {
          "https://www.emergentmind.com/assets/logo/favicon-e0f26fd04308a9c98635aa08353f11780220c26740b7c8733ca2c4a5bd79040b.png": {
            "sizes": "any"
          },
          "https://www.emergentmind.com/assets/logo/icon-ca36f21ff6275fcde05a9be02ad97f58b413b83ff4b2e727f91cce8442f65515.svg": {
            "type": "image/svg+xml"
          }
        },
        "apple-touch-icon": {
          "https://www.emergentmind.com/assets/logo/apple-touch-icon-ad1f78e0419265ed63adfe2b5f0dd8f3835497e464535be5da744591c0c096db.png": {}
        },
        "preload": {
          "https://www.emergentmind.com/assets/charter_regular-b09165d1a0acb8c9dd990cb2077ffc5bf2dd0fe9ac46df63b93d4d3e7a284551.woff2": {
            "as": "font",
            "type": "font/woff2",
            "crossorigin": "anonymous"
          }
        },
        "modulepreload": {
          "https://www.emergentmind.com/assets/application-6617385414669750d6e0346d3ee3160b3ef44bdcf40f00919ddf76420848f684.js": {},
          "https://www.emergentmind.com/assets/turbo.min-569fe252dd55eef2e3cff9a6e83c8b9a2b0e2374a72d15522515e1ff9999ec78.js": {},
          "https://www.emergentmind.com/assets/stimulus.min-59f6a188a51873d87a6ae8218ac6e829404b5cacd7f2a8fb7249abfdec5ece6a.js": {},
          "https://www.emergentmind.com/assets/stimulus-loading-6024ee603e0509bba59098881b54a52936debca30ff797835b5ec6a4ef77ba37.js": {},
          "https://ga.jspm.io/npm:alpinejs@3.10.2/dist/module.esm.js": {},
          "https://ga.jspm.io/npm:reqwest@2.0.5/reqwest.js": {},
          "https://ga.jspm.io/npm:xhr2@0.2.1/lib/browser.js": {},
          "https://ga.jspm.io/npm:linkify-string@4.0.2/dist/linkify-string.es.js": {},
          "https://ga.jspm.io/npm:linkifyjs@4.0.2/dist/linkify.es.js": {},
          "https://ga.jspm.io/npm:@ryangjchandler/alpine-tooltip@1.2.0/dist/module.esm.js": {},
          "https://www.emergentmind.com/assets/luxon-ae68e346c68113dab2a7d3bdd0836fad456aff6c699b778a37c2c4940d47ccfd.js": {},
          "https://ga.jspm.io/npm:lodash@4.17.21/lodash.js": {},
          "https://ga.jspm.io/npm:js-cookie@3.0.5/dist/js.cookie.mjs": {},
          "https://ga.jspm.io/npm:@alpinejs/collapse@3.13.3/dist/module.esm.js": {},
          "https://ga.jspm.io/npm:mixpanel-browser@2.48.1/dist/mixpanel.cjs.js": {},
          "https://www.emergentmind.com/assets/controllers/application-44e5edd38372876617b8ba873a82d48737d4c089e5180f706bdea0bb7b6370be.js": {},
          "https://www.emergentmind.com/assets/controllers/article_controller-cba562c934a663f6f9c5c5e93ab0a4ada1ade8bdbd4ff1c890021ea43c143aa3.js": {},
          "https://www.emergentmind.com/assets/controllers/conversation_controller-1928b82a5439d9c50179a279728665a1a9d1935408880d91511f0475110a71f4.js": {},
          "https://www.emergentmind.com/assets/controllers/index-c1418d0fda496766b43bd794d1685e371f00ed7858f54eebc0deb4f6c5c8c843.js": {},
          "https://www.emergentmind.com/assets/controllers/message_status_controller-a8f7371f392d7ed29b9844a3f98c5c0c159034284c6340e84249b71b32e109c3.js": {},
          "https://www.emergentmind.com/assets/controllers/pdf_fullscreen_controller-56f757dcfc81a85cbc2f77e03cf46d665f984e7d862ac8a10280afd094b8c024.js": {},
          "https://www.emergentmind.com/assets/controllers/tts_controller-d5f48ca62287eb8333d198611d4b9ec4c096b0adb852ef97d715580ce43170cb.js": {},
          "https://www.emergentmind.com/assets/controllers/turbo_stream_controller-e9e1ed01d7d7d90f7c300f2391f14cdca3c62f1452ff82ec5cc78ad5b93788f2.js": {}
        }
      },
      "usage": {
        "tokens": 23627
      }
    },
    {
      "title": "Unlock Next-Level RAG Performance with the Jina v4 Embedding Model",
      "url": "https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag",
      "description": "Unlock next-level RAG performance with Jina v4, the embedding model designed for precision, efficiency, and complex data challenges.",
      "date": "Jul 4, 2025",
      "content": "How Jina v4 Transforms Multimodal and Multilingual RAG Systems - Geeky Gadgets\n\n===============\n\n*   [Skip to main content](https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag#genesis-content)\n*   [Skip to secondary menu](https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag#genesis-nav-secondary)\n*   [Skip to primary sidebar](https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag#genesis-sidebar-primary)\n*   [Skip to footer](https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag#genesis-footer-widgets)\n\n[Geeky Gadgets](https://www.geeky-gadgets.com/)\n\nThe Latest Technology News\n\n*   [Home](https://www.geeky-gadgets.com/)\n*   [Top News](https://www.geeky-gadgets.com/category/top-news/)\n*   [AI](https://www.geeky-gadgets.com/category/artificial-intelligence/)\n*   [Apple](https://www.geeky-gadgets.com/category/apple/)\n*   [Android](https://www.geeky-gadgets.com/category/android/)\n*   [Technology](https://www.geeky-gadgets.com/category/technology-news/)\n*   [Guides](https://www.geeky-gadgets.com/category/guides/)\n*   [Gadgets](https://www.geeky-gadgets.com/category/gadgets/)\n*   [Hardware](https://www.geeky-gadgets.com/category/pc-hardware/)\n*   [Gaming](https://www.geeky-gadgets.com/category/gaming/)\n*   [Autos](https://www.geeky-gadgets.com/category/autos/)\n*   [Deals](https://deals.geeky-gadgets.com/)\n*   [About](https://www.geeky-gadgets.com/about/ \"About Geeky Gadgets\")\n\nUnlock Next-Level RAG Performance with the Jina v4 Embedding Model\n==================================================================\n\n7:39 am July 4, 2025 By [Julian Horsey](https://www.geeky-gadgets.com/author/jhorsey/)\n\nWhat if the key to unlocking next-level performance in **retrieval-augmented generation (RAG)** wasn‚Äôt just about better algorithms or more data, but the embedding model powering it all? In a world where precision and adaptability are paramount, choosing the right embedding model can mean the difference between fantastic insights and frustrating inefficiencies. Enter Jina v4‚Äîa model that doesn‚Äôt just keep up with the demands of modern RAG systems but redefines what‚Äôs possible. With its **multimodal and multilingual capabilities**, Jina v4 isn‚Äôt just another tool; it‚Äôs a fantastic option for industries tackling complex, data-rich challenges.\n\nPrompt Engineering uncovers why Jina v4 stands out as the ultimate embedding model for RAG. From its ability to seamlessly integrate text and images into a unified space to its **task-specific adaptability** and storage efficiency, Jina v4 offers a suite of features designed to tackle even the most intricate workflows. Whether you‚Äôre optimizing search systems, enhancing content generation, or managing multilingual datasets, this model promises to deliver results that go beyond expectations. But what makes it truly unique? Let‚Äôs explore the innovations that set Jina v4 apart and why it might just be the embedding solution you didn‚Äôt know you needed.\n\nJina v4 Embedding Overview\n--------------------------\n\n**TL;DR Key Takeaways :**\n\n*   Jina v4 is a multimodal and multilingual embedding model capable of processing both text and images, supporting 29 languages and high-resolution images up to 20 megapixels.\n*   It features advanced embedding capabilities, including dense and multi-vector representations, adjustable embedding sizes (128 to 2448 dimensions), long-context support (up to 32,000 tokens), and late chunking for improved accuracy.\n*   The model offers task-specific adaptability through Low-Rank Adaptations (LoRAs), allowing fine-tuning for specialized applications like text retrieval, code search, and classification.\n*   Efficiency is a key strength, with fixed-size vector outputs reducing storage requirements and streamlining multimodal retrieval-augmented generation (RAG) workflows.\n*   Jina v4 is ideal for diverse use cases, including retrieval-augmented generation, text matching, code retrieval, and multimodal search systems, but requires significant computational resources for optimal performance.\n\nMultimodal and Multilingual Excellence\n--------------------------------------\n\nJina v4 is engineered to integrate diverse data types, combining text and image inputs into a unified embedding space. This capability allows you to handle complex queries that involve multiple modalities, such as searching for text descriptions of images or retrieving images based on textual input. Supporting **29 languages**, the model ensures global applicability, making it an ideal choice for multilingual use cases. Additionally, it processes high-resolution images up to **20 megapixels**, allowing the embedding of intricate visual data with remarkable accuracy.\n\nThis multimodal and multilingual design makes Jina v4 particularly effective for industries requiring cross-lingual and cross-modal retrieval, such as e-commerce, media, and research. By embedding text and images in the same space, the model simplifies workflows and enhances the precision of search results.\n\nAdvanced Embedding Features\n---------------------------\n\nJina v4 introduces a range of advanced features that enhance both its accuracy and flexibility:\n\n*   **Dense and Multi-Vector Representations:** Dense embeddings provide compact, efficient representations, while multi-vector options offer detailed and granular data encoding for more complex tasks.\n*   **Adjustable Embedding Sizes:** The model supports dimensions ranging from 128 to 2448, allowing you to balance computational efficiency and performance based on your specific requirements.\n*   **Long-Context Support:** With the ability to process up to 32,000 tokens, Jina v4 ensures that large documents or extended conversations retain their contextual relevance.\n*   **Late Chunking:** This feature segments data only when necessary, preserving the integrity of the context for more accurate embeddings.\n\nThese features collectively make Jina v4 a versatile tool, capable of addressing a wide variety of embedding challenges. Whether you are working with short queries or extensive datasets, the model‚Äôs adaptability ensures optimal performance.\n\nBest Embedding Model You Need for RAG\n-------------------------------------\n\n[Watch this video on YouTube](https://youtu.be/p7yRLIj9IyQ).\n\nHere are more guides from our previous articles and guides related to **Multimodal embedding** that you may find helpful.\n\n*   [How to build AI apps on Vertex AI with LangChain](https://www.geeky-gadgets.com/building-ai-powered-apps/)\n*   [AI Advancements: Claude 3.7, GPT 4.5, and Multimodal Tools](https://www.geeky-gadgets.com/multimodal-ai-tools-for-creators/)\n*   [Llama 4 Meta Open LLMs : Everything You Need to Know](https://www.geeky-gadgets.com/llama-4-ai-models-tested/)\n*   [How LocalGPT 2.0 Protects Your Data While Delivering AI Precision](https://www.geeky-gadgets.com/localgpt-2-0-unlock-ai-power-without-sacrificing-privacy/)\n*   [From Code to Vision: Explore Ollama‚Äôs Powerful AI Models](https://www.geeky-gadgets.com/from-code-to-vision-explore-ollamas-powerful-ai-models/)\n*   [OpenAI‚Äôs AI Image Generator Enhancing Visual Content Creation](https://www.geeky-gadgets.com/how-to-use-chatgpt-image-generator/)\n*   [How to supercharge Llama 2 with vision and hearing](https://www.geeky-gadgets.com/llama-2-vision-and-hearing/)\n*   [How to setup Google Gemini Pro API key and AI model](https://www.geeky-gadgets.com/how-to-setup-gemini-pro-api/)\n*   [ChatGPT-5 Everything we know so far](https://www.geeky-gadgets.com/chatgpt-5-details-revealed/)\n*   [How Anthropic‚Äôs Think Tool Enhances AI Problem-Solving](https://www.geeky-gadgets.com/anthropic-think-tool-enhanced-ai-performance/)\n\nTask-Specific Adaptability\n--------------------------\n\nJina v4‚Äôs adaptability is further enhanced by its use of **Low-Rank Adaptations (LoRAs)**, which are task-specific adapters designed to fine-tune the model for specialized applications. These adapters allow you to optimize embeddings for tasks such as text retrieval, code search, or classification. By tailoring the model to your unique requirements, you can achieve improved accuracy and efficiency across a wide range of use cases.\n\nThis task-specific flexibility is particularly valuable for organizations with diverse needs. For example, a company might use Jina v4 to power a multilingual customer support chatbot, while simultaneously employing it for internal code search and document retrieval. The ability to fine-tune the model for each task ensures consistent, high-quality results.\n\nEfficiency and Storage Optimization\n-----------------------------------\n\nOne of Jina v4‚Äôs most notable strengths is its focus on efficiency. By generating **fixed-size vector outputs**, the model significantly reduces storage requirements compared to traditional multi-vector approaches. This is a critical advantage for large-scale applications, where storage costs can quickly become prohibitive. Additionally, the model‚Äôs ability to embed text and images in the same space streamlines multimodal RAG pipelines, reducing the complexity of processing and retrieval workflows.\n\nFor organizations managing extensive datasets, this efficiency translates into tangible cost savings and operational improvements. By minimizing storage demands without compromising performance, Jina v4 enables scalable solutions for even the most resource-intensive tasks.\n\nApplications and Use Cases\n--------------------------\n\nJina v4‚Äôs versatility makes it suitable for a wide range of applications, including:\n\n*   **Retrieval-Augmented Generation:** Enhances the quality of generated content by retrieving relevant data to inform responses or outputs.\n*   **Text Matching and Topic Clustering:** Assists accurate categorization and similarity analysis for content organization and discovery.\n*   **Code Retrieval:** Optimizes search and retrieval processes in programming-related tasks, improving developer productivity.\n*   **Multimodal Search Systems:** Combines text and image queries to deliver comprehensive and precise search results.\n\nThese use cases highlight the model‚Äôs ability to address complex challenges across industries, from improving customer experiences to streamlining internal operations.\n\nTechnical Specifications\n------------------------\n\nBuilt on a robust **3.8 billion parameter architecture**, Jina v4 features a vision-language backbone that seamlessly integrates text and image processing. This design ensures high performance across a variety of tasks, but it also demands significant computational resources. Tasks involving multi-vector representations or high-resolution image embeddings, in particular, require advanced infrastructure to achieve optimal results.\n\nOrganizations considering Jina v4 should evaluate their computational capabilities to ensure they can fully use the model‚Äôs potential. For those with the necessary resources, the model offers a powerful combination of performance and versatility.\n\nHow Jina v4 Compares to Other Models\n------------------------------------\n\nJina v4 sets itself apart from traditional dense embedding models and ColBERT-inspired multi-vector systems through its superior multimodal capabilities and flexibility. Competing directly with models like Nvidia‚Äôs NeMo Retriever, Jina v4 offers additional features such as **adjustable embedding sizes** and **task-specific adapters**, providing greater control and customization. These enhancements make it a compelling choice for embedding processes, particularly for organizations seeking a model that can adapt to diverse and evolving needs.\n\nChallenges to Consider\n----------------------\n\nWhile Jina v4 offers numerous advantages, it is not without challenges. Its high computational requirements, particularly for tasks involving multi-vector representations and image embeddings, may pose a barrier for smaller organizations or those with limited resources. However, for organizations equipped to meet these demands, the model delivers unmatched performance and versatility.\n\nBy carefully assessing your infrastructure and resource availability, you can determine whether Jina v4 is the right fit for your needs. For those who can accommodate its requirements, the model‚Äôs benefits far outweigh its challenges, making it a valuable investment in advanced embedding technology.\n\nMedia Credit: [Prompt Engineering](https://www.youtube.com/watch?v=p7yRLIj9IyQ)\n\n*   [Share](https://www.facebook.com/dialog/feed?app_id=364456067078847&display=popup&caption=Unlock%20Next-Level%20RAG%20Performance%20with%20the%20Jina%20v4%20Embedding%20Model&link=https%3A%2F%2Fwww.geeky-gadgets.com%2Fjina-v4-embedding-model-for-rag%2F&description=%0D%0A%0D%0AWhat%20if%20the%20key%20to%20unlocking%20next-level%20performance%20in%20retrieval-augmented%20generation%20%28RAG%29%20wasn%E2%80%99t%20just%20about%20better%20algorithms%20or%20more%20data%2C%20but%20the%20embedding%20model%20powering%20it%20all%3F%20In%20a%20world%20where%20precision%20and%20adaptability%20are%20%E2%80%A6&picture=https%3A%2F%2Fwww.geeky-gadgets.com%2Fwp-content%2Fuploads%2F2025%2F07%2Fjenna-v4-embedding-model-for-rag_optimized-1024x576.jpg \"Share on Facebook\")\n*   [Tweet](https://twitter.com/intent/tweet?text=Unlock%20Next-Level%20RAG%20Performance%20with%20the%20Jina%20v4%20Embedding%20Model%20https%3A%2F%2Fwww.geeky-gadgets.com%2Fjina-v4-embedding-model-for-rag%2F \"Tweet\")\n*   [Pin](https://pinterest.com/pin/create/bookmarklet/?media=https%3A%2F%2Fwww.geeky-gadgets.com%2Fwp-content%2Fuploads%2F2025%2F07%2Fjenna-v4-embedding-model-for-rag_optimized-1024x576.jpg&url=https%3A%2F%2Fwww.geeky-gadgets.com%2Fjina-v4-embedding-model-for-rag%2F&is_video=false&description=Unlock%20Next-Level%20RAG%20Performance%20with%20the%20Jina%20v4%20Embedding%20Model%20-%20%0D%0A%0D%0AWhat%20if%20the%20key%20to%20unlocking%20next-level%20performance%20in%20retrieval-augmented%20generation%20%28RAG%29%20wasn%E2%80%99t%20just%20about%20better%20algorithms%20or%20more%20data%2C%20but%20the%20embedding%20model%20powering%20it%20all%3F%20In%20a%20world%20where%20precision%20and%20adaptability%20are%20%E2%80%A6 \"Pin\")\n*   [Email](mailto:?subject=Unlock%20Next-Level%20RAG%20Performance%20with%20the%20Jina%20v4%20Embedding%20Model&body=%0D%0A%0D%0AWhat%20if%20the%20key%20to%20unlocking%20next-level%20performance%20in%20retrieval-augmented%20generation%20%28RAG%29%20wasn%E2%80%99t%20just%20about%20better%20algorithms%20or%20more%20data%2C%20but%20the%20embedding%20model%20powering%20it%20all%3F%20In%20a%20world%20where%20precision%20and%20adaptability%20are%20%E2%80%A6%20https%3A%2F%2Fwww.geeky-gadgets.com%2Fjina-v4-embedding-model-for-rag%2F \"Email\")\n\nFiled Under: [AI](https://www.geeky-gadgets.com/category/artificial-intelligence/), [Top News](https://www.geeky-gadgets.com/category/top-news/)\n**Latest Geeky Gadgets Deals**\n\n**Disclosure:** Some of our articles include affiliate links. If you buy something through one of these links, Geeky Gadgets may earn an affiliate commission. Learn about our [Disclosure Policy](https://www.geeky-gadgets.com/disclosure-policy/).\n\nPrimary Sidebar\n---------------\n\n*   [](mailto:rols@geeky-gadgets.com)\n*   [](https://www.facebook.com/GeekyGadgets/)\n*   [](https://www.pinterest.co.uk/geekygadgets/)\n*   [](https://follow.it/geeky-gadgets)\n*   [](https://twitter.com/geekygadgets)\n\nSearch the site ... \n\n### Top News\n\n[](https://www.geeky-gadgets.com/flawless-ai-code-generation-with-claude-code-hooks/)\n#### [Getting Claude Code to do Exactly What You Ask : Hooks](https://www.geeky-gadgets.com/flawless-ai-code-generation-with-claude-code-hooks/)\n\n[](https://www.geeky-gadgets.com/iphone-20-xx/)\n#### [iPhone 20 XX: The ULTIMATE Leak & Rumor Roundup](https://www.geeky-gadgets.com/iphone-20-xx/)\n\n[](https://www.geeky-gadgets.com/samsung-galaxy-g-fold-release-date/)\n#### [Samsung Galaxy G Fold Release Date: When Will It Launh?](https://www.geeky-gadgets.com/samsung-galaxy-g-fold-release-date/)\n\n[](https://www.geeky-gadgets.com/model-context-protocol-mcp-explained-2/)\n#### [Model Context Protocol (MCP) Explained With Code Examples)](https://www.geeky-gadgets.com/model-context-protocol-mcp-explained-2/)\n\n[](https://www.geeky-gadgets.com/apple-music-ios-26-deep-dive/)\n#### [The ULTIMATE Apple Music iOS 26 Deep Dive: All 25+ New Features!](https://www.geeky-gadgets.com/apple-music-ios-26-deep-dive/)\n\n### Guides\n\n[](https://www.geeky-gadgets.com/apple-music-ios-26-deep-dive/)\n#### [The ULTIMATE Apple Music iOS 26 Deep Dive: All 25+ New Features!](https://www.geeky-gadgets.com/apple-music-ios-26-deep-dive/)\n\n[](https://www.geeky-gadgets.com/building-ai-voice-agents-with-pipecat/)\n#### [Build Real-Time AI Voice Agents with ChatGPT and Pipecat](https://www.geeky-gadgets.com/building-ai-voice-agents-with-pipecat/)\n\n[](https://www.geeky-gadgets.com/how-to-install-claude-code-on-windows/)\n#### [How to Install Claude Code on Windows](https://www.geeky-gadgets.com/how-to-install-claude-code-on-windows/)\n\n[](https://www.geeky-gadgets.com/midjourney-video-generator-tips/)\n#### [11 Tips to Master Midjourney‚Äôs New AI Video Generator for Stunning Results](https://www.geeky-gadgets.com/midjourney-video-generator-tips/)\n\n[](https://www.geeky-gadgets.com/iphone-security-essentials/)\n#### [iPhone Security Essentials: Change Passcode & Activate Stolen Device Protection](https://www.geeky-gadgets.com/iphone-security-essentials/)\n\n### Apple News\n\n[](https://www.geeky-gadgets.com/iphone-20-xx/)\n#### [iPhone 20 XX: The ULTIMATE Leak & Rumor Roundup](https://www.geeky-gadgets.com/iphone-20-xx/)\n\n[](https://www.geeky-gadgets.com/apple-music-ios-26-deep-dive/)\n#### [The ULTIMATE Apple Music iOS 26 Deep Dive: All 25+ New Features!](https://www.geeky-gadgets.com/apple-music-ios-26-deep-dive/)\n\n[](https://www.geeky-gadgets.com/iphone-17-pro-max-leaks-3/)\n#### [The iPhone 17 Pro Max Leaks Deep Dive: What to Expect from Apple‚Äôs Next Flagship](https://www.geeky-gadgets.com/iphone-17-pro-max-leaks-3/)\n\n[](https://www.geeky-gadgets.com/iphone-camera-ios-26/)\n#### [A Complete Overhaul: iOS 26 Transforms Your iPhone Camera Experience](https://www.geeky-gadgets.com/iphone-camera-ios-26/)\n\n[](https://www.geeky-gadgets.com/iphone-security-essentials/)\n#### [iPhone Security Essentials: Change Passcode & Activate Stolen Device Protection](https://www.geeky-gadgets.com/iphone-security-essentials/)\n\n### Technology News\n\n[](https://www.geeky-gadgets.com/airpods-pro-3-confirmed/)\n#### [AirPods Pro 3 CONFIRMED? Apple‚Äôs Own Code Spills the Beans!](https://www.geeky-gadgets.com/airpods-pro-3-confirmed/)\n\n[](https://www.geeky-gadgets.com/iios-26-confirms-apples-siri-replacement-is-coming/)\n#### [The AI Overhaul: iOS 26 Confirms Apple‚Äôs Siri Replacement is Coming!](https://www.geeky-gadgets.com/iios-26-confirms-apples-siri-replacement-is-coming/)\n\n[](https://www.geeky-gadgets.com/nothing-headphone-1/)\n#### [Nothing Headphone (1): The Bold Design You‚Äôve Been Waiting For, Plus Incredible Sound](https://www.geeky-gadgets.com/nothing-headphone-1/)\n\n[](https://www.geeky-gadgets.com/create-pro-level-designs-with-the-ultimate-laser-engraver-and-laser-cutter-machine/)\n#### [Create Pro-Level Designs with the Ultimate Laser Engraver and Laser Cutter Machine](https://www.geeky-gadgets.com/create-pro-level-designs-with-the-ultimate-laser-engraver-and-laser-cutter-machine/)\n\n[](https://www.geeky-gadgets.com/ai-large-language-models-security-vulnerabilities/)\n#### [How AI‚Äôs Greatest Strengths Are Becoming Its Biggest Weaknesses](https://www.geeky-gadgets.com/ai-large-language-models-security-vulnerabilities/)\n\n### Android News\n\n[](https://www.geeky-gadgets.com/samsung-galaxy-g-fold-release-date/)\n#### [Samsung Galaxy G Fold Release Date: When Will It Launh?](https://www.geeky-gadgets.com/samsung-galaxy-g-fold-release-date/)\n\n[](https://www.geeky-gadgets.com/samsung-galaxy-s26-ultra-9/)\n#### [Samsung Galaxy S26 Ultra: 4 Revolutionary Changes Coming to Samsung‚Äôs Flagship](https://www.geeky-gadgets.com/samsung-galaxy-s26-ultra-9/)\n\n[](https://www.geeky-gadgets.com/samsung-galaxy-z-fold-7-vs-z-fold-6-thinner-design-bigger-screens-a-new-camera/)\n#### [Samsung Galaxy Z Fold 7 vs. Z Fold 6: Thinner Design, Bigger Screens, & A New Camera!](https://www.geeky-gadgets.com/samsung-galaxy-z-fold-7-vs-z-fold-6-thinner-design-bigger-screens-a-new-camera/)\n\n[](https://www.geeky-gadgets.com/honor-magic-v5/)\n#### [Honor Magic V5: Is Being the Thinnest Foldable Worth It? (The Pros & Cons)](https://www.geeky-gadgets.com/honor-magic-v5/)\n\n[](https://www.geeky-gadgets.com/nothing-headphone-1/)\n#### [Nothing Headphone (1): The Bold Design You‚Äôve Been Waiting For, Plus Incredible Sound](https://www.geeky-gadgets.com/nothing-headphone-1/)\n\nFooter\n------\n\n### About Us\n\n*   [About Geeky Gadgets](https://www.geeky-gadgets.com/about/)\n*   [Advertise On Geeky Gadgets](https://www.geeky-gadgets.com/advertise/)\n*   [Archives](https://www.geeky-gadgets.com/archives/)\n*   [Contact Geeky Gadgets](https://www.geeky-gadgets.com/contact/)\n*   [Disclosure Policy](https://www.geeky-gadgets.com/disclosure-policy/)\n*   [Free Newsletter](https://www.geeky-gadgets.com/free-newsletter/)\n*   [Geeky Gadgets Logo](https://www.geeky-gadgets.com/geeky-gadgets-logo/)\n*   [Privacy Policy](https://www.geeky-gadgets.com/privacy/)\n*   [Site Map](https://www.geeky-gadgets.com/site-map/)\n\n### Further Reading\n\n*   [AI](https://www.geeky-gadgets.com/category/artificial-intelligence/)\n*   [Apple iPad](https://www.geeky-gadgets.com/category/ipad/)\n*   [Apple iPhone](https://www.geeky-gadgets.com/category/iphone/)\n*   [Auto News](https://www.geeky-gadgets.com/category/autos/)\n*   [Camera News](https://www.geeky-gadgets.com/category/cameras/)\n*   [Concepts & Design](https://www.geeky-gadgets.com/category/concepts/)\n*   [Design News](https://www.geeky-gadgets.com/category/design/)\n*   [Entertainment News](https://www.geeky-gadgets.com/category/entertainment/)\n*   [Geeky Stuff](https://www.geeky-gadgets.com/category/geeky/)\n*   [Guides](https://www.geeky-gadgets.com/category/guides/)\n*   [Mobile Phone News](https://www.geeky-gadgets.com/category/mobile-phones/)\n*   [Reviews](https://www.geeky-gadgets.com/category/reviews/)\n*   [Sponsored](https://www.geeky-gadgets.com/category/sponsored/)\n*   [Tablet News](https://www.geeky-gadgets.com/category/tablets/)\n\nCopyright 2007 - 2023 Geeky Gadgets \n\n[](https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag#)\n\n[Geeky Gadgets](https://www.geeky-gadgets.com/)\n\n[](https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag#)\n\n*   [Home](https://www.geeky-gadgets.com/)\n*   [Top News](https://www.geeky-gadgets.com/category/top-news/)\n*   [AI](https://www.geeky-gadgets.com/category/artificial-intelligence/)\n*   [Apple](https://www.geeky-gadgets.com/category/apple/)\n*   [Android](https://www.geeky-gadgets.com/category/android/)\n*   [Technology](https://www.geeky-gadgets.com/category/technology-news/)\n*   [Guides](https://www.geeky-gadgets.com/category/guides/)\n*   [Gadgets](https://www.geeky-gadgets.com/category/gadgets/)\n*   [Hardware](https://www.geeky-gadgets.com/category/pc-hardware/)\n*   [Gaming](https://www.geeky-gadgets.com/category/gaming/)\n*   [Autos](https://www.geeky-gadgets.com/category/autos/)\n*   [Deals](https://deals.geeky-gadgets.com/)\n*   [About](https://www.geeky-gadgets.com/about/ \"About Geeky Gadgets\")",
      "publishedTime": "2025-07-04T06:39:54+00:00",
      "metadata": {
        "viewport": "width=device-width, initial-scale=1",
        "robots": "index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1",
        "description": "Unlock next-level RAG performance with Jina v4, the embedding model designed for precision, efficiency, and complex data challenges.",
        "og:locale": "en_US",
        "og:type": "article",
        "og:title": "Unlock Next-Level RAG Performance with the Jina v4 Embedding Model",
        "og:description": "Unlock next-level RAG performance with Jina v4, the embedding model designed for precision, efficiency, and complex data challenges.",
        "og:url": "https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag/",
        "og:site_name": "Geeky Gadgets",
        "article:publisher": "https://www.facebook.com/GeekyGadgets",
        "article:published_time": "2025-07-04T06:39:54+00:00",
        "og:image": "https://www.geeky-gadgets.com/wp-content/uploads/2025/07/jenna-v4-embedding-model-for-rag_optimized.jpg",
        "og:image:width": "1280",
        "og:image:height": "720",
        "og:image:type": "image/jpeg",
        "author": "Julian Horsey",
        "twitter:card": "summary_large_image",
        "twitter:creator": "@geekygadgets",
        "twitter:site": "@geekygadgets",
        "plausible-analytics-version": "2.3.1"
      },
      "external": {
        "preload": {
          "https://www.geeky-gadgets.com/wp-content/uploads/2025/07/multimodal-multilingual-embedding-jenna-v4.webp": {
            "as": "image",
            "imagesrcset": "https://www.geeky-gadgets.com/wp-content/uploads/2025/07/multimodal-multilingual-embedding-jenna-v4.webp 1280w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/multimodal-multilingual-embedding-jenna-v4-300x169.webp 300w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/multimodal-multilingual-embedding-jenna-v4-1024x576.webp 1024w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/multimodal-multilingual-embedding-jenna-v4-75x42.webp 75w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/multimodal-multilingual-embedding-jenna-v4-768x432.webp 768w",
            "imagesizes": "(max-width: 1280px) 100vw, 1280px",
            "fetchpriority": "high"
          },
          "https://www.geeky-gadgets.com/wp-content/uploads/2025/07/ai-code-generation-hooks-guide_optimized-75x42.jpg": {
            "as": "image",
            "imagesrcset": "https://www.geeky-gadgets.com/wp-content/uploads/2025/07/ai-code-generation-hooks-guide_optimized-75x42.jpg 75w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/ai-code-generation-hooks-guide_optimized-300x168.jpg 300w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/ai-code-generation-hooks-guide_optimized-1024x572.jpg 1024w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/ai-code-generation-hooks-guide_optimized-768x429.jpg 768w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/ai-code-generation-hooks-guide_optimized-630x350.jpg 630w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/ai-code-generation-hooks-guide_optimized-750x420.jpg 750w, https://www.geeky-gadgets.com/wp-content/uploads/2025/07/ai-code-generation-hooks-guide_optimized.jpg 1280w",
            "imagesizes": "(max-width: 75px) 100vw, 75px",
            "fetchpriority": "high"
          },
          "https://a.pub.network/core/pubfig/cls.css": {
            "as": "style"
          },
          "https://a.pub.network/geeky-gadgets-com/pubfig.min.js": {
            "as": "script"
          }
        },
        "canonical": {
          "https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag/": {}
        },
        "dns-prefetch": {
          "https://geeky-gadgets.net/": {}
        },
        "alternate": {
          "https://www.geeky-gadgets.com/feed/": {
            "type": "application/rss+xml",
            "title": "Geeky Gadgets ¬ª Feed"
          },
          "https://www.geeky-gadgets.com/comments/feed/": {
            "type": "application/rss+xml",
            "title": "Geeky Gadgets ¬ª Comments Feed"
          },
          "https://www.geeky-gadgets.com/jina-v4-embedding-model-for-rag/feed/": {
            "type": "application/rss+xml",
            "title": "Geeky Gadgets ¬ª Unlock Next-Level RAG Performance with the Jina v4 Embedding Model Comments Feed"
          }
        },
        "icon": {
          "https://www.geeky-gadgets.com/wp-content/themes/magazine-pro/images/favicon.ico": {}
        }
      },
      "usage": {
        "tokens": 6159
      }
    },
    {
      "title": "jinaai/jina-embeddings-v4-vllm-code ¬∑ Hugging Face",
      "url": "https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code",
      "description": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
      "content": "jinaai/jina-embeddings-v4-vllm-code ¬∑ Hugging Face\n\n===============\n\n[Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*    Community  \n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n*    \n*   \n* * *\n\n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n[](https://huggingface.co/jinaai)\n\n[jinaai](https://huggingface.co/jinaai)\n\n/\n\n[jina-embeddings-v4-vllm-code](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code)\n\nlike 1\n\nFollow\n\nJina AI 1.14k\n==============================================================================================================================================================================================================\n\n[Visual Document Retrieval](https://huggingface.co/models?pipeline_tag=visual-document-retrieval)[Transformers](https://huggingface.co/models?library=transformers)[Safetensors](https://huggingface.co/models?library=safetensors)[ColPali](https://huggingface.co/models?library=colpali)[multilingual](https://huggingface.co/models?language=multilingual)[qwen2_5_vl](https://huggingface.co/models?other=qwen2_5_vl)[image-text-to-text](https://huggingface.co/models?other=image-text-to-text)[vidore](https://huggingface.co/models?other=vidore)[multimodal-embedding](https://huggingface.co/models?other=multimodal-embedding)[multilingual-embedding](https://huggingface.co/models?other=multilingual-embedding)[Text-to-Visual Document (T‚ÜíVD) retrieval](https://huggingface.co/models?other=Text-to-Visual+Document+%28T%E2%86%92VD%29+retrieval)[feature-extraction](https://huggingface.co/models?other=feature-extraction)[sentence-similarity](https://huggingface.co/models?other=sentence-similarity)[mteb](https://huggingface.co/models?other=mteb)[text-generation-inference](https://huggingface.co/models?other=text-generation-inference)\n\narxiv:2506.18902\n\nLicense:cc-by-nc-4.0\n\n[üá™üá∫ Region: EU](https://huggingface.co/models?other=region%3Aeu)\n\n[Model card](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code)[Files Files and versions](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code/tree/main)[Community 1](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code/discussions)\n\n Train \n\n Deploy \n\n Use this model \n\n*   [Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code#jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval \"Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval\")\n    *   [Model Overview](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code#model-overview \"Model Overview\")\n\n    *   [Usage](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code#usage \"Usage\")\n\n**The embedding model trained by [**Jina AI**](https://jina.ai/).**\n\n[](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code#jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval) Jina Embeddings v4: Universal Embeddings for Multimodal Multilingual Retrieval\n===========================================================================================================================================================================================================================\n\n[Original Model](https://huggingface.co/jinaai/jina-embeddings-v4) | [Blog](https://jina.ai/news/jina-embeddings-v4-universal-embeddings-for-multimodal-multilingual-retrieval) | [Technical Report](https://arxiv.org/abs/2506.18902) | [API](https://jina.ai/embeddings)\n\n[](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code#model-overview) Model Overview\n--------------------------------------------------------------------------------------------\n\nThis repository hosts a vLLM-compatible version of [`jina-embeddings-v4`](https://huggingface.co/jinaai/jina-embeddings-v4) with the **code** adapter merged into the base `Qwen2.5-VL` weights. This architecture modification enables native compatibility with vLLM without requiring custom adapter-handling code.\n\n[](https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code#usage) Usage\n--------------------------------------------------------------------------\n\n```python\nimport torch\nfrom PIL import Image\n\nfrom vllm import LLM\nfrom vllm.config import PoolerConfig\nfrom vllm.inputs.data import TextPrompt\n\n# Initialize model\nmodel = LLM(\n    model=\"jinaai/jina-embeddings-v4-vllm-code\",\n    task=\"embed\",\n    override_pooler_config=PoolerConfig(pooling_type=\"ALL\", normalize=False),\n    dtype=\"float16\",\n)\n\n# Create text prompts\nquery =query = \"Find a function that prints a greeting message to the console\"\nquery_prompt = TextPrompt(\n    prompt=f\"Query: {query}\"\n)\n\npassage = \"def hello_world():\\n    print('Hello, World!')\"\npassage_prompt = TextPrompt(\n    prompt=f\"Passage: {passage}\"\n)\n\n# Create image prompt\nimage = Image.open(\"<path_to_image>\")\nimage_prompt = TextPrompt(\n    prompt=\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|>\\n\",\n    multi_modal_data={\"image\": image},\n)\n\n# Encode all prompts\nprompts = [query_prompt, passage_prompt, image_prompt]\noutputs = model.encode(prompts)\n\ndef get_embeddings(outputs):\n    VISION_START_TOKEN_ID, VISION_END_TOKEN_ID = 151652, 151653\n\n    embeddings = []\n    for output in outputs:\n        if VISION_START_TOKEN_ID in output.prompt_token_ids:\n            # Gather only vision tokens\n            img_start_pos = torch.where(\n                torch.tensor(output.prompt_token_ids) == VISION_START_TOKEN_ID\n            )[0][0]\n            img_end_pos = torch.where(\n                torch.tensor(output.prompt_token_ids) == VISION_END_TOKEN_ID\n            )[0][0]\n            embeddings_tensor = output.outputs.data.detach().clone()[\n                img_start_pos : img_end_pos + 1\n            ]\n        else:\n            # Use all tokens for text-only prompts\n            embeddings_tensor = output.outputs.data.detach().clone()\n        \n        # Pool and normalize embeddings\n        pooled_output = (\n            embeddings_tensor.sum(dim=0, dtype=torch.float32)\n            / embeddings_tensor.shape[0]\n        )\n        embeddings.append(torch.nn.functional.normalize(pooled_output, dim=-1))\n    return embeddings\n\nembeddings = get_embeddings(outputs)\n```\n\nDownloads last month 86 \n\nSafetensors[](https://huggingface.co/docs/safetensors)\n\nModel size\n\n3.75B params\n\nTensor type\n\nBF16 \n\n¬∑\n\nChat template\n\nFiles info\n\nInference Providers[NEW](https://huggingface.co/docs/inference-providers)\n\n[Visual Document Retrieval](https://huggingface.co/tasks/visual-document-retrieval \"Learn more about visual-document-retrieval\")\n\nThis model isn't deployed by any Inference Provider.[üôãAsk for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=jinaai/jina-embeddings-v4-vllm-code&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bjinaai%2Fjina-embeddings-v4-vllm-code%5D(%2Fjinaai%2Fjina-embeddings-v4-vllm-code)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n\n System theme \n\nCompany\n\n[TOS](https://huggingface.co/terms-of-service)[Privacy](https://huggingface.co/privacy)[About](https://huggingface.co/huggingface)[Jobs](https://apply.workable.com/huggingface/)[](https://huggingface.co/)\n\nWebsite\n\n[Models](https://huggingface.co/models)[Datasets](https://huggingface.co/datasets)[Spaces](https://huggingface.co/spaces)[Pricing](https://huggingface.co/pricing)[Docs](https://huggingface.co/docs)",
      "metadata": {
        "viewport": "width=device-width, initial-scale=1.0, user-scalable=no",
        "description": "We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.",
        "fb:app_id": "1321688464574422",
        "twitter:card": "summary_large_image",
        "twitter:site": "@huggingface",
        "twitter:image": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v4-vllm-code.png",
        "og:title": "jinaai/jina-embeddings-v4-vllm-code ¬∑ Hugging Face",
        "og:type": "website",
        "og:url": "https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code",
        "og:image": "https://cdn-thumbnails.huggingface.co/social-thumbnails/models/jinaai/jina-embeddings-v4-vllm-code.png"
      },
      "external": {
        "preconnect": {
          "https://fonts.gstatic.com/": {}
        },
        "preload": {
          "https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css": {
            "as": "style",
            "onload": "this.onload=null;this.rel='stylesheet'"
          }
        },
        "canonical": {
          "https://huggingface.co/jinaai/jina-embeddings-v4-vllm-code": {}
        }
      },
      "usage": {
        "tokens": 2043
      }
    },
    {
      "title": "Embedding API",
      "url": "https://jina.ai/embeddings",
      "description": "Top-performing multimodal multilingual long-context embeddings for search, RAG, agents applications.",
      "content": "Embedding API\n\n===============\n\n[](https://jina.ai/)\n\n_search_[News](https://jina.ai/news)[Models](https://jina.ai/models)Products _arrow\\_drop\\_down_ Company _arrow\\_drop\\_down_\n\n_language_\n\n [_login_ Log in](https://jina.ai/api-dashboard?login=true)\n\nEmbeddings \n\n_new\\_releases_ v4 release!\n========================================\n\nTop-performing multimodal multilingual long-context embeddings for search, RAG, agents applications.\n\n_code_ API\n\n* * *\n\n_attach\\_money_ Pricing\n\nEmbedding API\n-------------\n\nTry our world-class embedding models to improve your search and RAG systems. Start with a free trial!\n\n_report\\_problem_ We cannot generate an API key because we couldn't verify if you are human. Refreshing the page multiple times may help. If you believe this is a mistake, please contact us with the debug info. \n\n_refresh_ Refresh _bug\\_report_ Debug Info[Contact](https://jina.ai/contact-sales)\n\n_login_\n\n_key_\n\nAPI Key & Billing\n\n_code_\n\nUsage\n\n_more\\_horiz_\n\nMore\n\n_service\\_toolbox_ Tools _arrow\\_drop\\_down_\n\n_chevron\\_left_ _chevron\\_right_\n\n* * *\n\n[_home_](https://jina.ai/embeddings)\n\n[_speed_ Rate Limit](https://jina.ai/api-dashboard/rate-limit)\n\n[_bug\\_report_ Raise issue](https://huggingface.co/jinaai/undefined/discussions)\n\n_cloud_ On CSP _arrow\\_drop\\_down_\n\n[_help\\_outline_ FAQ](https://jina.ai/embeddings#faq)\n\n[_api_](https://api.jina.ai/redoc#tag/embeddings)\n\n[Status](https://status.jina.ai/)\n\n_chevron\\_left_ _chevron\\_right_\n\n* * *\n\nSelect embeddings\n\n_arrow\\_drop\\_down_\n\n \n\nL2 normalization\n\nScales the embedding so its Euclidean (L2) norm becomes 1, preserving direction. Useful when downstream involves dot-product, classification, visualization.\n\n- [x] \n\n \n\nOutput data type\n\nInstead of float, you can set it to binary for faster vector retrieval, or as base64 encoding for faster transmission.\n\nDefault (as float)\n\n_arrow\\_drop\\_down_\n\n \n\n \n\n* * *\n\nExample input\n\nChange them and see how the response changes!\n\n1\n\n_text\\_fields_\n\n \n\n2\n\n_text\\_fields_\n\n \n\n3\n\n_text\\_fields_\n\n \n\n4\n\n_text\\_fields_\n\n \n\n5\n\n_text\\_fields_\n\n \n\n* * *\n\n_upload_\n\nRequest\n\nBash\n\nLanguage\n\n_arrow\\_drop\\_down_\n\n \n\n_wrap\\_text_\n\n```\ncurl https://api.jina.ai/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer \" \\\n  -d @- <<EOFEOF\n  {\n    \"normalized\": true,\n    \"embedding_type\": \"float\",\n    \"input\": [\n        \"Organic skincare for sensitive skin with aloe vera and chamomile: Imagine the soothing embrace of nature with our organic skincare range, crafted specifically for sensitive skin. Infused with the calming properties of aloe vera and chamomile, each product provides gentle nourishment and protection. Say goodbye to irritation and hello to a glowing, healthy complexion.\",\n        \"Bio-Hautpflege f√ºr empfindliche Haut mit Aloe Vera und Kamille: Erleben Sie die wohltuende Wirkung unserer Bio-Hautpflege, speziell f√ºr empfindliche Haut entwickelt. Mit den beruhigenden Eigenschaften von Aloe Vera und Kamille pflegen und sch√ºtzen unsere Produkte Ihre Haut auf nat√ºrliche Weise. Verabschieden Sie sich von Hautirritationen und genie√üen Sie einen strahlenden Teint.\",\n        \"Cuidado de la piel org√°nico para piel sensible con aloe vera y manzanilla: Descubre el poder de la naturaleza con nuestra l√≠nea de cuidado de la piel org√°nico, dise√±ada especialmente para pieles sensibles. Enriquecidos con aloe vera y manzanilla, estos productos ofrecen una hidrataci√≥n y protecci√≥n suave. Desp√≠dete de las irritaciones y saluda a una piel radiante y saludable.\",\n        \"ÈíàÂØπÊïèÊÑüËÇå‰∏ìÈó®ËÆæËÆ°ÁöÑÂ§©ÁÑ∂ÊúâÊú∫Êä§ËÇ§‰∫ßÂìÅÔºö‰ΩìÈ™åÁî±Ëä¶ËçüÂíåÊ¥ãÁîòËèäÊèêÂèñÁâ©Â∏¶Êù•ÁöÑËá™ÁÑ∂ÂëµÊä§„ÄÇÊàë‰ª¨ÁöÑÊä§ËÇ§‰∫ßÂìÅÁâπÂà´‰∏∫ÊïèÊÑüËÇåËÆæËÆ°ÔºåÊ∏©ÂíåÊªãÊ∂¶Ôºå‰øùÊä§ÊÇ®ÁöÑËÇåËÇ§‰∏çÂèóÂà∫ÊøÄ„ÄÇËÆ©ÊÇ®ÁöÑËÇåËÇ§ÂëäÂà´‰∏çÈÄÇÔºåËøéÊù•ÂÅ•Â∫∑ÂÖâÂΩ©„ÄÇ\",\n        \"Êñ∞„Åó„ÅÑ„É°„Ç§„ÇØ„ÅÆ„Éà„É¨„É≥„Éâ„ÅØÈÆÆ„ÇÑ„Åã„Å™Ëâ≤„Å®Èù©Êñ∞ÁöÑ„Å™ÊäÄË°ì„Å´ÁÑ¶ÁÇπ„ÇíÂΩì„Å¶„Å¶„ÅÑ„Åæ„Åô: ‰ªä„Ç∑„Éº„Ç∫„É≥„ÅÆ„É°„Ç§„ÇØ„Ç¢„ÉÉ„Éó„Éà„É¨„É≥„Éâ„ÅØ„ÄÅÂ§ßËÉÜ„Å™Ëâ≤ÂΩ©„Å®Èù©Êñ∞ÁöÑ„Å™ÊäÄË°ì„Å´Ê≥®ÁõÆ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Éç„Ç™„É≥„Ç¢„Ç§„É©„Ç§„Éä„Éº„Åã„Çâ„Éõ„É≠„Ç∞„É©„Éï„Ç£„ÉÉ„ÇØ„Éè„Ç§„É©„Ç§„Çø„Éº„Åæ„Åß„ÄÅ„ÇØ„É™„Ç®„Ç§„ÉÜ„Ç£„Éì„ÉÜ„Ç£„ÇíËß£„ÅçÊîæ„Å°„ÄÅÊØéÂõû„É¶„Éã„Éº„ÇØ„Å™„É´„ÉÉ„ÇØ„ÇíÊºîÂá∫„Åó„Åæ„Åó„Çá„ÅÜ„ÄÇ\"\n    ]\n  }\nEOFEOF\n```\n\n_content\\_copy_\n\n* * *\n\n_send_ GET RESPONSE\n\n* * *\n\n_key_\n\nAPI key\n\n_visibility\\_off_ _content\\_copy_\n\n* * *\n\nAvailable tokens\n\n0 _sync_\n\nThis is your unique key. Store it securely!\n\n \n\nv4: Universal Embeddings for Multimodal Multilingual Retrieval\n--------------------------------------------------------------\n\njina-embeddings-v4 is our most significant leap yet ‚Äî a 3.8B model that embeds text and images through a unified pathway, supporting both dense and late-interaction retrieval while outperforming proprietary models from Google, OpenAI and Voyage AI especially on visually rich document retrieval.\n\n[Three Ways to Purchase](https://jina.ai/embeddings#pricing)\n------------------------------------------------------------\n\nSubscribe to our API, purchase through cloud providers, or obtain a commercial license for your organization.\n\n_radio\\_button\\_unchecked_\n\n_cloud_\n\nWith **3** cloud service providers\n\nUsing AWS or Azure? You can deploy our models directly on your company's cloud platform and handle billing through the CSP account.\n\n AWS SageMaker\n\nEmbeddings\n\nReranker\n\n Microsoft Azure\n\nEmbeddings\n\nReranker\n\n Google Cloud\n\nEmbeddings\n\nReranker\n\n_radio\\_button\\_checked_\n\nWith Jina Search Foundation API\n\nThe easiest way to access all of our products. Top-up tokens as you go.\n\n_key_\n\n_content\\_copy_\n\nEnter the API key you wish to recharge\n\n_error_\n\n_visibility\\_off_\n\n \n\n_verified\\_user_\n\nTop up this API key with more tokens\n\nDepending on your location, you may be charged in USD, EUR, or other currencies. Taxes may apply.\n\nToy Experiment\n\n10 Million\n\nTokens valid for: \n\n Non-commercial use only (CC-BY-NC).\n\nFree\n\nEnjoy your new API key with free tokens, no credit card required.\n\nPrototype Development\n\n1 Billion\n\nTokens valid for: \n\n_key_ Standard key\n\n_task\\_alt_ Basic key management\n\n_task\\_alt_ Technical support\n\n$50\n\n0.050 / 1M tokens\n\n_add\\_shopping\\_cart_\n\nProduction Deployment\n\n11 Billion\n\nTokens valid for: \n\n_key_ Premium key with much higher rate limits\n\n_task\\_alt_ Advanced key management\n\n_task\\_alt_ Premium customer support in 24 hours\n\n_task\\_alt_ One-hour integration consultation\n\n$500\n\n0.045 / 1M tokens\n\n_add\\_shopping\\_cart_\n\nPlease input the right API key to top up\n\n_speed_\n\nUnderstand the rate limit\n\nRate limits are the maximum number of requests that can be made to an API within a minute per IP address/API key (RPM). Find out more about the rate limits for each product and tier below.\n\n_keyboard\\_arrow\\_down_\n\nRate Limit\n\nRate limits are tracked in three ways: **RPM** (requests per minute), and **TPM** (tokens per minute). Limits are enforced per IP/API key and will be triggered when either the RPM or TPM threshold is reached first. When you provide an API key in the request header, we track rate limits by key rather than IP address.\n\nColumns\n\n_arrow\\_drop\\_down_\n\n \n\n_fullscreen_\n\n|  | Product | API Endpoint | Description _arrow\\_upward_ | w/o API Key _key\\_off_ | w/ API Key _key_ | w/ Premium API Key _key_ | Average Latency | Token Usage Counting | Allowed Request |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | Embedding API | `https://api.jina.ai/v1/embeddings` | Convert text/images to fixed-length vectors | _block_ | 500 RPM & 1,000,000 TPM | _trending\\_up_ 2,000 RPM & 5,000,000 TPM | _ssid\\_chart_ depends on the input size _help_ | Count the number of tokens in the input request. | POST |\n|  | Reader API | `https://r.jina.ai` | Convert URL to LLM-friendly text | 20 RPM | 500 RPM | _trending\\_up_ 5000 RPM | 7.9s | Count the number of tokens in the output response. | GET/POST |\n|  | Reader API | `https://s.jina.ai` | Search the web and convert results to LLM-friendly text | _block_ | 100 RPM | _trending\\_up_ 1000 RPM | 2.5s | Every request costs a fixed number of tokens, starting from 10000 tokens | GET/POST |\n|  | DeepSearch | `https://deepsearch.jina.ai/v1/chat/completions` | Reason, search and iterate to find the best answer | _block_ | 50 RPM | 500 RPM | 56.7s | Count the total number of tokens in the whole process. | POST |\n|  | Reranker API | `https://api.jina.ai/v1/rerank` | Rank documents by query | _block_ | 500 RPM & 1,000,000 TPM | _trending\\_up_ 2,000 RPM & 5,000,000 TPM | _ssid\\_chart_ depends on the input size _help_ | Count the number of tokens in the input request. | POST |\n|  | Classifier API | `https://api.jina.ai/v1/train` | Train a classifier using labeled examples | _block_ | 20 RPM & 200,000 TPM | 60 RPM & 1,000,000 TPM | _ssid\\_chart_ depends on the input size | Tokens counted as: input_tokens √ó num_iters | POST |\n|  | Classifier API (Few-shot) | `https://api.jina.ai/v1/classify` | Classify inputs using a trained few-shot classifier | _block_ | 20 RPM & 200,000 TPM | 60 RPM & 1,000,000 TPM | _ssid\\_chart_ depends on the input size | Tokens counted as: input_tokens | POST |\n|  | Classifier API (Zero-shot) | `https://api.jina.ai/v1/classify` | Classify inputs using zero-shot classification | _block_ | 200 RPM & 500,000 TPM | 1,000 RPM & 3,000,000 TPM | _ssid\\_chart_ depends on the input size | Tokens counted as: input_tokens + label_tokens | POST |\n|  | Segmenter API | `https://api.jina.ai/v1/segment` | Tokenize and segment long text | 20 RPM | 200 RPM | 1,000 RPM | 0.3s | Token is not counted as usage. | GET/POST |\n\n_currency\\_exchange_\n\nAuto top-up on low token balance\n\nRecommended for uninterrupted service in production. When your token balance drops below the set threshold, we will automatically recharge your saved payment method for the last purchased package, until the threshold is met.\n\n_info_ We introduced a new pricing model on May 6th, 2025. If you enabled auto-recharge before this date, you'll continue to pay the old price (the one when you purchased). The new pricing only applies if you modify your auto-recharge settings or purchase a new API key.\n\n_check_\n\n< 1M Tokens\n\nTop up when\n\n_arrow\\_drop\\_down_\n\n \n\n \n\n[_radio\\_button\\_unchecked_ _encrypted_ With a commercial license for on-prem use Require 100% control and privacy? Purchase a commercial license to use our models on-premises.](https://jina.ai/api-dashboard/license-config?login=true)\n\nOn-premises deployment\n----------------------\n\nDeploy Jina Embeddings models in AWS Sagemaker and Microsoft Azure, and soon in Google Cloud Services, or contact our sales team to get customized Kubernetes deployments for your Virtual Private Cloud and on-premises servers.\n\n AWS SageMaker\n\nEmbeddings\n\nReranker\n\n Microsoft Azure\n\nEmbeddings\n\nReranker\n\n Google Cloud\n\nEmbeddings\n\nReranker\n\nAPI Integrations\n\nOur Embedding API is natively integrated with various renowned databases, vector stores, RAG, and LLMOps frameworks. To begin, just copy and paste your API key into any of the listed integrations for a quick and seamless start.\n\nVector Store\n\nLLMOps\n\nRAG\n\nObservability\n\n_open\\_in\\_new_\n\nMongoDB\n\n_open\\_in\\_new_\n\nDataStax\n\n_open\\_in\\_new_\n\nQdrant\n\n_open\\_in\\_new_\n\nPinecone\n\n_open\\_in\\_new_\n\nChroma\n\n_open\\_in\\_new_\n\nWeaviate\n\n_open\\_in\\_new_\n\nMilvus\n\n_open\\_in\\_new_\n\nEpsilla\n\n_open\\_in\\_new_\n\nMyScale\n\n_open\\_in\\_new_\n\nLlamaIndex\n\n_open\\_in\\_new_\n\nHaystack\n\n_open\\_in\\_new_\n\nLangchain\n\n_open\\_in\\_new_\n\nDify\n\n_open\\_in\\_new_\n\nSuperDuperDB\n\n_open\\_in\\_new_\n\nDashVector\n\n_open\\_in\\_new_\n\nPortkey\n\n_open\\_in\\_new_\n\nBaseten\n\n_open\\_in\\_new_\n\nTiDB\n\n_open\\_in\\_new_\n\nLanceDB\n\n_open\\_in\\_new_\n\nCarbon\n\nOur Publications\n----------------\n\nUnderstand how our frontier search models were trained from scratch, check out our latest publications. Meet our team at EMNLP, SIGIR, ICLR, NeurIPS, and ICML!\n\n[arXiv June 24, 2025 jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)[ICLR 2025 March 04, 2025 ReaderLM-v2: Small Language Model for HTML to Markdown and JSON](https://arxiv.org/abs/2503.01151)[ACL 2025 December 17, 2024 AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark](https://arxiv.org/abs/2412.13102)[ICLR 2025 December 12, 2024 jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images](https://arxiv.org/abs/2412.08802)[ECIR 2025 September 18, 2024 jina-embeddings-v3: Multilingual Embeddings With Task LoRA](https://arxiv.org/abs/2409.10173)[SIGIR 2025 September 07, 2024 Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models](https://arxiv.org/abs/2409.04701)[EMNLP 2024 August 30, 2024 Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction Retriever](https://arxiv.org/abs/2408.16672)[WWW 2025 June 21, 2024 Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models](https://arxiv.org/abs/2406.14848)[ICML 2024 May 30, 2024 Jina CLIP: Your CLIP Model Is Also Your Text Retriever](https://arxiv.org/abs/2405.20204)[arXiv February 26, 2024 Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings](https://arxiv.org/abs/2402.17016)[arXiv October 30, 2023 Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents](https://arxiv.org/abs/2310.19923)[EMNLP 2023 July 20, 2023 Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models](https://arxiv.org/abs/2307.11224)\n\n12 publications in total.\n\nLearning about Embeddings\n-------------------------\n\nWhere to start with embeddings? We've got you covered. Learn about embeddings from the ground up with our comprehensive guide.\n\nComparison of Reranker, Vector Search, and BM25\n-----------------------------------------------\n\nThe table below provides a comprehensive comparison of the Reranker, Vector/Embeddings Search, and BM25, highlighting their strengths and weaknesses across various categories.\n\n|  | Reranker | Vector Search | BM25 |\n| --- | --- | --- | --- |\n| **Best For** | Enhanced search precision and relevance | Initial, rapid filtering | General text retrieval across wide-ranging queries |\n| **Granularity** | Detailed: Sub-document and query segment | Broad: Entire documents | Intermediate: Various text segments |\n| **Query Time Complexity** | High | Medium | Low |\n| **Indexing Time Complexity** | Not required | High | Low, utilizes pre-built index |\n| **Training Time Complexity** | High | High | Not required |\n| **Search Quality** | Superior for nuanced queries | Balanced between efficiency and accuracy | Consistent and reliable for a broad set of queries |\n| **Strengths** | Highly accurate with deep contextual understanding | Quick and efficient, with moderate accuracy | Highly scalable, with established efficacy |\n|  | [Try reranker API for free](https://jina.ai/reranker) | [Try embedding API for free](https://jina.ai/embeddings) |  |\n\nThe Evolution of Embeddings Poster\n----------------------------------\n\nDiscover the ideal poster for your space, featuring captivating infographics or breathtaking visuals tracing the evolution of text embedding models since 1950.\n\n[Learn how we made it](https://jina.ai/news/the-1950-2024-text-embeddings-evolution-poster)\n\n* * *\n\n[_shopping\\_cart_ Buy a hard copy](https://buy.stripe.com/cN2aHS4Ax5F19DqfZ7)\n\n[FAQ](https://jina.ai/embeddings#faq)\n-------------------------------------\n\nHow were the jina-embeddings-v3 models trained?\n\n_keyboard\\_arrow\\_down_\n\nFor detailed information on our training processes, data sources, and evaluations, please refer to our technical report available on arXiv.\n\n[_launch_ arXiv](https://arxiv.org/abs/2409.10173)\n\nWhat are the jina-clip models, and can I use them for text and image search?\n\n_keyboard\\_arrow\\_down_\n\nJina CLIP `jina-clip-v2` is an advanced multimodal embedding model that supports text-text, text-image, image-image, and image-text retrieval tasks. Unlike the original OpenAI CLIP, which struggles with text-text search, Jina CLIP excels as a text retriever. `jina-clip-v2` offers a 3% performance improvement over `jina-clip-v1` in both text-image and text-text retrieval tasks, supports 89 languages for multilingual image retrieval, processes higher resolution images (512x512), and reduces storage requirements with Matryoshka representations. You can read more about it in our tech report.\n\n[_launch_ arXiv](https://arxiv.org/abs/2412.08802)\n\nWhich languages do your models support?\n\n_keyboard\\_arrow\\_down_\n\nAs of its release on September 18, 2024, `jina-embeddings-v3` is the best multilingual model and ranks 2nd on the MTEB English leaderboard for models with fewer than 1 billion parameters. v3 supports a total of 89 languages, including the top 30 with the best performance: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, Georgian, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Latvian, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Spanish, Swedish, Thai, Turkish, Ukrainian, Urdu, and Vietnamese. For more details, please refer to the `jina-embeddings-v3` tech report.\n\n[_launch_ arXiv](https://arxiv.org/abs/2409.10173)\n\nWhat is the maximum length for a single sentence input?\n\n_keyboard\\_arrow\\_down_\n\nOur models allow for an input length of up to 8192 tokens, which is significantly higher than most other models. A token can range from a single character, like 'a', to an entire word, such as 'apple'. The total number of characters that can be input depends on the length and complexity of the words used. This extended input capability enables our `jina-embeddings-v3` and `jina-clip` models to perform more comprehensive text analysis and achieve higher accuracy in context understanding, especially for extensive textual data.\n\nWhat is the maximum number of sentences I can include in a single request?\n\n_keyboard\\_arrow\\_down_\n\nA single API call can process up to 2048 sentences or texts, facilitating extensive text analysis in one request.\n\nHow do I send images to the jina-clip models?\n\n_keyboard\\_arrow\\_down_\n\nYou can use either `url` or `bytes` in the `input` field of the API request. For `url`, provide the URL of the image you want to process. For `bytes`, encode the image in base64 format and include it in the request. The model will return the embeddings of the image in the response.\n\nHow do Jina Embeddings models compare to OpenAI's and Cohere's latest embeddings?\n\n_keyboard\\_arrow\\_down_\n\nIn evaluations on the MTEB English, Multilingual, and LongEmbed benchmarks, `jina-embeddings-v3` outperforms the latest proprietary embeddings from OpenAI and Cohere on English tasks, and surpasses `multilingual-e5-large-instruct` across all multilingual tasks. With a default output dimension of 1024, users can truncate the embedding dimensions down to 32 without sacrificing performance, thanks to the integration of Matryoshka Representation Learning (MRL).\n\nHow seamless is the transition from OpenAI's text-embedding-3-large to your solution?\n\n_keyboard\\_arrow\\_down_\n\nThe transition is streamlined, as [our API endpoint](https://api.jina.ai/v1/embeddings), matches the input and output JSON schemas of OpenAI‚Äôs `text-embedding-3-large` model. This compatibility ensures users can easily replace the OpenAI model with ours when using OpenAI‚Äôs endpoint.\n\nHow tokens are calculated when using jina-clip and jina-embeddings models?\n\n_keyboard\\_arrow\\_down_\n\nTokens are calculated based on the text length and image size. For text in the request, tokens are counted in the standard way. For images, the following steps are conducted: 1. Tile Size: Each image is divided into tiles. For `jina-embeddings-v4`, tiles are 28x28 pixels, for `jina-clip-v2`, tiles are 512x512 pixels, while for `jina-clip-v1`, tiles are 224x224 pixels. 2. Coverage: The number of tiles required to cover the input image is calculated. Even if the image dimensions are not perfectly divisible by the tile size, partial tiles are counted as full tiles. 3. Total Tiles: The total number of tiles covering the image determines the cost. For example, a 600x600 pixel image would be covered by 22x22 tiles (484 tiles) in jina-embeddings-v4, by 2x2 tiles (4 tiles) in jina-clip-v2 and 3x3 tiles (9 tiles) in jina-clip-v1. 4. Cost Calculation: For `jina-embeddings-v4`, each tile costs 10 tokens, for `jina-clip-v2`, each tile costs 4000 tokens, while for `jina-clip-v1`, each tile costs 1000 tokens. Example: For an image with dimensions 600x600 pixels: ‚Ä¢ With `jina-embeddings-v4` ‚Ä¢ The image is divided into 28x28 pixel tiles. ‚Ä¢ The total number of tiles required is 22 (horizontal) x 22 (vertical) = 484 tiles. ‚Ä¢ The cost for `jina-embeddings-v4` will be 484*10 = 4840 tokens. ‚Ä¢ With `jina-clip-v2` ‚Ä¢ The image is divided into 512x512 pixel tiles. ‚Ä¢ The total number of tiles required is 2 (horizontal) x 2 (vertical) = 4 tiles. ‚Ä¢ The cost for `jina-clip-v2` will be 4*4000 = 16000 tokens. ‚Ä¢ With `jina-clip-v1` ‚Ä¢ The image is divided into 224x224 pixel tiles. ‚Ä¢ The total number of tiles required is 3 (horizontal) x 3 (vertical) = 9 tiles. ‚Ä¢ The cost for jina-clip-v1 will be 9*1000 = 9000 tokens.\n\nDo you provide models for embedding images or audio?\n\n_keyboard\\_arrow\\_down_\n\nYes, `jina-embeddings-v4`, `jina-clip-v2` and `jina-clip-v1` can embed both images and texts. Embedding models on more modalities will be announced soon!\n\nCan Jina Embedding models be fine-tuned with private or company data?\n\n_keyboard\\_arrow\\_down_\n\nFor inquiries about fine-tuning our models with specific data, please contact us to discuss your requirements. We are open to exploring how our models can be adapted to meet your needs.\n\n[Contact](https://jina.ai/contact-sales)\n\nCan your endpoints be hosted privately on AWS, Azure, or GCP?\n\n_keyboard\\_arrow\\_down_\n\nYes, our services are available on AWS, Azure, and GCP marketplaces. If you have specific requirements, please contact us at sales AT jina.ai.\n\n[_launch_ AWS SageMaker](https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy)[_launch_ Google Cloud](https://console.cloud.google.com/marketplace/browse?q=jina&pli=1&inv=1&invt=AbmydQ)[_launch_ Microsoft Azure](https://azuremarketplace.microsoft.com/en-US/marketplace/apps?page=1&search=jina)\n\n### [How to get my API key?](https://jina.ai/embeddings#get-api-key)\n\n video_not_supported\n\n### [What's the rate limit?](https://jina.ai/embeddings#rate-limit)\n\nRate Limit\n\nRate limits are tracked in three ways: **RPM** (requests per minute), and **TPM** (tokens per minute). Limits are enforced per IP/API key and will be triggered when either the RPM or TPM threshold is reached first. When you provide an API key in the request header, we track rate limits by key rather than IP address.\n\nColumns\n\n_arrow\\_drop\\_down_\n\n \n\n_fullscreen_\n\n|  | Product | API Endpoint | Description _arrow\\_upward_ | w/o API Key _key\\_off_ | w/ API Key _key_ | w/ Premium API Key _key_ | Average Latency | Token Usage Counting | Allowed Request |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | Embedding API | `https://api.jina.ai/v1/embeddings` | Convert text/images to fixed-length vectors | _block_ | 500 RPM & 1,000,000 TPM | _trending\\_up_ 2,000 RPM & 5,000,000 TPM | _ssid\\_chart_ depends on the input size _help_ | Count the number of tokens in the input request. | POST |\n|  | Reader API | `https://r.jina.ai` | Convert URL to LLM-friendly text | 20 RPM | 500 RPM | _trending\\_up_ 5000 RPM | 7.9s | Count the number of tokens in the output response. | GET/POST |\n|  | Reader API | `https://s.jina.ai` | Search the web and convert results to LLM-friendly text | _block_ | 100 RPM | _trending\\_up_ 1000 RPM | 2.5s | Every request costs a fixed number of tokens, starting from 10000 tokens | GET/POST |\n|  | DeepSearch | `https://deepsearch.jina.ai/v1/chat/completions` | Reason, search and iterate to find the best answer | _block_ | 50 RPM | 500 RPM | 56.7s | Count the total number of tokens in the whole process. | POST |\n|  | Reranker API | `https://api.jina.ai/v1/rerank` | Rank documents by query | _block_ | 500 RPM & 1,000,000 TPM | _trending\\_up_ 2,000 RPM & 5,000,000 TPM | _ssid\\_chart_ depends on the input size _help_ | Count the number of tokens in the input request. | POST |\n|  | Classifier API | `https://api.jina.ai/v1/train` | Train a classifier using labeled examples | _block_ | 20 RPM & 200,000 TPM | 60 RPM & 1,000,000 TPM | _ssid\\_chart_ depends on the input size | Tokens counted as: input_tokens √ó num_iters | POST |\n|  | Classifier API (Few-shot) | `https://api.jina.ai/v1/classify` | Classify inputs using a trained few-shot classifier | _block_ | 20 RPM & 200,000 TPM | 60 RPM & 1,000,000 TPM | _ssid\\_chart_ depends on the input size | Tokens counted as: input_tokens | POST |\n|  | Classifier API (Zero-shot) | `https://api.jina.ai/v1/classify` | Classify inputs using zero-shot classification | _block_ | 200 RPM & 500,000 TPM | 1,000 RPM & 3,000,000 TPM | _ssid\\_chart_ depends on the input size | Tokens counted as: input_tokens + label_tokens | POST |\n|  | Segmenter API | `https://api.jina.ai/v1/segment` | Tokenize and segment long text | 20 RPM | 200 RPM | 1,000 RPM | 0.3s | Token is not counted as usage. | GET/POST |\n\n### [Do I need a commercial license?](https://jina.ai/embeddings#cc-self-check)\n\nCC BY-NC License Self-Check\n\n* * *\n\n_play\\_arrow_\n\nAre you using our official API or official images on Azure or AWS?\n\n_play\\_arrow_\n\n_done_\n\nYes\n\n_play\\_arrow_\n\nAre you using a paid API key or free trial key?\n\n_play\\_arrow_\n\n_done_\n\nPaid API key\n\nNo restrictions. Use as per your current agreement.\n\n_play\\_arrow_\n\n_info_\n\nFree API key\n\nFree trial key can be only used for non-commercial purposes. Please purchase a paid package for commercial use.\n\n_play\\_arrow_\n\nAre you using our official model images on AWS and Azure?\n\nNo restrictions. Use as per your current agreement.\n\n_play\\_arrow_\n\n_close_\n\nNo\n\n_play\\_arrow_\n\nAre you using these models?\n\njina-embeddings-v4\n\njina-reranker-m0\n\njina-clip-v2\n\njina-embeddings-v3\n\njina-reranker-v2-base-multilingual\n\njina-colbert-v2\n\nreader-lm-1.5b\n\nreader-lm-0.5b\n\nReaderLM-v2\n\n_play\\_arrow_\n\n_close_\n\nNo\n\nNo restrictions apply.\n\n_play\\_arrow_\n\n_done_\n\nYes\n\n_play\\_arrow_\n\nIs your use commercial?\n\n_play\\_arrow_\n\n_question\\_mark_\n\nNot sure\n\n_play\\_arrow_\n\nAre you:\n\n_play\\_arrow_\n\nUsing it for personal or hobby projects?\n\nThis is non-commercial. You can use the models freely.\n\n_play\\_arrow_\n\nA for-profit company using it internally?\n\nThis is commercial. Contact our sales team.\n\n[Contact sales](https://jina.ai/api-dashboard/contact-sales)\n\n_play\\_arrow_\n\nAn educational institution using it for teaching?\n\nThis is typically non-commercial. You can use the models freely.\n\n_play\\_arrow_\n\nA non-profit or NGO using it for your mission?\n\nThis is typically non-commercial, but check with us if unsure.\n\n[Contact sales](https://jina.ai/api-dashboard/contact-sales)\n\n_play\\_arrow_\n\nUsing it in a product or service you sell?\n\nThis is commercial. Contact our sales team.\n\n[Contact sales](https://jina.ai/api-dashboard/contact-sales)\n\n_play\\_arrow_\n\nA government entity using it for public services?\n\nThis may be commercial. Please contact us for clarification.\n\n[Contact sales](https://jina.ai/api-dashboard/contact-sales)\n\n_play\\_arrow_\n\n_close_\n\nNo\n\nYou can use the models freely.\n\n_play\\_arrow_\n\n_done_\n\nYes\n\nContact our sales team for licensing.\n\n[Contact sales](https://jina.ai/api-dashboard/contact-sales)\n\nAPI-related common questions\n\n_code_\n\nCan I use the same API key for reader, embedding, reranking, classifying and fine-tuning APIs?\n\n_keyboard\\_arrow\\_down_\n\nYes, the same API key is valid for all search foundation products from Jina AI. This includes the reader, embedding, reranking, classifying and fine-tuning APIs, with tokens shared between the all services.\n\n_code_\n\nCan I monitor the token usage of my API key?\n\n_keyboard\\_arrow\\_down_\n\nYes, token usage can be monitored in the 'API Key & Billing' tab by entering your API key, allowing you to view the recent usage history and remaining tokens. If you have logged in to the API dashboard, these details can also be viewed in the 'Manage API Key' tab.\n\n_code_\n\nWhat should I do if I forget my API key?\n\n_keyboard\\_arrow\\_down_\n\nIf you have misplaced a topped-up key and wish to retrieve it, please contact support AT jina.ai with your registered email for assistance. It's recommended to log in to keep your API key securely stored and easily accessible.\n\n[Contact](https://jina.ai/contact-sales)\n\n_code_\n\nDo API keys expire?\n\n_keyboard\\_arrow\\_down_\n\nNo, our API keys do not have an expiration date. However, if you suspect your key has been compromised and wish to retire it, please contact our support team for assistance. You can also revoke your key in [the API Key Management dashboard](https://jina.ai/api-dashboard).\n\n[Contact](https://jina.ai/contact-sales)\n\n_code_\n\nCan I transfer tokens between API keys?\n\n_keyboard\\_arrow\\_down_\n\nYes, you can transfer tokens from a premium key to another. After logging into your account on [the API Key Management dashboard](https://jina.ai/api-dashboard), use the settings of the key you want to transfer out to move all remaining paid tokens.\n\n_code_\n\nCan I revoke my API key?\n\n_keyboard\\_arrow\\_down_\n\nYes, you can revoke your API key if you believe it has been compromised. Revoking a key will immediately disable it for all users who have stored it, and all remaining balance and associated properties will be permanently unusable. If the key is a premium key, you have the option to transfer the remaining paid balance to another key before revocation. Notice that this action cannot be undone. To revoke a key, go to the key settings in [the API Key Management dashboard](https://jina.ai/api-dashboard).\n\n_code_\n\nWhy is the first request for some models slow?\n\n_keyboard\\_arrow\\_down_\n\nThis is because our serverless architecture offloads certain models during periods of low usage. The initial request activates or 'warms up' the model, which may take a few seconds. After this initial activation, subsequent requests process much more quickly.\n\n_code_\n\nIs user input data used for training your models?\n\n_keyboard\\_arrow\\_down_\n\nWe adhere to a strict privacy policy and do not use user input data for training our models. We are also SOC 2 Type I and Type II compliant, ensuring high standards of security and privacy.\n\nBilling-related common questions\n\n_attach\\_money_\n\nIs billing based on the number of sentences or requests?\n\n_keyboard\\_arrow\\_down_\n\nOur pricing model is based on the total number of tokens processed, allowing users the flexibility to allocate these tokens across any number of sentences, offering a cost-effective solution for diverse text analysis requirements.\n\n_attach\\_money_\n\nIs there a free trial available for new users?\n\n_keyboard\\_arrow\\_down_\n\nWe offer a welcoming free trial to new users, which includes ten millions tokens for use with any of our models, facilitated by an auto-generated API key. Once the free token limit is reached, users can easily purchase additional tokens for their API keys via the 'Buy tokens' tab.\n\n_attach\\_money_\n\nAre tokens charged for failed requests?\n\n_keyboard\\_arrow\\_down_\n\nNo, tokens are not deducted for failed requests.\n\n_attach\\_money_\n\nWhat payment methods are accepted?\n\n_keyboard\\_arrow\\_down_\n\nPayments are processed through Stripe, supporting a variety of payment methods including credit cards, Google Pay, and PayPal for your convenience.\n\n_attach\\_money_\n\nIs invoicing available for token purchases?\n\n_keyboard\\_arrow\\_down_\n\nYes, an invoice will be issued to the email address associated with your Stripe account upon the purchase of tokens.\n\nOffices\n\n_location\\_on_\n\nSunnyvale, CA\n\n710 Lakeway Dr, Ste 200, Sunnyvale, CA 94085, USA\n\n_location\\_on_\n\nBerlin, Germany (HQ)\n\nPrinzessinnenstra√üe 19-20, 10969 Berlin, Germany\n\n_location\\_on_\n\nBeijing, China\n\nLevel 5, Building 6, No.48 Haidian West St. Beijing, China\n\n_location\\_on_\n\nShenzhen, China\n\n402 Floor 4, Fu'an Technology Building, Shenzhen, China\n\nSearch Foundation\n\n[Reader](https://jina.ai/reader)[Embeddings](https://jina.ai/embeddings)[Reranker](https://jina.ai/reranker)[DeepSearch](https://jina.ai/deepsearch)[Classifier](https://jina.ai/classifier)[Segmenter](https://jina.ai/segmenter)[API Documentation](https://docs.jina.ai/)\n\nGet Jina API key\n\n[Rate Limit](https://jina.ai/contact-sales#rate-limit)[API Status](https://status.jina.ai/)\n\nCompany\n\n[About us](https://jina.ai/about-us)[Contact sales](https://jina.ai/contact-sales)[Newsroom](https://jina.ai/news)[Intern program](https://jina.ai/internship)[Join us _open\\_in\\_new_](https://app.dover.com/jobs/jinaai)[Download logo _open\\_in\\_new_](https://jina.ai/logo-Jina-1024.zip)\n\nTerms\n\n[Security](https://jina.ai/legal#security-as-company-value)[Terms & Conditions](https://jina.ai/legal/#terms-and-conditions)[Privacy](https://jina.ai/legal/#privacy-policy)[Manage Cookies](javascript:UC_UI.showSecondLayer();)[](https://app.eu.vanta.com/jinaai/trust/vz7f4mohp0847aho84lmva)\n\n[](https://x.com/jinaAI_)[](https://www.linkedin.com/company/jinaai/)[](https://github.com/jina-ai)[](https://huggingface.co/jinaai)[](https://discord.jina.ai/)[_email_](mailto:support@jina.ai)\n\n Jina AI ¬© 2020-2025.",
      "publishedTime": "Wed, 02 Jul 2025 05:14:59 GMT",
      "metadata": {
        "title": "Embedding API",
        "description": "Top-performing multimodal multilingual long-context embeddings for search, RAG, agents applications.",
        "og:type": "website",
        "og:url": "https://jina.ai/embeddings",
        "og:title": "Embedding API",
        "og:description": "Top-performing multimodal multilingual long-context embeddings for search, RAG, agents applications.",
        "og:image": "https://jina.ai/banner-embedding-api.png",
        "twitter:site": "@JinaAI_",
        "twitter:creator": "@JinaAI_",
        "twitter:card": "summary_large_image",
        "twitter:url": "https://jina.ai/embeddings",
        "twitter:title": "Embedding API",
        "twitter:description": "Top-performing multimodal multilingual long-context embeddings for search, RAG, agents applications.",
        "twitter:image": "https://jina.ai/banner-embedding-api.png",
        "format-detection": "telephone=no",
        "msapplication-tap-highlight": "no",
        "viewport": "user-scalable=no,initial-scale=1,maximum-scale=1,minimum-scale=1,width=device-width"
      },
      "external": {
        "icon": {
          "https://jina.ai/icons/favicon-128x128.png": {
            "type": "image/png",
            "sizes": "128x128"
          },
          "https://jina.ai/icons/favicon-96x96.png": {
            "type": "image/png",
            "sizes": "96x96"
          },
          "https://jina.ai/icons/favicon-32x32.png": {
            "type": "image/png",
            "sizes": "32x32"
          },
          "https://jina.ai/icons/favicon-16x16.png": {
            "type": "image/png",
            "sizes": "16x16"
          },
          "https://jina.ai/favicon.ico": {
            "type": "image/ico"
          }
        },
        "apple-touch-startup-image": {
          "https://jina.ai/icons/apple-launch-1284x2778.png": {
            "media": "(device-width: 428px) and (device-height: 926px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1170x2532.png": {
            "media": "(device-width: 390px) and (device-height: 844px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-828x1792.png": {
            "media": "(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1125x2436.png": {
            "media": "(device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1242x2688.png": {
            "media": "(device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-750x1334.png": {
            "media": "(device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1242x2208.png": {
            "media": "(device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3)"
          },
          "https://jina.ai/icons/apple-launch-1620x2160.png": {
            "media": "(device-width: 810px) and (device-height: 1080px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1536x2048.png": {
            "media": "(device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1668x2224.png": {
            "media": "(device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-1668x2388.png": {
            "media": "(device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2)"
          },
          "https://jina.ai/icons/apple-launch-2048x2732.png": {
            "media": "(device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2)"
          }
        },
        "modulepreload": {
          "https://jina.ai/assets/i18n-Bdzv5wkq.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/dynamic-import-helper-BheWnx7M.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/index-Cg0dwsIc.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/register-BVZZyrKh.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QTooltip-Bhb3YjpT.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/position-engine-C6jJfQWu.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/copy-to-clipboard-B2NwLaUR.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/MainLayout-iZRq4mDN.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/use-dialog-plugin-component-B0XQMl3F.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QBadge-COmWcOel.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/UserAvatarComponent-BgLGBt03.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QItemLabel-BpiRbV4W.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QChip-akpAcJA2.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QBtnDropdown-DCzAb9Oh.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QMenu-Cm1rPQ14.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QList-B9xrfd3b.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QLinearProgress-YROGvQfG.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QLayout-BuBW0QaA.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QResizeObserver-CQjNIUfW.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QScrollObserver-DtKL7UgA.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/TouchPan-DLCYdDWn.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/touch-BjYP5sR0.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QExpansionItem-Bp3L_jaT.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QSpinnerRings-mv8kdTDH.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/blogs-dG2W3-fu.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/ClosePopup-BV3m_dwK.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/search-De4nRjK2.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/VideoDialog-CilCHCHb.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/useRoute-CUxTLjTu.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/EmbeddingPage-CN2ilsW7.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QCarousel-DUqXNjP9.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/toInteger-BJDGI-QX.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/isSymbol-DhPpPzCf.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/use-panel-BFB69iRE.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/use-fullscreen-B01rDXj2.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QPage-Bf01fk3Y.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/useMetaTags-Dwn8KkH_.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/package-D9JPNs5w.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/BrowserInfoDialog-CjTRL1Ts.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/date-BRu9qQEH.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/PurchaseTab-C2jsM50e.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/finetune-HuFpbn3G.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/orderBy-BSFqKfNS.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/useModels-CxoY87wV.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QTab-Cq4zbKTf.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QTabs-CCw17oWQ.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QBanner-jqPNijYy.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/PurchaseDialog-Cu_iddv1.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/ResearchersComponent-DuMbZlyY.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/CompareTable-DJ0DlUY1.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/SXTooltip-C2xxFrs6.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/H2TitleBlock-BvqaGmm8.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/LabeledPanel-DRuKffEb.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/t8MpEwbUdl-mjUu1CMp.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/PricingComponent-_JU0p2R4.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/TitleBlock-HZriLLS2.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/FAQComponent-C5JY4SL8.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/SeFoComponent-DjxaPsHU.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/QForm-QNf7tyx4.js": {
            "as": "script",
            "crossorigin": ""
          },
          "https://jina.ai/assets/se-fo-DaUmeJEN.js": {
            "as": "script",
            "crossorigin": ""
          }
        },
        "preload": {
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/WebSdk.lib.bb0442d7.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/GdprCmpController.3cf0d250.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/UcGdprCmpView.bc665a94.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/sections.8a9daccf.js": {
            "as": "script"
          },
          "https://web.cmp.usercentrics.eu/ui/v/3.59.0/secondLayer.383e56e4.js": {
            "as": "script"
          }
        }
      },
      "usage": {
        "tokens": 8543
      }
    },
    {
      "title": "Jina AI (@JinaAI_) / X",
      "url": "https://x.com/JinaAI_?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor",
      "description": "",
      "content": "Jina AI (@JinaAI_) / X\n\n===============\n\nDon‚Äôt miss what‚Äôs happening\n\nPeople on X are the first to know.\n\n[Log in](https://x.com/login)\n\n[Sign up](https://x.com/i/flow/signup)\n\n[](https://x.com/)\n==================\n\nJina AI\n-------\n\n2,179 posts\n\nSee new posts\n\n[](https://x.com/JinaAI_/header_photo)\n\n[](https://x.com/JinaAI_/photo)\n\nFollow\n\nClick to Follow JinaAI_\n\nJina AI\n\n@JinaAI_\n\nYour Search Foundation, Supercharged!\n\nSunnyvale, CA[jina.ai](https://t.co/FcfbX3tRUX)Joined March 2020\n\n[146 Following](https://x.com/JinaAI_/following)\n\n[15.6K Followers](https://x.com/JinaAI_/verified_followers)\n\n[Posts](https://x.com/JinaAI_?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)\n\n[Replies](https://x.com/JinaAI_/with_replies?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)\n\n[Highlights](https://x.com/JinaAI_/highlights?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)\n\n[Media](https://x.com/JinaAI_/media?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)\n\nJina AI‚Äôs posts\n===============\n\n[](https://x.com/JinaAI_)\n\n[Jina AI](https://x.com/JinaAI_)\n\n[@JinaAI_](https://x.com/JinaAI_)\n\n¬∑\n\n[Sep 11, 2024](https://x.com/JinaAI_/status/1833861180445860168)\n\nAnnouncing reader-lm-0.5b and reader-lm-1.5b, [https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown?nocache=1‚Ä¶](https://t.co/jnxcxPzndy) two Small Language Models (SLMs) inspired by Jina Reader, and specifically trained to generate clean markdown directly from noisy raw HTML. Both models are multilingual and support a context length of up to\n\nShow more\n\n[Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown](https://t.co/jnxcxPzndy)\n\n[From jina.ai](https://t.co/jnxcxPzndy)\n\n26\n\n241\n\n1.2K\n\n[156K](https://x.com/JinaAI_/status/1833861180445860168/analytics)\n\n[](https://x.com/JinaAI_)\n\n[Jina AI](https://x.com/JinaAI_)\n\n[@JinaAI_](https://x.com/JinaAI_)\n\n¬∑\n\n[Aug 14, 2024](https://x.com/JinaAI_/status/1823756993108304135)\n\nBased. Semantic chunking is overrated. Especially when you write a super regex that leverages all possible boundary cues and heuristics to segment text accurately without the need for complex language models. Just think about the speed and the hosting cost. This 50-line,\n\nShow more\n\n[](https://x.com/JinaAI_/status/1823756993108304135/photo/1)\n\n34\n\n100\n\n751\n\n[111K](https://x.com/JinaAI_/status/1823756993108304135/analytics)\n\n[](https://x.com/JinaAI_)\n\n[Jina AI](https://x.com/JinaAI_)\n\n[@JinaAI_](https://x.com/JinaAI_)\n\n¬∑\n\n[Feb 14](https://x.com/JinaAI_/status/1890410008590086278)\n\nIntroducing jina-deepsearch-v1, it search, read, reason, search, read, reason, search, read, reason, search, read, reason, search, read, reason, search, read, reason, search, read, reason, search, read, reason, search, read, reason, search, read, reason, ...  until the\n\nShow more\n\n20\n\n98\n\n705\n\n[83K](https://x.com/JinaAI_/status/1890410008590086278/analytics)\n\n[](https://x.com/JinaAI_)\n\n[Jina AI](https://x.com/JinaAI_)\n\n[@JinaAI_](https://x.com/JinaAI_)\n\n¬∑\n\n[Oct 30, 2024](https://x.com/JinaAI_/status/1851651702635847729)\n\ncurl [http://docs.jina.ai](https://t.co/52oLHl30I3) This is our Meta-Prompt. It allows LLMs to understand our Reader, Embeddings, Reranker, and Classifier APIs for improved codegen. Using the meta-prompt is straightforward. Just copy the prompt into your preferred LLM interface like ChatGPT, Claude, or\n\nShow more\n\n15\n\n109\n\n671\n\n[207K](https://x.com/JinaAI_/status/1851651702635847729/analytics)\n\n[](https://x.com/JinaAI_)\n\n[Jina AI](https://x.com/JinaAI_)\n\n[@JinaAI_](https://x.com/JinaAI_)\n\n¬∑\n\n[Sep 18, 2024](https://x.com/JinaAI_/status/1836388833698680949)\n\nFinally, jina-embeddings-v3 is here! A frontier multilingual embedding model with 570M parameters, 8192-token length, achieving SOTA performance on multilingual and long-context retrieval tasks. It outperforms the latest proprietary models from OpenAI and Cohere, and outperforms\n\nShow more\n\n10\n\n118\n\n586\n\n[72K](https://x.com/JinaAI_/status/1836388833698680949/analytics)\n\nNew to X?\n---------\n\nSign up now to get your own personalized timeline!\n\nSign up with Apple\n\n[Create account](https://x.com/i/flow/signup)\n\nBy signing up, you agree to the [Terms of Service](https://x.com/tos) and [Privacy Policy](https://x.com/privacy), including [Cookie Use.](https://help.x.com/rules-and-policies/twitter-cookies)\n\n[Terms of Service](https://x.com/tos)\n\n|\n\n[Privacy Policy](https://x.com/privacy)\n\n|\n\n[Cookie Policy](https://support.x.com/articles/20170514)\n\n|\n\n[Accessibility](https://help.x.com/resources/accessibility)\n\n|\n\n[Ads info](https://business.x.com/en/help/troubleshooting/how-twitter-ads-work.html?ref=web-twc-ao-gbl-adsinfo&utm_source=twc&utm_medium=web&utm_campaign=ao&utm_content=adsinfo)\n\n|\n\nMore\n\n¬© 2025 X Corp.",
      "publishedTime": "Fri, 04 Jul 2025 16:39:17 GMT",
      "metadata": {
        "viewport": "width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover",
        "fb:app_id": "2231777543",
        "og:site_name": "X (formerly Twitter)",
        "google-site-verification": "reUF-TgZq93ZGtzImw42sfYglI2hY0QiGRmfc4jeKbs",
        "facebook-domain-verification": "x6sdcc8b5ju3bh8nbm59eswogvg6t1",
        "mobile-web-app-capable": "yes",
        "apple-mobile-web-app-title": "Twitter",
        "apple-mobile-web-app-status-bar-style": "white",
        "twitter-site-verification": "MKFbWaws5QK8vaduqi3+odzBD1DDstljmLY7QU/GPYx17vaOUR0b4u0TMCyXm+RJ",
        "theme-color": [
          "#FFFFFF",
          "#000000"
        ],
        "og:image": "https://abs.twimg.com/responsive-web/client-web/icon-ios.77d25eba.png",
        "apple-itunes-app": "app-id=333903271",
        "og:title": "Jina AI (@JinaAI_) / X"
      },
      "external": {
        "preconnect": {
          "https://abs.twimg.com/": {},
          "https://api.twitter.com/": {},
          "https://api.x.com/": {},
          "https://pbs.twimg.com/": {},
          "https://t.co/": {},
          "https://video.twimg.com/": {}
        },
        "dns-prefetch": {
          "https://abs.twimg.com/": {},
          "https://api.twitter.com/": {},
          "https://api.x.com/": {},
          "https://pbs.twimg.com/": {},
          "https://t.co/": {},
          "https://video.twimg.com/": {}
        },
        "preload": {
          "https://abs.twimg.com/responsive-web/client-web/vendor-7940b00b.aafae34a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-adcb47af.21d62e8a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-38c57b44.7266e9ba.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-85aa29ea.b9ec208a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-cb2d071c.790f8aca.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-aaaf2b0c.5c59c02a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-49d0a293.b1a6b16a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-6b20cc7c.3951c3aa.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-dfe82965.8f17b49a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-e395fecc.40e2e8aa.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-63e37921.8411268a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-de539588.b967545a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-669c86db.f6db5f1a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-3dfac8a4.346d055a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-27545368.77b8a3ba.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-58c6fc15.0f415a1a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-57216f32.d6191e3a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-e5bca7e4.1458567a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-744bed60.79286f6a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-3e5eb623.067d49da.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-98a766dd.9802f00a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-2bf3abf4.f8393bda.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-ec4c1ee7.89c8b22a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-c22f700c.0254af5a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-bfc04956.5b6ec3fa.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-48a4958c.22189dca.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-eb8eaa08.b1c15f8a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-91c40cd8.e33ef86a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-c4d1d074.c9bdc52a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-ccf8c62e.f0cf07fa.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-821262ff.fc6cf03a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/vendor-49ceb22a.1a89ac1a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/i18n/en.5a97e14a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          },
          "https://abs.twimg.com/responsive-web/client-web/main.7359a36a.js": {
            "nonce": "MGUzMDEwYTAtZDZmNS00Mjc3LWJlOWYtY2E3Nzk3Y2E2ZWVk",
            "as": "script",
            "crossorigin": "anonymous"
          }
        },
        "search": {
          "https://x.com/os-x.xml": {
            "type": "application/opensearchdescription+xml",
            "title": "X"
          },
          "https://x.com/os-grok.xml": {
            "type": "application/opensearchdescription+xml",
            "title": "Grok"
          }
        },
        "apple-touch-icon": {
          "https://abs.twimg.com/responsive-web/client-web/icon-ios.77d25eba.png": {
            "sizes": "192x192"
          }
        },
        "manifest": {
          "https://x.com/manifest.json": {
            "crossorigin": "use-credentials"
          }
        },
        "mask-icon": {
          "https://abs.twimg.com/responsive-web/client-web/icon-svg.ea5ff4aa.svg": {
            "sizes": "any",
            "color": "#1D9BF0"
          }
        },
        "icon": {
          "https://abs.twimg.com/favicons/twitter.3.ico": {}
        },
        "canonical": {
          "https://x.com/jinaai_": {
            "data-rh": "true"
          }
        },
        "alternate": {
          "https://x.com/jinaai_": {
            "hreflang": "x-default",
            "data-rh": "true"
          }
        }
      },
      "usage": {
        "tokens": 1513
      }
    }
  ],
  "meta": {
    "usage": {
      "tokens": 83726
    }
  }
}
```
