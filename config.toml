# DeepSearchAgents config file - Simplified Version
# Without Streamable HTTP & SSE functionality

# Debug mode (enable verbose output)
debug = true

# Model configuration
[models]
orchestrator_id = "openai/openrouter/openai/o4-mini-high"  # for main LLM orchestration
search_id = "openai/openrouter/openai/o4-mini-high"        # for search (if different)
reranker_type = "jina-reranker-m0"

# Tools configuration
[tools]
# List of Hugging Face Hub collection slugs to load tools from
hub_collections = []  # e.g. ["huggingface-tools/diffusion-tools-collection"]
# Whether to trust remote code for Hub and MCP tools (SECURITY RISK!)
trust_remote_code = false
# List of MCP server configurations
mcp_servers = []
# Example MCP server configuration:
# [[tools.mcp_servers]]
# type = "stdio"  # "stdio" or "sse"
# command = "uv"
# args = ["--quiet", "langchain-mcp"]
# env = {}  # Additional environment variables
# [[tools.mcp_servers]]
# type = "sse" 
# url = "http://127.0.0.1:8000/sse"

# Tool-specific configurations
[tools.specific]
# Reranking tool configuration
rerank_texts = { default_model = "jina-reranker-m0" }
# Search tool configuration
search_links = { num_results = 10, location = "us" }
# ReadURL tool configuration
read_url = { reader_model = "readerlm-v2" }
# Chunk text tool configuration
chunk_text = { chunk_size = 150, chunk_overlap = 50 }

# Agent common settings
[agents.common]
verbose_tool_callbacks = true  # if true, show full tool input/output

# React agent specific settings
[agents.react]
max_steps = 25                # maximum number of reasoning steps
planning_interval = 7         # agent planning step interval

# CodeAct agent specific settings
[agents.codact]
executor_type = "local"       # local or lambda
max_steps = 25                # maximum number of steps in execution
verbosity_level = 2           # 0=minimum, 1=normal, 2=detailed
planning_interval = 4         # agent planning step interval

# This is an empty dictionary and an empty array
executor_kwargs = {}          # executor additional parameters
additional_authorized_imports = []  # additional Python modules allowed to import

# Service configuration
[service]
host = "0.0.0.0"
port = 8000
version = "0.2.6.dev"
deepsearch_agent_mode = "codact"  # "react" or "codact"

# Logging configuration
[logging]
litellm_level = "WARNING"     # use WARNING level to reduce INFO logs
filter_repeated_logs = false   # enable repeated log filtering
filter_cost_calculator = false # filter cost calculation related logs
filter_token_counter = false   # filter token count related logs
enable_token_counting = true   # enable token counting feature
log_tokens = true             # enable token logging
format = "minimal"            # use simplified format

# Tested and working model list
[[model_list]]
model_name = "claude-3-7-sonnet-20250219"
[model_list.litellm_params]
model = "claude-3-7-sonnet-20250219" 
api_base = "LITELLM_BASE_URL"
api_key = "LITELLM_MASTER_KEY"

[[model_list]]
model_name = "gemini-2.5-pro-exp-03-25"
[model_list.litellm_params]
model = "openai/gemini-2.5-pro-exp-03-25"
api_base = "LITELLM_BASE_URL"
api_key = "LITELLM_MASTER_KEY"
