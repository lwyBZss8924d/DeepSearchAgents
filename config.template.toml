# DeepSearchAgents config file - Simplified Version

# Debug mode (enable verbose output)
debug = true

# Model configuration
[models]
orchestrator_id = "gemini/gemini-2.5-pro-preview-05-06"  # for main LLM orchestration
search_id = "anthropic/claude-sonnet-4-20250514"        # for search (if different)
reranker_type = "jina-reranker-m0" # reranker model for reranking search results(optional)

# Tools configuration
[tools]
# List of Hugging Face Hub collection slugs to load tools from
hub_collections = []  # e.g. ["huggingface-tools/diffusion-tools-collection"]
# Whether to trust remote code for Hub and MCP tools (SECURITY RISK!)
trust_remote_code = false
# List of MCP server configurations
mcp_servers = []
# Example MCP server configuration:
# [[tools.mcp_servers]]
# type = "stdio"  # "stdio" or "sse"
# command = "uv"
# args = ["--quiet", "langchain-mcp"]
# env = {}  # Additional environment variables
# [[tools.mcp_servers]]
# type = "sse" 
# url = "http://127.0.0.1:8000/sse"

# Tool-specific configurations
[tools.specific]
# Reranking tool configuration
rerank_texts = { default_model = "jina-reranker-m0" }
# Search tool configuration
search_links = { num_results = 10, location = "us" }
# ReadURL tool configuration
read_url = { reader_model = "readerlm-v2" }
# Chunk text tool configuration
chunk_text = { chunk_size = 150, chunk_overlap = 50 }

# Agent common settings
[agents.common]
verbose_tool_callbacks = true  # if true, show full tool input/output

# React agent specific settings
[agents.react]
max_steps = 25                # maximum number of reasoning steps
planning_interval = 7         # agent planning step interval
max_tool_threads = 1          # max threads for parallel tool execution (1=sequential)
verbosity_level = 2           # 0=minimum, 1=normal, 2=detailed

# CodeAct agent specific settings
[agents.codact]
executor_type = "local"       # local or lambda
max_steps = 25                # maximum number of steps in execution
planning_interval = 5         # agent planning step interval
verbosity_level = 2           # 0=minimum, 1=normal, 2=detailed
use_structured_outputs = true   # Enable JSON-structured outputs (default: true)

# Executor security settings (can be overridden but not recommended)
# Default security settings applied:
# - timeout: 30 seconds per code execution
# - max_output_size: 100KB output limit
# - restricted_paths: ["/etc", "/root", "/home"] are blocked
executor_kwargs = {}          # executor additional parameters

# Additional Python modules to allow (dangerous modules are auto-filtered)
# Blocked modules: os, sys, subprocess, eval, exec, compile, etc.
additional_authorized_imports = []  # additional Python modules allowed to import

# Manager agent configuration (for hierarchical agent orchestration)
[agents.manager]
enabled = true                      # Enable managed agents support
max_delegation_depth = 3            # Maximum depth of agent delegation
default_managed_agents = ["react", "codact"]  # Default sub-agents for manager

# Service configuration
[service]
host = "0.0.0.0"
port = 8000
version = "0.2.8"
deepsearch_agent_mode = "codact"  # "react" or "codact"

# Logging configuration
[logging]
litellm_level = "WARNING"     # use WARNING level to reduce INFO logs
filter_repeated_logs = false   # enable repeated log filtering
filter_cost_calculator = false # filter cost calculation related logs
filter_token_counter = false   # filter token count related logs
enable_token_counting = true   # enable token counting feature
log_tokens = true             # enable token logging
format = "minimal"            # use simplified format

# Tested and working model list
[[model_list]]
model_name = "openai-codex-mini"
[model_list.litellm_params]
model = "openai/codex-mini" 
api_base = "LITELLM_BASE_URL"
api_key = "LITELLM_MASTER_KEY"

[[model_list]]
model_name = "o4-mini-high"
[model_list.litellm_params]
model = "openai/o4-mini-high"
api_base = "LITELLM_BASE_URL"
api_key = "LITELLM_MASTER_KEY"
